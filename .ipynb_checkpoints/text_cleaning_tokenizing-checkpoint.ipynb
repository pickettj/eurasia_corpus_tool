{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora Cleaning, Tokenizing, Pickling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arabic_cleaning as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, os, glob, pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Home Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set home directory path\n",
    "hdir = os.path.expanduser('~')\n",
    "\n",
    "#external relative path\n",
    "ext_corp_path = hdir + \"/Box/Notes/Digital_Humanities/Corpora\"\n",
    "\n",
    "#internal relative path\n",
    "int_corp_path = hdir + \"/Box/Notes/Primary_Sources\"\n",
    "\n",
    "#pickle path\n",
    "pickle_path = hdir + \"/Box/Notes/Digital_Humanities/Corpora/pickled_tokenized_cleaned_corpora\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-existing Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indic Narrative\n",
    "indo_path = ext_corp_path + \"/indo-persian_corpora\"\n",
    "\n",
    "# Transoxania Narrative (Persian)\n",
    "trans_path = ext_corp_path + \"/machine_readable_persian_transoxania_texts\"\n",
    "\n",
    "# Khiva documents\n",
    "khiva_path = ext_corp_path + \"/khiva_khanate_chancery_corpus\"\n",
    "\n",
    "# Muscovite Persian diplomatic documents\n",
    "musc_path = ext_corp_path + \"/khorezm_muscovy_diplomatic\"\n",
    "\n",
    "# Persian Lit\n",
    "perslit_path = ext_corp_path + \"/persian_literature_digital_corpus_roshan\"\n",
    "\n",
    "# Turkic Narrative sources\n",
    "turk_path = ext_corp_path + \"/turkic_corpora\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Self-created Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indian Narrative\n",
    "indo_man_path = int_corp_path + \"/non-machine-readable_notes/indian_manuscripts\"\n",
    "\n",
    "# Transoxania Narrative\n",
    "trans_man_path = int_corp_path + \"/non-machine-readable_notes/bactriana_notes\"\n",
    "\n",
    "# Transoxania Documents\n",
    "trans_man_docs_path = int_corp_path + \"/xml_notes_stage2/bukhara_xml\"\n",
    "\n",
    "# Hyderabad Documents\n",
    "hyd_man_docs_path = int_corp_path + \"/xml_notes_stage2/hyderabad_xml\"\n",
    "\n",
    "# Indian Documents (misc. transcribed)\n",
    "indo_man_docs_path = int_corp_path + \"/xml_notes_stage2/indic_corpus_xml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unorganized Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted to XML, pre-sorted\n",
    "parser_xml_path = int_corp_path + \"/xml_notes_stage2/parser_depository\"\n",
    "\n",
    "# Old system, yet to update\n",
    "xml_old_sys_path = int_corp_path + \"/xml_notes_stage2/xml_transcriptions_old_system\"\n",
    "\n",
    "# Markdown stage\n",
    "markdown_path = int_corp_path + \"/transcription_markdown_drafting_stage1\"\n",
    "\n",
    "# Markdown backlog (old system)\n",
    "md_backlog_path = int_corp_path + \"/transcription_markdown_drafting_stage1/document_conversion_backlog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Globbing Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-existing Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indic Narrative\n",
    "Thackston corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ain-i_akbari_murty', 'an1_nassim', 'an2_nassim', 'an3_nassim', 'badauni_muntakhab_al-tawwarikh', 'jahangirnama', 'mu_vol1', 'mu_vol2', 'mu_vol3', 'psn1', 'psn2', 'psn3', 'siyar_al-mutaakhirin1', 'siyar_al-mutaakhirin2', 'sjn1', 'sjn2', 'sjn3'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indo_corpus_files = glob.glob(indo_path + r'//**/*.txt', recursive=True)\n",
    "\n",
    "indo_corpus = {}\n",
    "for longname in indo_corpus_files:\n",
    "    with open(longname) as f:\n",
    "        txt = f.read()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    indo_corpus[short[0]] = txt\n",
    "    \n",
    "indo_corpus.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transoxania Narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ikromcha', 'khumuli', 'samarat', 'damla_abid_akhund_ser722', 'darbandi_alexiii_coronation_ser728', 'prisoedineniia_samarkand_ser723', 'proshenie_k_general-gubernatory_ser721', 'rasskaz_praviteli_shahrisabz_ser724', 'tarikh-i_jadida_tashkent_ser725', 'tuhfa-ahli-bukhara_ser25', 'tuhfa-i_taib_ser726'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_corpus_files = glob.glob(trans_path + r'//**/*.txt', recursive=True)\n",
    "\n",
    "trans_corpus = {}\n",
    "for longname in trans_corpus_files:\n",
    "    with open(longname) as f:\n",
    "        txt = f.read()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    trans_corpus[short[0]] = txt\n",
    "    \n",
    "    \n",
    "trans_corpus.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persian Literature\n",
    "*See below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Khiva Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "khiva_corpus_files = glob.glob(khiva_path + r'//**/*.txt', recursive=True)\n",
    "\n",
    "khiva_corpus = {}\n",
    "for longname in khiva_corpus_files:\n",
    "    with open(longname) as f:\n",
    "        txt = f.read()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    khiva_corpus[short[0]] = txt\n",
    "    \n",
    "    \n",
    "#khiva_corpus.keys.()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turkic Documents\n",
    "*TBD*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muscovite Persian diplomatic documents\n",
    "*TBD*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-created Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indic Narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['awrangabadi_gul-i_rana_ser794', 'dustur_al-amal_ser790', 'gawhar_khan_waqa-i_shaykh_dalil_ser796', 'hidayat-i_zururiyya_kotwali_ser791', 'husayni_waqay-i_dakkan_ser779', 'jilani_salar_al-intizam_ser789', 'muhammad_al-madrasi_minhaj_al-adala_ser801', 'qanuncha_adalat_ser788', 'ratan_lal_tuhfa-i_dakkan_ser783', 'shahjahanpuri_yadgar-i_makhan_lal_ser780'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indo_man_files = glob.glob(indo_man_path + r'//**/*.txt', recursive=True)\n",
    "\n",
    "indo_man = {}\n",
    "for longname in indo_man_files:\n",
    "    with open(longname) as f:\n",
    "        txt = f.read()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    indo_man[short[0]] = txt\n",
    "    \n",
    "    \n",
    "#indo_corpus.keys()\n",
    "indo_man.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transoxania Narrative\n",
    "Corpus based on partially transcribed manuscripts from early modern Transoxania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_man_files = glob.glob(trans_man_path + r'/*.txt')\n",
    "\n",
    "trans_man = {}\n",
    "for longname in trans_man_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    trans_man[short[0]] = txt\n",
    "\n",
    "#trans_man.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transoxania Documents\n",
    "Qushbegi documents at XML stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_man_doc_files = glob.glob(trans_man_docs_path + r'/*.xml')\n",
    "\n",
    "trans_man_docs = {}\n",
    "for longname in trans_man_doc_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    trans_man_docs[short[0]] = txt\n",
    "\n",
    "#trans_man_docs['ser179']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyderabad Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyderabad Documents\n",
    "\n",
    "hyd_man_doc_files = glob.glob(hyd_man_docs_path + r'/*.xml')\n",
    "\n",
    "hyd_man_docs = {}\n",
    "for longname in hyd_man_doc_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    hyd_man_docs[short[0]] = txt\n",
    "\n",
    "hyd_man_docs.keys()\n",
    "#Note: nothing in that folder yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indic Documents\n",
    "Misc. Indic documents other than those from the Nizam State collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ser818'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_man_doc_files = glob.glob(indo_man_docs_path + r'/*.xml')\n",
    "\n",
    "ind_man_docs = {}\n",
    "for longname in ind_man_doc_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    ind_man_docs[short[0]] = txt\n",
    "\n",
    "ind_man_docs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unorganized Documents\n",
    "E.g. documents still at the markdown stage, and not yet sorted by region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML, pre-sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ser560', 'ser808', 'ser809', 'ser811', 'ser812', 'ser813', 'ser814', 'ser815', 'ser816', 'ser817', 'ser842', 'ser843', 'ser857', 'ser876', 'ser877', 'ser898'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_presort_files = glob.glob(parser_xml_path + r'/*.xml')\n",
    "\n",
    "xml_presort_docs = {}\n",
    "for longname in xml_presort_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    xml_presort_docs[short[0]] = txt\n",
    "\n",
    "xml_presort_docs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML, old system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['IVANUz_1936_ser185', 'NLR_f-940_ser190', 'RGVIA_400-1-1015_ser143', 'TsGARUz_i126-1-938-2_ser82', 'TsGARUz_i126_1_1160_ser193', 'TsGARUZ_i126_1_1729_101_ser213', 'TsGARUz_i126_1_1730_19_ser218', 'TsGARUz_i126_1_1730_22_ser217', 'TsGARUz_i126_1_1730_2_ser188', 'TsGARUZ_i126_1_1730_81_ser227', 'TsGARUZ_i126_1_1986_1_ser201', 'TsGARUz_i126_1_1990_20_ser186', 'TsGARUZ_i126_1_1990_3_ser192', 'TsGARUz_R-2678_ser184'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_oldsys_files = glob.glob(xml_old_sys_path + r'//**/*.xml', recursive=True)\n",
    "\n",
    "xml_oldsys_docs = {}\n",
    "for longname in xml_oldsys_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    xml_oldsys_docs[short[0]] = txt\n",
    "\n",
    "xml_oldsys_docs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling XML Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ser818', 'ser179', 'ser183', 'ser187', 'ser212', 'ser215', 'ser237', 'ser537', 'ser561', 'ser596', 'ser626', 'ser706', 'ser72', 'ser91', 'IVANUz_1936_ser185', 'NLR_f-940_ser190', 'RGVIA_400-1-1015_ser143', 'TsGARUz_i126-1-938-2_ser82', 'TsGARUz_i126_1_1160_ser193', 'TsGARUZ_i126_1_1729_101_ser213', 'TsGARUz_i126_1_1730_19_ser218', 'TsGARUz_i126_1_1730_22_ser217', 'TsGARUz_i126_1_1730_2_ser188', 'TsGARUZ_i126_1_1730_81_ser227', 'TsGARUZ_i126_1_1986_1_ser201', 'TsGARUz_i126_1_1990_20_ser186', 'TsGARUZ_i126_1_1990_3_ser192', 'TsGARUz_R-2678_ser184', 'ser560', 'ser808', 'ser809', 'ser811', 'ser812', 'ser813', 'ser814', 'ser815', 'ser816', 'ser817', 'ser842', 'ser843', 'ser857', 'ser876', 'ser877', 'ser898'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merges\n",
    "\n",
    "## All final stage XML documents\n",
    "combo_xml_final = {**ind_man_docs, **hyd_man_docs, **trans_man_docs}\n",
    "## All XML all stages\n",
    "combo_xml_all = {**combo_xml_final, **xml_oldsys_docs, **xml_presort_docs}\n",
    "\n",
    "\n",
    "combo_xml_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to pickle sub-directories of unsorted XML files\n",
    "with open(pickle_path + \"/xml_corpora.pkl\", \"wb\") as f:\n",
    "    pickle.dump((ind_man_docs, hyd_man_docs, trans_man_docs,\\\n",
    "                combo_xml_final, combo_xml_all), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markdown Stage\n",
    "Transcribed docs, yet to be ported over to XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_files = glob.glob(markdown_path + r'/*.xml')\n",
    "\n",
    "markdown_docs = {}\n",
    "for longname in markdown_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    markdown_docs[short[0]] = txt\n",
    "\n",
    "markdown_docs.keys()\n",
    "#Will be empty if everything was recently parsed and transfered, per workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markdown, old system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_old_files = glob.glob(md_backlog_path + r'//**/*.txt', recursive=True)\n",
    "\n",
    "markdown_old_docs = {}\n",
    "for longname in markdown_old_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    markdown_old_docs[short[0]] = txt\n",
    "\n",
    "#markdown_old_docs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persian Literature Digital Corpus\n",
    "Massive corpus of Persian literature, pulled from Ganjur (http://ganjoor.net/) by Roshan (https://persdigumd.github.io/PDL/)\n",
    "\n",
    "*Corpus pre-cleaned, tokenized, and pickled from a separate script. (Cleaning takes a long time; and this corpus doesn't change very often, and so does not need to be re-run.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(perslit_path + '/persian_lit_toks.pkl', 'rb') \n",
    "\n",
    "pers_lit_toks = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pers_lit_toks.keys()\n",
    "#pers_lit_toks[\"hafez.masnavi\"][:50]\n",
    "#pers_lit_toks['ferdowsi.shahnameh']\n",
    "\n",
    "#type (pers_lit_toks['ferdowsi.shahnameh'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning edited texts and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible to do this once by iterating over the following? crashed computer last time...\n",
    "\n",
    "# indo_corpus, trans_corpus, khiva_corpus\n",
    "# indo_man, trans_man\n",
    "# trans_man_docs, hyd_man_docs, ind_man_docs\n",
    "# xml_presort_docs, xml_oldsys_docs, markdown_docs, markdown_old_docs\n",
    "\n",
    "\n",
    "clean_indo = {fn: ac.clean_document(doc) for fn, doc in indo_corpus.items()}\n",
    "clean_trans = {fn: ac.clean_document(doc) for fn, doc in trans_corpus.items()}\n",
    "clean_khiva = {fn: ac.clean_document(doc) for fn, doc in khiva_corpus.items()}\n",
    "\n",
    "clean_indo_man = {fn: ac.clean_document(doc) for fn, doc in indo_man.items()}\n",
    "clean_trans_man = {fn: ac.clean_document(doc) for fn, doc in trans_man.items()}\n",
    "\n",
    "clean_trans_man_docs = {fn: ac.clean_document(doc) for fn, doc in trans_man_docs.items()}\n",
    "clean_hyd_man_docs = {fn: ac.clean_document(doc) for fn, doc in hyd_man_docs.items()}\n",
    "clean_ind_man_docs = {fn: ac.clean_document(doc) for fn, doc in ind_man_docs.items()}\n",
    "\n",
    "clean_xml_presort_docs = {fn: ac.clean_document(doc) for fn, doc in xml_presort_docs.items()}\n",
    "clean_xml_oldsys_docs = {fn: ac.clean_document(doc) for fn, doc in xml_oldsys_docs.items()}\n",
    "clean_markdown_docs = {fn: ac.clean_document(doc) for fn, doc in markdown_docs.items()}\n",
    "clean_markdown_old_docs = {fn: ac.clean_document(doc) for fn, doc in markdown_old_docs.items()}\n",
    "\n",
    "\n",
    "\n",
    "#clean_trans['ikromcha'][:1000]\n",
    "#clean_trans['ikromcha'][:1000]\n",
    "\n",
    "\n",
    "#clean_xml['ser561']\n",
    "\n",
    "#clean_indo['mu_vol1'][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# External Corpora Toks\n",
    "\n",
    "indo_nar_ext_toks = {}\n",
    "for (fn, txt) in clean_indo.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    indo_nar_ext_toks[fn] = toks\n",
    "\n",
    "trans_nar_ext_toks = {}\n",
    "for (fn, txt) in clean_trans.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    trans_nar_ext_toks[fn] = toks \n",
    "    \n",
    "khiva_doc_toks = {}\n",
    "for (fn, txt) in clean_khiva.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    khiva_doc_toks[fn] = toks\n",
    "\n",
    "    \n",
    "# Manually Entered Manuscript Toks\n",
    "\n",
    "indo_nar_toks = {}\n",
    "for (fn, txt) in clean_indo_man.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    indo_nar_toks[fn] = toks\n",
    "    \n",
    "trans_nar_toks = {}\n",
    "for (fn, txt) in clean_trans_man.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    trans_nar_toks[fn] = toks\n",
    "\n",
    "# Clean XML-stage Document Toks\n",
    " \n",
    "trans_xml_toks = {}\n",
    "for (fn, txt) in clean_trans_man_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    trans_xml_toks[fn] = toks\n",
    "    \n",
    "hyd_xml_toks = {}\n",
    "for (fn, txt) in clean_hyd_man_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    hyd_xml_toks[fn] = toks\n",
    "\n",
    "indo_xml_toks = {}\n",
    "for (fn, txt) in clean_ind_man_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    indo_xml_toks[fn] = toks\n",
    "\n",
    "\n",
    "# Unorganized Markdown-stage Toks\n",
    "\n",
    "\n",
    "presort_xml_toks = {}\n",
    "for (fn, txt) in clean_xml_presort_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    presort_xml_toks[fn] = toks\n",
    "    \n",
    "oldsys_xml_toks = {}\n",
    "for (fn, txt) in clean_xml_oldsys_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    oldsys_xml_toks[fn] = toks\n",
    "    \n",
    "md_stage_toks = {}\n",
    "for (fn, txt) in clean_markdown_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    md_stage_toks[fn] = toks\n",
    "\n",
    "md_oldsys_toks = {}\n",
    "for (fn, txt) in clean_markdown_old_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    md_oldsys_toks[fn] = toks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First-stage combinations*: Collapse unsorted documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsorted_doc_toks = {**presort_xml_toks, **oldsys_xml_toks, **md_stage_toks, **md_oldsys_toks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsorted_doc_toks['ser560']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = hdir + \"/Box/Notes/Digital_Humanities/Corpora/pickled_tokenized_cleaned_corpora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_path + \"/corpora.pkl\", \"wb\") as f:\n",
    "    pickle.dump((unsorted_doc_toks,\\\n",
    "                indo_xml_toks, hyd_xml_toks, trans_xml_toks,\\\n",
    "                trans_nar_toks, indo_nar_toks,\\\n",
    "                trans_nar_ext_toks, indo_nar_ext_toks), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Corpuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Explanation on merging dictionaries](https://www.webucator.com/how-to/how-merge-dictionaries-python.cfm)\n",
    "\n",
    "Combos:\n",
    "- Indic narrative sources (combining external and self-transcribed)\n",
    "- Transoxania narrative sources (combining external and self-transcribed)\n",
    "- All narrative sources\n",
    "- All Persian documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comb_india_nar_toks = {**indo_nar_toks, **indo_nar_ext_toks}\n",
    "\n",
    "comb_trans_nar_toks = {**trans_nar_toks, **trans_nar_ext_toks}\n",
    "\n",
    "nar_corpus_toks = {**comb_trans_nar_toks, **comb_india_nar_toks}\n",
    "\n",
    "doc_corpus_toks = {**unsorted_doc_toks, **indo_xml_toks, **hyd_xml_toks, **trans_xml_toks}\n",
    "\n",
    "\n",
    "# Meta-Corpus (except Persian lit)\n",
    "combined_corpus_toks = {**nar_corpus_toks, **doc_corpus_toks}\n",
    "\n",
    "# Mega-Corpus\n",
    "mega_corpus_toks = {**combined_corpus_toks, **pers_lit_toks}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_corpus_toks['mu_vol1'][:50]\n",
    "#mega_corpus_toks[\"hafez.masnavi\"][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling combined corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_path + \"/meta_corpora.pkl\", \"wb\") as f:\n",
    "    pickle.dump((comb_india_nar_toks, comb_trans_nar_toks, nar_corpus_toks, doc_corpus_toks,\\\n",
    "                combined_corpus_toks, mega_corpus_toks), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losing corpus hierarchy for simple token lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined Tokens (loses corpus text designation)\n",
    "\n",
    "raw_doc_toks = []\n",
    "for (fn, text) in doc_corpus_toks.items():\n",
    "    raw_doc_toks.extend(doc_corpus_toks[fn])\n",
    "    \n",
    "raw_nar_toks = []\n",
    "for (fn, text) in nar_corpus_toks.items():\n",
    "    raw_nar_toks.extend(nar_corpus_toks[fn])\n",
    "    \n",
    "raw_lit_toks = []\n",
    "for (fn, text) in pers_lit_toks.items():\n",
    "    raw_lit_toks.extend(pers_lit_toks[fn])\n",
    "\n",
    "raw_combo_toks = []\n",
    "for (fn, text) in combined_corpus_toks.items():\n",
    "    raw_combo_toks.extend(combined_corpus_toks[fn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_combo_toks[100:125]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling Raw Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_path + \"/raw_tokens.pkl\", \"wb\") as f:\n",
    "    pickle.dump((raw_doc_toks, raw_nar_toks, raw_lit_toks, raw_combo_toks), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive\n",
    "(Old methods, now memorialized in markdown\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Defunct method: [creating an NLTK corpus](http://www.nltk.org/book/ch02.html#loading-your-own-corpus)\n",
    "\n",
    "```python\n",
    "\n",
    "os.chdir('/Users/Enkidu/Documents/digital_humanities/jupyter_notebooks')\n",
    "corpus_root = 'machine_readable_persian_transoxania_texts'\n",
    "turkestan_corpus = PlaintextCorpusReader(corpus_root, '.*')\n",
    "turkestan_corpus.fileids()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning: Now a function is pulled from an external file (arabic_cleaning.py). Previous method saved for posterity:\n",
    "\n",
    "```python\n",
    "clean_edited_i = {}\n",
    "for fn in raw_edited_corpus:\n",
    "    clean_edited_i[fn] = re.sub(r'ي', 'ی', raw_edited_corpus[fn])\n",
    "\n",
    "clean_edited = {}\n",
    "for fn in clean_edited_i:\n",
    "    clean_edited[fn] = re.sub(r'[^آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهس ي یی ]', '', clean_edited_i[fn])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning XML documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dormant XML cleaning method using BeautifulSoup (still in use for Persian literature tokenization in separate script)*\n",
    "\n",
    "```python\n",
    "bstree = bs4.BeautifulSoup(clean_xml[\"ser561\"], 'lxml')\n",
    "\n",
    "\n",
    "print(bstree.get_text())\n",
    "\n",
    "clean_xml = {}\n",
    "for fn in raw_xml:\n",
    "    bstree = bs4.BeautifulSoup(raw_xml[fn], 'lxml')\n",
    "    clean_xml[fn] = bstree.get_text()\n",
    "    \n",
    "clean_xml['TsGARUZ_i126_1_1986_1_ser201']\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
