{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpora Cleaning, Tokenizing, Pickling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arabic_cleaning as ac\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, glob, os, pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Home Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set home directory path\n",
    "hdir = os.path.expanduser('~')\n",
    "\n",
    "#external relative path\n",
    "ext_corp_path = hdir + \"/Dropbox/Active_Directories/Digital_Humanities/Corpora\"\n",
    "\n",
    "#internal relative path\n",
    "int_corp_path = hdir + \"/Dropbox/Active_Directories/Notes/Primary_Sources\"\n",
    "\n",
    "#pickle path\n",
    "pickle_path = hdir + \"/Dropbox/Active_Directories/Digital_Humanities/Corpora/pickled_tokenized_cleaned_corpora\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-existing Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indic Narrative\n",
    "indo_path = ext_corp_path + \"/indo-persian_corpora\"\n",
    "\n",
    "# Transoxania Narrative (Persian)\n",
    "trans_path = ext_corp_path + \"/machine_readable_persian_transoxania_texts\"\n",
    "\n",
    "# Khiva documents\n",
    "khiva_path = ext_corp_path + \"/khiva_khanate_chancery_corpus\"\n",
    "\n",
    "# Muscovite Persian diplomatic documents\n",
    "musc_path = ext_corp_path + \"/khorezm_muscovy_diplomatic\"\n",
    "\n",
    "# Persian Lit\n",
    "perslit_path = ext_corp_path + \"/pickled_tokenized_cleaned_corpora\"\n",
    "\n",
    "# Turkic Narrative sources\n",
    "turk_path = ext_corp_path + \"/turkic_corpora\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Self-created Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indian Narrative\n",
    "indo_man_path = int_corp_path + \"/non-machine-readable_notes/india_manuscripts\"\n",
    "\n",
    "# Transoxania Narrative\n",
    "trans_man_path = int_corp_path + \"/non-machine-readable_notes/transoxania_manuscripts\"\n",
    "\n",
    "# Transoxania Documents\n",
    "trans_man_docs_path = int_corp_path + \"/xml_notes_stage3_final/bukhara_xml\"\n",
    "\n",
    "# Hyderabad Documents\n",
    "hyd_man_docs_path = int_corp_path + \"/xml_notes_stage3_final/hyderabad_xml\"\n",
    "\n",
    "# Indian Documents (misc. transcribed)\n",
    "indo_man_docs_path = int_corp_path + \"/xml_notes_stage3_final/indic_corpus_xml\"\n",
    "\n",
    "# Qajar Documents (misc. transcribed)\n",
    "qajar_man_docs_path = int_corp_path + \"/xml_notes_stage3_final/qajar_xml\"\n",
    "\n",
    "# Qajar Documents (misc. transcribed)\n",
    "saf_man_docs_path = int_corp_path + \"/xml_notes_stage3_final/qajar_xml\"\n",
    "\n",
    "# Misc Documents (misc. transcribed)\n",
    "misc_man_docs_path = int_corp_path + \"/xml_notes_stage3_final/misc_xml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unorganized Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted to XML, pre-sorted, Stage 2\n",
    "parser_xml_path = int_corp_path + \"/xml_notes_stage2/parser_depository\"\n",
    "\n",
    "# Converted to XML, pre-sorted, Stage 3\n",
    "updated_docs_path = int_corp_path + \"/xml_notes_stage3_final/updater_repository\"\n",
    "\n",
    "# Old system, yet to update\n",
    "xml_old_sys_path = int_corp_path + \"/xml_notes_stage2/xml_transcriptions_old_system\"\n",
    "\n",
    "# Markdown stage\n",
    "markdown_path = int_corp_path + \"/transcription_markdown_drafting_stage1\"\n",
    "\n",
    "# Markdown backlog (old system)\n",
    "md_backlog_path = int_corp_path + \"/transcription_markdown_drafting_stage1/document_conversion_backlog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Globbing Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-existing Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indic Narrative\n",
    "Thackston corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indo_corpus_files = glob.glob(indo_path + r'//**/*.txt', recursive=True)\n",
    "\n",
    "indo_corpus = {}\n",
    "for longname in indo_corpus_files:\n",
    "    with open(longname) as f:\n",
    "        txt = f.read()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    indo_corpus[short[0]] = txt\n",
    "    \n",
    "#indo_corpus.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transoxania Narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['khumuli', 'samarat', 'ikromcha', 'proshenie_k_general-gubernatory_ser721', 'rasskaz_praviteli_shahrisabz_ser724', 'prisoedineniia_samarkand_ser723', 'damla_abid_akhund_ser722', 'tarikh-i_jadida_tashkent_ser725', 'tuhfa-ahli-bukhara_ser25', 'darbandi_alexiii_coronation_ser728', 'tuhfa-i_taib_ser726'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_corpus_files = glob.glob(trans_path + r'//**/*.txt', recursive=True)\n",
    "\n",
    "trans_corpus = {}\n",
    "for longname in trans_corpus_files:\n",
    "    with open(longname) as f:\n",
    "        txt = f.read()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    trans_corpus[short[0]] = txt\n",
    "    \n",
    "    \n",
    "trans_corpus.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persian Literature\n",
    "*See below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Khiva Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "khiva_corpus_files = glob.glob(khiva_path + r'//**/*.txt', recursive=True)\n",
    "\n",
    "khiva_corpus = {}\n",
    "for longname in khiva_corpus_files:\n",
    "    with open(longname) as f:\n",
    "        txt = f.read()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    khiva_corpus[short[0]] = txt\n",
    "    \n",
    "    \n",
    "#khiva_corpus.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turkic Documents\n",
    "*TBD*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muscovite Persian diplomatic documents\n",
    "*TBD*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-created Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: need to update processes below to reflect new file organization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indic Narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "indo_man_files = glob.glob(indo_man_path + r'//**/*.txt', recursive=True)\n",
    "\n",
    "indo_man = {}\n",
    "for longname in indo_man_files:\n",
    "    with open(longname) as f:\n",
    "        txt = f.read()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    indo_man[short[0]] = txt\n",
    "    \n",
    "    \n",
    "#indo_man.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transoxania Narrative\n",
    "Corpus based on partially transcribed manuscripts from early modern Transoxania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_man_files = glob.glob(trans_man_path + r'//**/*.txt', recursive=True)\n",
    "\n",
    "trans_man = {}\n",
    "for longname in trans_man_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    trans_man[short[0]] = txt\n",
    "\n",
    "#trans_man.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transoxania Documents\n",
    "Qushbegi documents at XML stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ser934', 'ser89'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_man_doc_files = glob.glob(trans_man_docs_path + r'/*.xml')\n",
    "\n",
    "trans_man_docs = {}\n",
    "for longname in trans_man_doc_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    trans_man_docs[short[0]] = txt\n",
    "\n",
    "trans_man_docs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyderabad Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyderabad Documents\n",
    "\n",
    "hyd_man_doc_files = glob.glob(hyd_man_docs_path + r'/*.xml')\n",
    "\n",
    "hyd_man_docs = {}\n",
    "for longname in hyd_man_doc_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    hyd_man_docs[short[0]] = txt\n",
    "\n",
    "hyd_man_docs.keys()\n",
    "#Note: nothing in that folder yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indic Documents\n",
    "Misc. Indic documents other than those from the Nizam State collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ser935', 'ser936'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_man_doc_files = glob.glob(indo_man_docs_path + r'/*.xml')\n",
    "\n",
    "ind_man_docs = {}\n",
    "for longname in ind_man_doc_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    ind_man_docs[short[0]] = txt\n",
    "\n",
    "ind_man_docs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unorganized Documents\n",
    "E.g. documents still at the markdown stage, and not yet sorted by region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML, pre-sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ser193', 'ser187', 'ser811', 'ser596', 'ser970', 'ser958', 'ser179', 'ser812', 'ser621', 'ser972', 'ser967', 'ser973', 'ser813', 'ser817', 'ser963', 'ser988', 'ser989', 'ser816', 'ser814', 'ser960', 'ser237', 'ser961', 'ser626', 'ser183', 'ser815', 'ser906', 'ser537', 'ser898', 'ser1004', 'ser1006', 'ser939', 'ser84', 'ser905', 'ser904', 'ser85', 'ser938', 'ser91', 'ser1003', 'ser81', 'ser80', 'ser929', 'ser108', 'ser877', 'ser903', 'ser97', 'ser902', 'ser876', 'ser106', 'ser105', 'ser72', 'ser501', 'ser110', 'ser706', 'ser842', 'ser937', 'ser843', 'ser857', 'ser818', 'ser944', 'ser993', 'ser561', 'ser212', 'ser560', 'ser945', 'ser979', 'ser990', 'ser991', 'ser952', 'ser215', 'ser809', 'ser808'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_presort_files = glob.glob(parser_xml_path + r'/*.xml')\n",
    "\n",
    "xml_presort_docs = {}\n",
    "for longname in xml_presort_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    xml_presort_docs[short[0]] = txt\n",
    "\n",
    "xml_presort_docs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_updated_files = glob.glob(updated_docs_path + r'/*.xml')\n",
    "\n",
    "xml_updated_docs = {}\n",
    "for longname in xml_updated_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    xml_updated_docs[short[0]] = txt\n",
    "\n",
    "xml_updated_docs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML, old system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['NLR_f-940_ser190', 'IVANUz_1936_ser185', 'TsGARUz_R-2678_ser184', 'TsGARUz_i126_1_1990_20_ser186', 'TsGARUZ_i126_1_1729_101_ser213', 'TsGARUZ_i126_1_1730_81_ser227', 'TsGARUZ_i126_1_1986_1_ser201', 'TsGARUz_i126_1_1730_19_ser218', 'TsGARUz_i126-1-938-2_ser82', 'TsGARUZ_i126_1_1990_3_ser192', 'TsGARUz_i126_1_1730_2_ser188', 'TsGARUz_i126_1_1730_22_ser217', 'RGVIA_400-1-1015_ser143'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_oldsys_files = glob.glob(xml_old_sys_path + r'//**/*.xml', recursive=True)\n",
    "\n",
    "xml_oldsys_docs = {}\n",
    "for longname in xml_oldsys_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    xml_oldsys_docs[short[0]] = txt\n",
    "\n",
    "xml_oldsys_docs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling XML Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ser935', 'ser936', 'ser934', 'ser89', 'NLR_f-940_ser190', 'IVANUz_1936_ser185', 'TsGARUz_R-2678_ser184', 'TsGARUz_i126_1_1990_20_ser186', 'TsGARUZ_i126_1_1729_101_ser213', 'TsGARUZ_i126_1_1730_81_ser227', 'TsGARUZ_i126_1_1986_1_ser201', 'TsGARUz_i126_1_1730_19_ser218', 'TsGARUz_i126-1-938-2_ser82', 'TsGARUZ_i126_1_1990_3_ser192', 'TsGARUz_i126_1_1730_2_ser188', 'TsGARUz_i126_1_1730_22_ser217', 'RGVIA_400-1-1015_ser143', 'ser193', 'ser187', 'ser811', 'ser596', 'ser970', 'ser958', 'ser179', 'ser812', 'ser621', 'ser972', 'ser967', 'ser973', 'ser813', 'ser817', 'ser963', 'ser988', 'ser989', 'ser816', 'ser814', 'ser960', 'ser237', 'ser961', 'ser626', 'ser183', 'ser815', 'ser906', 'ser537', 'ser898', 'ser1004', 'ser1006', 'ser939', 'ser84', 'ser905', 'ser904', 'ser85', 'ser938', 'ser91', 'ser1003', 'ser81', 'ser80', 'ser929', 'ser108', 'ser877', 'ser903', 'ser97', 'ser902', 'ser876', 'ser106', 'ser105', 'ser72', 'ser501', 'ser110', 'ser706', 'ser842', 'ser937', 'ser843', 'ser857', 'ser818', 'ser944', 'ser993', 'ser561', 'ser212', 'ser560', 'ser945', 'ser979', 'ser990', 'ser991', 'ser952', 'ser215', 'ser809', 'ser808'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merges\n",
    "\n",
    "## All final stage XML documents\n",
    "combo_xml_final = {**ind_man_docs, **hyd_man_docs, **trans_man_docs}\n",
    "## All XML all stages\n",
    "combo_xml_all = {**combo_xml_final, **xml_oldsys_docs, **xml_presort_docs, **xml_updated_docs}\n",
    "\n",
    "\n",
    "combo_xml_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to pickle sub-directories of unsorted XML files\n",
    "with open(pickle_path + \"/xml_corpora.pkl\", \"wb\") as f:\n",
    "    pickle.dump((ind_man_docs, hyd_man_docs, trans_man_docs,\\\n",
    "                combo_xml_final, combo_xml_all), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markdown Stage\n",
    "Transcribed docs, yet to be ported over to XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_files = glob.glob(markdown_path + r'/*.xml')\n",
    "\n",
    "markdown_docs = {}\n",
    "for longname in markdown_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    markdown_docs[short[0]] = txt\n",
    "\n",
    "markdown_docs.keys()\n",
    "#Will be empty if everything was recently parsed and transfered, per workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markdown, old system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['apsa_119', 'apsa_520', 'apsa_534', 'apsa_118', 'apsa_536', 'apsa_527', 'apsa_533', 'apsa_532', 'apsa_526', 'apsa_530', 'apsa_524', 'apsa_531', 'apsa_109', 'apsa_556', 'apsa_557', 'apsa_555', 'apsa_554', 'apsa_550', 'apsa_551', 'apsa_545', 'apsa_69', 'apsa_553', 'apsa_546', 'apsa_552', 'apsa_70', 'apsa_549', 'apsa_71', 'apsa_76', 'apsa_77', 'apsa_113', 'apsa_107', 'apsa_117', 'apsa_115', 'apsa_511', 'apsa_114', 'tsgaruz_i126_1_1897_3_181', 'tsgaruz_i126_1_1953_2_86', 'tsgaruz_i_323_1_749_99', 'tsgaruz_i126_1_1867_3_90', 'tsgaruz_i126_2_317_1_74', 'tsgaruz_i_323_1_581_102', 'tsgaruz_i126_1_1953_5_88', 'tsgaruz_i126_1_1756_2_79', 'tsgaruz_i126_1_1953_4_87', 'tsgaruz_i_323_1_53_98', 'perepiska_glavnogo_shtaba_o_vnutripoliticheskom_polozhenii_v_bukhare_i_afganistane_rgvia_483_1_132', 'tsgaruz_i126_1_1867_5_93', 'tsgaruz_i126_1_1897_1_180', 'tsgaruz_i126_1_1906_1_130', 'tsgaruz_i_323_1_1125_101', 'tsgaruz_i126_1_1953_1_111', 'tsgaruz_i_323_1_1171_100', 'tsgaruz_i126_1_1990_1_177', 'tsgaruz_i126_1_1892_1_83', 'po_khodaistvu_byvshego_kashgarskogo_pravitelia_bek_kuli_beka_o_vydache_emu_posobiia_rgvia_400_1_2168', 'tsgaruz_i126_1_931_1_78', 'tsgaruz_i126_2_317_2_75', 'tsgaruz_i126_1_230_1_72', 'tsgaruz_ i126-1-1904-4_ser518', 'tsgaruz_ i126-1-1906-3_ser558', 'tsgaruz_i126_1_938_2_82', 'tsgaruz_ i126-1-1903-27_ser523', 'tsgaruz_i126_1_522_2_73', 'tsgaruz_i_323_1_751_103', 'rgia_1396-1-342_samarqand_madrasas_ser223', 'tsgaruz_i126_1_1867_13_96', 'tsgaruz_i126_1_1903_1_178', 'tsgaruz_i126_1_1898_17_182', 'tsgaruz_i126_1_1867_8_95', 'khorezmian_student_dispute_over_bukharan_madrasa_1910_i126_2_317', 'tsgaruz_i126_1_1867_6_94', 'tsgaruz_i126_1_1867_4_92', 'tsgaruz_i126-1-1900-1_ser503', 'rossiskaia_natsionalnaia_biblioteka_st_petersburg_index'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_old_files = glob.glob(md_backlog_path + r'//**/*.txt', recursive=True)\n",
    "\n",
    "markdown_old_docs = {}\n",
    "for longname in markdown_old_files:\n",
    "    f = open(longname)\n",
    "    txt = f.read()\n",
    "    f.close()\n",
    "    start = os.path.basename(longname)\n",
    "    short = os.path.splitext(start)\n",
    "    markdown_old_docs[short[0]] = txt\n",
    "\n",
    "markdown_old_docs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persian Literature Digital Corpus\n",
    "Massive corpus of Persian literature, pulled from Ganjur (http://ganjoor.net/) by Roshan (https://persdigumd.github.io/PDL/)\n",
    "\n",
    "*Corpus pre-cleaned, tokenized, and pickled from a separate script. (Cleaning takes a long time; and this corpus doesn't change very often, and so does not need to be re-run.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(perslit_path + '/persian_lit_toks.pkl', 'rb') \n",
    "\n",
    "pers_lit_toks = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pers_lit_toks.keys()\n",
    "#pers_lit_toks[\"hafez.masnavi\"][:50]\n",
    "#pers_lit_toks['ferdowsi.shahnameh']\n",
    "\n",
    "#type (pers_lit_toks['ferdowsi.shahnameh'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning edited texts and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible to do this once by iterating over the following? crashed computer last time...\n",
    "\n",
    "# indo_corpus, trans_corpus, khiva_corpus\n",
    "# indo_man, trans_man\n",
    "# trans_man_docs, hyd_man_docs, ind_man_docs\n",
    "# xml_presort_docs, xml_oldsys_docs, markdown_docs, markdown_old_docs\n",
    "\n",
    "\n",
    "clean_indo = {fn: ac.clean_document(doc) for fn, doc in indo_corpus.items()}\n",
    "clean_trans = {fn: ac.clean_document(doc) for fn, doc in trans_corpus.items()}\n",
    "clean_khiva = {fn: ac.clean_document(doc) for fn, doc in khiva_corpus.items()}\n",
    "\n",
    "clean_indo_man = {fn: ac.clean_document(doc) for fn, doc in indo_man.items()}\n",
    "clean_trans_man = {fn: ac.clean_document(doc) for fn, doc in trans_man.items()}\n",
    "\n",
    "clean_trans_man_docs = {fn: ac.clean_document(doc) for fn, doc in trans_man_docs.items()}\n",
    "clean_hyd_man_docs = {fn: ac.clean_document(doc) for fn, doc in hyd_man_docs.items()}\n",
    "clean_ind_man_docs = {fn: ac.clean_document(doc) for fn, doc in ind_man_docs.items()}\n",
    "\n",
    "clean_xml_presort_docs = {fn: ac.clean_document(doc) for fn, doc in xml_presort_docs.items()}\n",
    "clean_xml_oldsys_docs = {fn: ac.clean_document(doc) for fn, doc in xml_oldsys_docs.items()}\n",
    "clean_markdown_docs = {fn: ac.clean_document(doc) for fn, doc in markdown_docs.items()}\n",
    "clean_markdown_old_docs = {fn: ac.clean_document(doc) for fn, doc in markdown_old_docs.items()}\n",
    "\n",
    "\n",
    "\n",
    "#clean_trans['ikromcha'][:1000]\n",
    "#clean_trans['ikromcha'][:1000]\n",
    "\n",
    "\n",
    "#clean_xml['ser561']\n",
    "\n",
    "#clean_indo['mu_vol1'][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apparently this dependency is needed for below\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# External Corpora Toks\n",
    "\n",
    "indo_nar_ext_toks = {}\n",
    "for (fn, txt) in clean_indo.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    indo_nar_ext_toks[fn] = toks\n",
    "\n",
    "trans_nar_ext_toks = {}\n",
    "for (fn, txt) in clean_trans.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    trans_nar_ext_toks[fn] = toks \n",
    "    \n",
    "khiva_doc_toks = {}\n",
    "for (fn, txt) in clean_khiva.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    khiva_doc_toks[fn] = toks\n",
    "\n",
    "    \n",
    "# Manually Entered Manuscript Toks\n",
    "\n",
    "indo_nar_toks = {}\n",
    "for (fn, txt) in clean_indo_man.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    indo_nar_toks[fn] = toks\n",
    "    \n",
    "trans_nar_toks = {}\n",
    "for (fn, txt) in clean_trans_man.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    trans_nar_toks[fn] = toks\n",
    "\n",
    "# Clean XML-stage Document Toks\n",
    " \n",
    "trans_xml_toks = {}\n",
    "for (fn, txt) in clean_trans_man_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    trans_xml_toks[fn] = toks\n",
    "    \n",
    "hyd_xml_toks = {}\n",
    "for (fn, txt) in clean_hyd_man_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    hyd_xml_toks[fn] = toks\n",
    "\n",
    "indo_xml_toks = {}\n",
    "for (fn, txt) in clean_ind_man_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    indo_xml_toks[fn] = toks\n",
    "\n",
    "\n",
    "# Unorganized Markdown-stage Toks\n",
    "\n",
    "\n",
    "presort_xml_toks = {}\n",
    "for (fn, txt) in clean_xml_presort_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    presort_xml_toks[fn] = toks\n",
    "    \n",
    "oldsys_xml_toks = {}\n",
    "for (fn, txt) in clean_xml_oldsys_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    oldsys_xml_toks[fn] = toks\n",
    "    \n",
    "md_stage_toks = {}\n",
    "for (fn, txt) in clean_markdown_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    md_stage_toks[fn] = toks\n",
    "\n",
    "md_oldsys_toks = {}\n",
    "for (fn, txt) in clean_markdown_old_docs.items():\n",
    "    toks = nltk.word_tokenize(txt)\n",
    "    md_oldsys_toks[fn] = toks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First-stage combinations*: Collapse unsorted documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsorted_doc_toks = {**presort_xml_toks, **oldsys_xml_toks, **md_stage_toks, **md_oldsys_toks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsorted_doc_toks['ser560']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = hdir + \"/Dropbox/Active_Directories/Digital_Humanities/Corpora/pickled_tokenized_cleaned_corpora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_path + \"/corpora.pkl\", \"wb\") as f:\n",
    "    pickle.dump((unsorted_doc_toks,\\\n",
    "                indo_xml_toks, hyd_xml_toks, trans_xml_toks,\\\n",
    "                trans_nar_toks, indo_nar_toks,\\\n",
    "                trans_nar_ext_toks, indo_nar_ext_toks, khiva_doc_toks), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trans_nar_toks[\"ziyarat_bukhara_kazan_manuscript_ser492\"]\n",
    "\n",
    "df = pd.DataFrame (trans_nar_toks[\"ziyarat_bukhara_kazan_manuscript_ser492\"], columns=['Token'])\n",
    "df['Text']='title'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Formation: Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Corpora Toks\n",
    "\n",
    "concat_indo_nar_ext_toks = sum([[(\"indo_nar_ext_toks\", text, idx, tok) for idx, tok in enumerate(toks)] for text, toks in indo_nar_ext_toks.items()], [])\n",
    "\n",
    "concat_trans_nar_ext_toks = sum([[(\"trans_nar_ext_toks\", text, idx, tok) for idx, tok in enumerate(toks)] for text, toks in trans_nar_ext_toks.items()], [])\n",
    "\n",
    "concat_khiva_doc_toks = sum([[(\"khiva_doc_toks\", text, idx, tok) for idx, tok in enumerate(toks)] for text, toks in khiva_doc_toks.items()], [])\n",
    "\n",
    "\n",
    "    \n",
    "# Manually Entered Manuscript Toks\n",
    "\n",
    "concat_trans_nar = sum([[(\"trans_nar\", text, idx, tok) for idx, tok in enumerate(toks)] for text, toks in trans_nar_toks.items()], [])\n",
    "\n",
    "concat_indo_nar = sum([[(\"indo_nar\", text, idx, tok) for idx, tok in enumerate(toks)] for text, toks in indo_nar_toks.items()], [])\n",
    "\n",
    "\n",
    "# Clean XML-stage Document Toks\n",
    " \n",
    "concat_trans_xml_toks = sum([[(\"trans_xml_toks\", text, idx, tok) for idx, tok in enumerate(toks)] for text, toks in trans_xml_toks.items()], [])\n",
    "\n",
    "concat_hyd_xml_toks = sum([[(\"hyd_xml_toks\", text, idx, tok) for idx, tok in enumerate(toks)] for text, toks in hyd_xml_toks.items()], [])\n",
    "\n",
    "concat_indo_xml_toks = sum([[(\"indo_xml_toks\", text, idx, tok) for idx, tok in enumerate(toks)] for text, toks in indo_xml_toks.items()], [])\n",
    "\n",
    "\n",
    "\n",
    "# Unorganized Markdown-stage Toks\n",
    "\n",
    "concat_presort_xml_toks = sum([[(\"presort_xml_toks\", text, idx, tok) for idx, tok in enumerate(toks)] for text, toks in presort_xml_toks.items()], [])\n",
    "\n",
    "concat_trans_nar = sum([[(\"oldsys_xml_toks\", text, idx, tok) for idx, tok in enumerate(toks)] for text, toks in oldsys_xml_toks.items()], [])\n",
    "\n",
    "concat_md_stage_toks = sum([[(\"md_stage_toks\", text, idx, tok) for idx, tok in enumerate(toks)] for text, toks in md_stage_toks.items()], [])\n",
    "\n",
    "concat_md_oldsys_toks = sum([[(\"md_oldsys_toks\", text, idx, tok) for idx, tok in enumerate(toks)] for text, toks in md_oldsys_toks.items()], [])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persian Lit\n",
    "\n",
    "concat_pers_lit_toks = sum([[(\"pers_lit_toks\", text, idx, tok) for idx, tok in enumerate(toks)] for text, toks in pers_lit_toks.items()], [])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just delete the 'test' column above; do the different categories separately, manually specifying the category\n",
    "# then just concat them all together at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat_indo_nar[0:10]\n",
    "\n",
    "concat = \\\n",
    "concat_indo_nar_ext_toks + concat_trans_nar_ext_toks + concat_khiva_doc_toks + \\\n",
    "concat_trans_nar + concat_indo_nar + \\\n",
    "concat_trans_xml_toks + concat_hyd_xml_toks + concat_indo_xml_toks +\\\n",
    "concat_presort_xml_toks + concat_trans_nar + concat_md_stage_toks + concat_md_oldsys_toks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = concat + concat_pers_lit_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(concat, columns = [\"Category\", \"Text\", \"No\", \"Token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>No</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5320388</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>anvari.divan</td>\n",
       "      <td>11499</td>\n",
       "      <td>جان</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Category          Text     No Token\n",
       "5320388  pers_lit_toks  anvari.divan  11499   جان"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(pickle_path,r'eurasia_corpus.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>No</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>indo_nar_ext_toks</td>\n",
       "      <td>sjn1</td>\n",
       "      <td>5</td>\n",
       "      <td>خنده</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>indo_nar_ext_toks</td>\n",
       "      <td>sjn1</td>\n",
       "      <td>6</td>\n",
       "      <td>ریزی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>indo_nar_ext_toks</td>\n",
       "      <td>sjn1</td>\n",
       "      <td>7</td>\n",
       "      <td>گلبن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>indo_nar_ext_toks</td>\n",
       "      <td>sjn1</td>\n",
       "      <td>8</td>\n",
       "      <td>سخن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>indo_nar_ext_toks</td>\n",
       "      <td>sjn1</td>\n",
       "      <td>9</td>\n",
       "      <td>از</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Category  Text  No Token\n",
       "5  indo_nar_ext_toks  sjn1   5  خنده\n",
       "6  indo_nar_ext_toks  sjn1   6  ریزی\n",
       "7  indo_nar_ext_toks  sjn1   7  گلبن\n",
       "8  indo_nar_ext_toks  sjn1   8   سخن\n",
       "9  indo_nar_ext_toks  sjn1   9    از"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[5:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
