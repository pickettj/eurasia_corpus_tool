{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de196cbe",
   "metadata": {},
   "source": [
    "# Comparing Verb Forms in Modern Tajik (Newspaper Corpus) with Bukhari Persian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71aef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78695319",
   "metadata": {},
   "source": [
    "Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af297b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set home directory path\n",
    "hdir = os.path.expanduser('~')\n",
    "\n",
    "# Tajik corpus directory\n",
    "taj_path = os.path.join(hdir, \"Dropbox/Active_Directories/Digital_Humanities/Corpora/tajik_newspaper_corpus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ef43c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold data\n",
    "data = []\n",
    "\n",
    "# Walk through the directory structure\n",
    "for subdir, dirs, files in os.walk(taj_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            subdir_name = os.path.basename(subdir)\n",
    "            data.append({'sub_directory': subdir_name, 'filename': file, 'content': content})\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b955566a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141572"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of Tajik newspaper articles\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c740e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_directory</th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55253</th>\n",
       "      <td>Ozodi</td>\n",
       "      <td>ozodi_2019-03-22-1.txt</td>\n",
       "      <td>–Ø–∫ –º–∞—ä–º—É—Ä–∏ –º–∏–ª–∏—Å–∞—Ä–æ –±–∞—Ä–æ–∏ –ª–∞—Ç—Ç—É –∫”Ø–±–∏ –∑–∞–Ω–µ –¥–∞—Ä ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107271</th>\n",
       "      <td>Khovar</td>\n",
       "      <td>Khovar_2022-09-25-6.txt</td>\n",
       "      <td>–î–∏—Ä”Ø–∑, —Å–µ–Ω—Ç—è–±—Ä –í–∞–∑–∏—Ä–∏ –∫–æ—Ä“≥–æ–∏ —Ö–æ—Ä–∏“∑–∏–∏ –¢–æ“∑–∏–∫–∏—Å—Ç–æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80687</th>\n",
       "      <td>Pressa.tj</td>\n",
       "      <td>pressatj_28.01.2019_4.txt</td>\n",
       "      <td>–¢–∞–≤—Ä–µ –∞–∑ –º–∞—Ä–∫–∞–∑–∏ “õ–∞–±—É–ª–∏ –±—é–ª–ª–µ—Ç–µ–Ω“≥–æ, —Ç–∞“õ—Å–∏–º–æ—Ç –≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133107</th>\n",
       "      <td>Ovozi_Tojik</td>\n",
       "      <td>ovozi_tojik_2022-09-09-13.txt</td>\n",
       "      <td>–î–∞—Ä —á–æ—Ä–∞–±–∏–Ω”£ –Ω–∞–º–æ—è–Ω–¥–∞–≥–æ–Ω–∏ –Ω–∞—Å–ª“≥–æ–∏ –≥—É–Ω–æ–≥—É–Ω–∏ –º—É“≥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125918</th>\n",
       "      <td>Khovar</td>\n",
       "      <td>khovar_2020-05-28-18.txt</td>\n",
       "      <td>–î–£–®–ê–ù–ë–ï, 28.05.2020 /–ê–ú–ò–¢ ¬´–•–æ–≤–∞—Ä¬ª/. 28 –º–∞–π –ê—Å–æ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sub_directory                       filename  \\\n",
       "55253          Ozodi         ozodi_2019-03-22-1.txt   \n",
       "107271        Khovar        Khovar_2022-09-25-6.txt   \n",
       "80687      Pressa.tj      pressatj_28.01.2019_4.txt   \n",
       "133107   Ovozi_Tojik  ovozi_tojik_2022-09-09-13.txt   \n",
       "125918        Khovar       khovar_2020-05-28-18.txt   \n",
       "\n",
       "                                                  content  \n",
       "55253   –Ø–∫ –º–∞—ä–º—É—Ä–∏ –º–∏–ª–∏—Å–∞—Ä–æ –±–∞—Ä–æ–∏ –ª–∞—Ç—Ç—É –∫”Ø–±–∏ –∑–∞–Ω–µ –¥–∞—Ä ...  \n",
       "107271  –î–∏—Ä”Ø–∑, —Å–µ–Ω—Ç—è–±—Ä –í–∞–∑–∏—Ä–∏ –∫–æ—Ä“≥–æ–∏ —Ö–æ—Ä–∏“∑–∏–∏ –¢–æ“∑–∏–∫–∏—Å—Ç–æ...  \n",
       "80687   –¢–∞–≤—Ä–µ –∞–∑ –º–∞—Ä–∫–∞–∑–∏ “õ–∞–±—É–ª–∏ –±—é–ª–ª–µ—Ç–µ–Ω“≥–æ, —Ç–∞“õ—Å–∏–º–æ—Ç –≤...  \n",
       "133107  –î–∞—Ä —á–æ—Ä–∞–±–∏–Ω”£ –Ω–∞–º–æ—è–Ω–¥–∞–≥–æ–Ω–∏ –Ω–∞—Å–ª“≥–æ–∏ –≥—É–Ω–æ–≥—É–Ω–∏ –º—É“≥...  \n",
       "125918  –î–£–®–ê–ù–ë–ï, 28.05.2020 /–ê–ú–ò–¢ ¬´–•–æ–≤–∞—Ä¬ª/. 28 –º–∞–π –ê—Å–æ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5149652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tajik_newspaper_corpus.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198cf1e",
   "metadata": {},
   "source": [
    "### Regex search pattern for Tajik newspapers\n",
    "\n",
    "`–º–µ` marks the beginning of the participle (unlike ŸÖ€å in Persian, there it is always attached to the verb without a space); then comes the verb participle; which ends with `–¥–∞–≥”£`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0fcc37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "taj_medagi_pattern = r'\\s–º–µ(?:[^\\s]*\\s){0,2}[^\\s]{0,15}–≥”£(?=\\s|$)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f82f6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame and select only rows where 'content' column matches the pattern\n",
    "filtered_df = df[df['content'].str.contains(taj_medagi_pattern, regex=True, na=False)]\n",
    "\n",
    "# Keep only the columns where 'content' column matches the pattern\n",
    "filtered_df = filtered_df[['sub_directory', 'filename', 'content']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c176b4a0-d153-4843-b702-38e9f5fc8205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_with_pattern(df, pattern):\n",
    "    \"\"\"\n",
    "    Extract sentences containing the regex pattern from the dataframe\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'content' column\n",
    "        pattern: regex pattern to search for\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with additional columns for matched sentences\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        content = row['content']\n",
    "        \n",
    "        # Split content into sentences (simple approach using common sentence endings)\n",
    "        # This handles Cyrillic punctuation as well\n",
    "        sentences = re.split(r'[.!?÷â]\\s+', content)\n",
    "        \n",
    "        # Find sentences that contain the pattern\n",
    "        matching_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if re.search(pattern, sentence):\n",
    "                # Clean up the sentence (remove extra whitespace)\n",
    "                clean_sentence = ' '.join(sentence.split())\n",
    "                matching_sentences.append(clean_sentence)\n",
    "        \n",
    "        # If we found matching sentences, add them to results\n",
    "        if matching_sentences:\n",
    "            for sentence in matching_sentences:\n",
    "                results.append({\n",
    "                    'sub_directory': row['sub_directory'],\n",
    "                    'filename': row['filename'],\n",
    "                    'matching_sentence': sentence\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ebe07c8-a731-4bf7-8141-7bf2dcaba789",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_df = extract_sentences_with_pattern(df, taj_medagi_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c717541e-bc16-4061-bbfb-d9326ed1a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sub_directory                      filename  \\\n",
      "0                Oila         oila_2022-12-31-2.txt   \n",
      "1  Javonon_Tojikiston      javonon_13_–ò—é–Ω_20173.txt   \n",
      "2  Javonon_Tojikiston     javonon_11_–ú–∞–∏ÃÜ_20231.txt   \n",
      "3         osiyoavrupo  osiyoavrupo_2022_03_19_0.txt   \n",
      "4         osiyoavrupo  osiyoavrupo_2022_03_19_0.txt   \n",
      "\n",
      "                                   matching_sentence  \n",
      "0  –î–∞—Ä –º—É–Ω–æ—Å–∏–±–∞—Ç“≥–æ–∏ –æ—à“õ–æ–Ω–∞–∏ –•–∞–Ω–¥–∞ –≤–∞ –ö–∞—Ä–∏–º “≥–∞–Ω”Ø–∑ ...  \n",
      "1  –ê–∫–Ω—É–Ω “õ–∞—Ç—ä”£ —Ç–∞–ª–∞–± –∫–∞—Ä–¥: - –ú–∞–Ω –±–∞ —Ç—É —á–∏–∑–∏ –º–µ—Ö”Ø—Ä...  \n",
      "2  –ê–º–º–æ –≤–æ–ª–∏–¥–æ–Ω –±–æ “≥–∞—Ä –±–∞“≥–æ–Ω–∞ –¥–∞—Ä –º–∞“∑–ª–∏—Å“≥–æ–∏ –ø–∞–¥–∞—Ä...  \n",
      "3  “≤–∞–Ω”Ø–∑ –ø–µ—à –∞–∑ —Ñ–∞—Ä–æ—Ä–∞—Å–∏–∏ –ù–∞–≤—Ä”Ø–∑ “õ–∞–∑–æ“õ“≥–æ –∞–Ω—ä–∞–Ω–∞–µ ...  \n",
      "4  –ë–∞ –∫–∞—Å–æ–Ω–µ, –∫–∏ —Ö—É—Å—É–º–∞—Ç –¥–æ—à—Ç–∞–Ω–¥, —Å—É–ª“≥—É –æ—à—Ç–∏—Ä–æ –±–∞...  \n"
     ]
    }
   ],
   "source": [
    "print(pat_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84fa3621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3285"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf1476fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141572"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0032c007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of total articles to articles with '–∏ –º–µ–¥–∞–≥”£': 0.02\n"
     ]
    }
   ],
   "source": [
    "ratio=len(filtered_df)/len(df)\n",
    "print(f\"Ratio of total articles to articles with '–∏ –º–µ–¥–∞–≥”£': {ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e89f213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def export_sentences_to_file(df, hdir=None):\n",
    "    \"\"\"Export all sentences to a text file with timestamp\"\"\"\n",
    "    import datetime\n",
    "    \n",
    "    # Use home directory if not provided\n",
    "    if hdir is None:\n",
    "        hdir = os.path.expanduser('~')\n",
    "    \n",
    "    # Create timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"taj_news_export_{timestamp}.txt\"\n",
    "    \n",
    "    # Create full path\n",
    "    full_path = os.path.join(hdir, \"Dropbox/Active_Directories/Inbox\", filename)\n",
    "    \n",
    "    # Export the sentences\n",
    "    with open(full_path, 'w', encoding='utf-8') as f:\n",
    "        for i, (idx, row) in enumerate(df.iterrows()):\n",
    "            f.write(f\"--- Example {i+1} ---\\n\")\n",
    "            f.write(f\"File: {row['filename']}\\n\")\n",
    "            f.write(f\"Directory: {row['sub_directory']}\\n\")\n",
    "            f.write(f\"Sentence: {row['matching_sentence']}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"Exported {len(df)} sentences to {full_path}\")\n",
    "    return full_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e65af8cc-dfce-4401-ae7a-e941bb53c301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 2807 sentences to /Users/pickettj/Dropbox/Active_Directories/Inbox/taj_news_export_20250815_103311.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/pickettj/Dropbox/Active_Directories/Inbox/taj_news_export_20250815_103311.txt'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_sentences_to_file(pat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fec9702-c74c-4762-be5f-31d9affacee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import historical_corpus_plain_text as hcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f55943ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loading corpus for the first time...\n",
      "Error parsing ser811.xml. Skipping this file.\n",
      "Error parsing ser812.xml. Skipping this file.\n",
      "Error parsing ser813.xml. Skipping this file.\n",
      "Error parsing ser817.xml. Skipping this file.\n",
      "Error parsing ser816.xml. Skipping this file.\n",
      "Error parsing ser814.xml. Skipping this file.\n",
      "Error parsing ser815.xml. Skipping this file.\n",
      "Error parsing ser1004.xml. Skipping this file.\n",
      "Error parsing ser904.xml. Skipping this file.\n",
      "Error parsing ser1003.xml. Skipping this file.\n",
      "Error parsing ser876.xml. Skipping this file.\n",
      "Error parsing ser842.xml. Skipping this file.\n",
      "Error parsing ser843.xml. Skipping this file.\n",
      "Error parsing ser857.xml. Skipping this file.\n",
      "Error parsing ser809.xml. Skipping this file.\n",
      "Error parsing ser808.xml. Skipping this file.\n",
      "‚úÖ Loaded 1066 documents\n",
      "üßπ Cleaning Arabic script...\n",
      "‚úÖ Cleaning complete\n"
     ]
    }
   ],
   "source": [
    "corpus_dict = hcp.get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b224c47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1066"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f97167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_corpus_by_pattern(corpus_dict, pattern=None):\n",
    "    \"\"\"\n",
    "    Filter corpus dictionary to keep only entries that contain at least one match\n",
    "    of the specified regex pattern.\n",
    "    \n",
    "    Args:\n",
    "        corpus_dict (dict): Dictionary where keys are filenames and values are text content\n",
    "        pattern (str, optional): Regex pattern to search for. If None, uses default Persian pattern.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Filtered dictionary containing only entries with matches\n",
    "        \n",
    "    Example:\n",
    "        filtered_corpus = filter_corpus_by_pattern(corpus_dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default pattern: ŸÖ€å + up to 15 chars (max 2 whitespaces) + ⁄Ø€å\n",
    "    if pattern is None:\n",
    "        # Pattern breakdown:\n",
    "        # ŸÖ€å - literal \"ŸÖ€å\"\n",
    "        # (?:[^\\s]*\\s){0,2} - non-capturing group for non-whitespace chars followed by space, 0-2 times\n",
    "        # [^\\s]{0,15} - up to 15 non-whitespace characters at the end\n",
    "        # ⁄Ø€å - literal \"⁄Ø€å\"\n",
    "        pattern = r'ŸÖ€å(?:[^\\s]*\\s){0,2}[^\\s]{0,15}⁄Ø€å'\n",
    "    \n",
    "    filtered_dict = {}\n",
    "    \n",
    "    for filename, text_content in corpus_dict.items():\n",
    "        # Search for the pattern in the text content\n",
    "        if re.search(pattern, text_content):\n",
    "            filtered_dict[filename] = text_content\n",
    "    \n",
    "    return filtered_dict\n",
    "\n",
    "def get_pattern_matches(corpus_dict, pattern=None, context_chars=30):\n",
    "    \"\"\"\n",
    "    Get all pattern matches with context from the corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus_dict (dict): Dictionary where keys are filenames and values are text content\n",
    "        pattern (str, optional): Regex pattern to search for\n",
    "        context_chars (int): Number of characters to include before/after each match\n",
    "    \n",
    "    Returns:\n",
    "        dict: Nested dictionary with matches and their context\n",
    "    \"\"\"\n",
    "    \n",
    "    if pattern is None:\n",
    "        pattern = r'\\sŸÖ€å(?:[^\\s]*\\s){0,2}[^\\s]{0,15}⁄Ø€å(?=\\s|$)'\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for filename, text_content in corpus_dict.items():\n",
    "        matches = re.finditer(pattern, text_content)\n",
    "        file_matches = {}\n",
    "        \n",
    "        for i, match in enumerate(matches, 1):\n",
    "            start_idx = max(0, match.start() - context_chars)\n",
    "            end_idx = match.end() + context_chars\n",
    "            context = text_content[start_idx:end_idx]\n",
    "            \n",
    "            match_key = f\"match_{i}_{match.group()}\"\n",
    "            file_matches[match_key] = context\n",
    "        \n",
    "        if file_matches:  # Only add files that have matches\n",
    "            results[filename] = file_matches\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# filtered_corpus = filter_corpus_by_pattern(corpus_dict)\n",
    "# print(f\"Original corpus: {len(corpus_dict)} documents\")\n",
    "# print(f\"Filtered corpus: {len(filtered_corpus)} documents\")\n",
    "\n",
    "# To get matches with context:\n",
    "# matches_with_context = get_pattern_matches(corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b9725be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original corpus: 1066 documents\n",
      "Filtered corpus: 198 documents\n"
     ]
    }
   ],
   "source": [
    "# Filter your corpus\n",
    "filtered_corpus = filter_corpus_by_pattern(corpus_dict)\n",
    "\n",
    "# Check results\n",
    "print(f\"Original corpus: {len(corpus_dict)} documents\")\n",
    "print(f\"Filtered corpus: {len(filtered_corpus)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c74e257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 1066\n",
      "Documents with pattern: 198\n",
      "Percentage: 18.57%\n"
     ]
    }
   ],
   "source": [
    "# Calculate percentage\n",
    "total_docs = len(corpus_dict)\n",
    "matching_docs = len(filtered_corpus)\n",
    "percentage = (matching_docs / total_docs) * 100\n",
    "\n",
    "print(f\"Total documents: {total_docs}\")\n",
    "print(f\"Documents with pattern: {matching_docs}\")\n",
    "print(f\"Percentage: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56243c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
