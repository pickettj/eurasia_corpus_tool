{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Deciphering Tool\n",
    "James Pickett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, re, nltk, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import DataFrame, Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general function in Pandas to set maximum number of rows; otherwise only shows a few\n",
    "\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set home directory path\n",
    "hdir = os.path.expanduser('~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sister files:\n",
    "- Pickled corpora cleaned in text_cleaning_tokenizing (optional: run `text_cleaning_tokenizing.py` - may take a few minutes to execute)\n",
    "- Corpora stats in corpora_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 'text_cleaning_tokenizing.py' to re-tokenize corpus\n"
     ]
    }
   ],
   "source": [
    "print (\"run 'text_cleaning_tokenizing.py' to re-tokenize corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_help ():\n",
    "    print (\"Eurasia Corpus Tool: Functions and Explanation\\n\\n\\\n",
    "           \\ttool_help: lists functions\\n\\\n",
    "           \\tlist_corpora: lists all of the sub-corpora options\\n\\\n",
    "           \\tfreq: returns the most likely terms matching the search term\\n\\\n",
    "           \\tindex_kwic: key word in context; optional arguments: 'category' to specify a specific corpus\\\n",
    "                   'exclusion=False' to also include the Persian literature corpus\\n\\\n",
    "           \\tmulti_dic: searches through all of the dictionary corpuses for the term\\n\\\n",
    "           \\tdic_freq: searches through dictionary corpuses, returns most frequent term appearing in the text corpuses\\n\\\n",
    "            \\n\\n\\n\"\n",
    "          )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tool_help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Corpora\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = hdir + \"/Dropbox/Active_Directories/Digital_Humanities/Corpora/pickled_tokenized_cleaned_corpora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe corpus\n",
    "\n",
    "df_eurcorp = pd.read_csv (os.path.join(pickle_path,r'eurasia_corpus.csv'))\n",
    "#df_eurcorp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_corpora():\n",
    "    categories = df_eurcorp['Category'].unique()\n",
    "    print (\"Corpora\\n:\", sorted(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset path\n",
    "\n",
    "ds_path = hdir + \"/Dropbox/Active_Directories/Digital_Humanities/Datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Von Melzer\n",
    "meltzer = pd.read_csv(ds_path + \"/von_melzer.csv\")\n",
    "#meltzer.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dehkhoda = pd.read_csv(ds_path + \"/dehkhoda_dictionary.csv\", names=['Term', 'Definition'])\n",
    "dehkhoda = dehkhoda.rename(columns={'Term': 'Emic_Term'})\n",
    "#dehkhoda.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations\n",
    "locations = pd.read_csv(ds_path + '/exported_database_data/locations.csv', names=['UID', 'Ar_Names', \\\n",
    "                                                'Lat_Name', 'Nickname', 'Type'])\n",
    "# Social Roles\n",
    "roles = pd.read_csv(ds_path + '/exported_database_data/roles.csv', names=['UID', 'Term', 'Emic', 'Etic', 'Scope'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glossary\n",
    "glossary = pd.read_csv(ds_path + '/exported_database_data/glossary.csv', names=['UID', 'Term', \\\n",
    "                                                'Eng_Term', 'Translation', 'Transliteration', 'Scope', 'Tags'])\n",
    "glossary = glossary.rename(columns={'Term': 'Emic_Term', 'Eng_Term':'Transliteration', 'Translation':'Definition'})\n",
    "#glossary.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Search Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify / standardize dictionaries\n",
    "\n",
    "simp_meltzer = meltzer.rename(columns={'Präs.-Stamm': 'Emic_Term', 'Transkription': 'Transcription', 'Deutsch':'Definition'})\\\n",
    "                .drop(['Volume', 'Unnamed: 2', 'Persisch',  'Bemerkung'], axis=1)\n",
    "simp_meltzer['Dataset']='meltzer'\n",
    "#simp_meltzer.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_dehkhoda = dehkhoda\n",
    "simp_dehkhoda['Dataset']='dehkhoda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_glossary = glossary\n",
    "simp_glossary['Dataset']='glossary'\n",
    "simp_glossary = simp_glossary.drop(['Transliteration'], axis=1)\n",
    "#simp_glossary.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dics = pd.concat([simp_dehkhoda, simp_glossary, simp_meltzer], axis=0)\n",
    "#concat_dics.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_dic (term):\n",
    "    query_mask = concat_dics[\"Emic_Term\"].str.contains(term, na=False)\n",
    "    query = concat_dics[query_mask]\n",
    "    result = query\n",
    "    \n",
    "    if len(result) > 50:\n",
    "        result = result.sample(50)\n",
    "    \n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi_dic(\"دار\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>Emic_Term</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Scope</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>80</td>\n",
       "      <td>چقر</td>\n",
       "      <td>street</td>\n",
       "      <td>transoxania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>glossary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    UID Emic_Term Definition        Scope Tags   Dataset\n",
       "78   80       چقر     street  transoxania  NaN  glossary"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term = \"چقر\"\n",
    "query_mask = simp_glossary[\"Emic_Term\"].str.contains(term, na=False)\n",
    "query1 = simp_glossary[query_mask]\n",
    "query1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emic_Term</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>UID</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>Quellenangaben</th>\n",
       "      <th>Scope</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>چقر</td>\n",
       "      <td>street</td>\n",
       "      <td>glossary</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>transoxania</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emic_Term Definition   Dataset   UID Transcription Quellenangaben  \\\n",
       "78       چقر     street  glossary  80.0           NaN            NaN   \n",
       "\n",
       "          Scope Tags  \n",
       "78  transoxania  NaN  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_dic('چقر')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?? how to return multiple dataframes without losing the dataframe pretty format\n",
    "# ++ need to use concat function for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom KWIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def index_kwic (term, category=None, exclusion=True):\n",
    "    \n",
    "    \"\"\"This function returns a dataframe filtered by the search term.\"\"\"\n",
    "    df_eurcorp_lite = df_eurcorp\n",
    "\n",
    "    # default state is to exclude the vast Persian literature corpus\n",
    "    if exclusion:\n",
    "        df_eurcorp_lite = df_eurcorp_lite[df_eurcorp_lite['Category'] != 'pers_lit_toks']\n",
    "    \n",
    "    # default state is to include all of the sub-corpora; but can also specify one of them\n",
    "    if category is not None:\n",
    "        df_eurcorp_lite = df_eurcorp_lite[df_eurcorp_lite['Category'] == category]\n",
    "\n",
    "        \n",
    "    result = df_eurcorp_lite[df_eurcorp_lite['Token'].str.match(term)]\n",
    "    return result\n",
    "    \n",
    "    # str.match; the str part is telling match how to behave; .match is a method specific to pandas\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index_kwic('اژدها', exclusion = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kwic (term, category=None, exclusion=True):\n",
    "    # optional arguments refer to index_kwic function\n",
    "    \n",
    "    for i, item in index_kwic(term, category, exclusion).iterrows():\n",
    "        \n",
    "        title = item[\"Text\"]\n",
    "        category = item[\"Category\"]\n",
    "        loc = item['No']\n",
    "        \n",
    "        filtered = df_eurcorp[(df_eurcorp['Text']==title)]\n",
    "        filt_toks = filtered[(filtered['No']>=(loc-5))&(filtered['No']<=(loc+5))]\n",
    "\n",
    "        \n",
    "        #filtered = filtered.sort_values(\"index\")\n",
    "        # probably already sorted, but better to be on the safe side\n",
    "                \n",
    "        text = \" \".join(filt_toks[\"Token\"])\n",
    "        \n",
    "        print(f'{title}: {loc} {category}\\n{text}\\n')\n",
    "        \n",
    "        # task: figure out how to color code results; termcolor package, has to be installed\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# iterrows(): research what this does exactly, has something to do with dataframes being composed of series\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#kwic('اژ.ها$', exclusion=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dic = pd.value_counts(df_eurcorp.Token).to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80965</th>\n",
       "      <td>پرخواری</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5728</th>\n",
       "      <td>بانو</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57547</th>\n",
       "      <td>بدانجام</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146803</th>\n",
       "      <td>حسروانه</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270</th>\n",
       "      <td>مطلق</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  Token\n",
       "80965   پرخواری      4\n",
       "5728       بانو    298\n",
       "57547   بدانجام      6\n",
       "146803  حسروانه      2\n",
       "2270       مطلق   1003"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dic.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq (term):\n",
    "    query_mask = freq_dic[\"index\"].str.contains(term, na=False)\n",
    "    query = freq_dic[query_mask]\n",
    "    result = query.head()\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#freq(\"اژد.ا\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addiing the frequency data into the dictionary\n",
    "\n",
    "# need to turn the transcribed word into a unique identifier (will lose some data):\n",
    "concat_dics_clean = concat_dics.drop_duplicates(subset=['Emic_Term'], keep='first')\n",
    "\n",
    "# merge with frequency data based on the unique identifier:\n",
    "merged_dics_freq = pd.merge(left=concat_dics_clean, right=freq_dic, how='left', left_on='Emic_Term', right_on='index')\n",
    "\n",
    "# clean up for readability\n",
    "cleaned_merged_dics_freq = merged_dics_freq.drop(columns=['index']).\\\n",
    "rename(columns={'Token': 'Frequency'})\n",
    "\n",
    "#cleaned_merged_dics_freq.sample(5)\n",
    "\n",
    "# ?? how to get rid of those decimal points / why did they enter in the first place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Definition with Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_freq (term):\n",
    "    query_mask = cleaned_merged_dics_freq[\"Emic_Term\"].str.contains(term, na=False)\n",
    "    query = cleaned_merged_dics_freq[query_mask]\n",
    "    result = query.sort_values('Frequency', ascending=False).head()\n",
    "    return (result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dic_freq('چ.ر$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confreq (term, group=False):\n",
    "    sel = df_eurcorp[df_eurcorp['Token']==term].copy()\n",
    "    sel['index_next'] = sel['No'] + 1\n",
    "    sel = sel.join(\n",
    "        df_eurcorp.set_index(['Text', 'Category', 'No'])['Token'].rename('token_next'),\n",
    "        on=['Text', 'Category', 'index_next']\n",
    "    )\n",
    "    # If there are only 1-frequency results, it will still show them;\n",
    "    # but if there are enough higher frequency results, it will omit the 1-frequency results.\n",
    "    result = sel['token_next'].value_counts()\n",
    "    short_result = [(x,y) for x,y in result.items() if y > 1]\n",
    "    if len(short_result) > 5:\n",
    "        result = short_result\n",
    "    # improvement: create a list of omitted words (e.g. ud, ī, etc.), and make a flag1=False\n",
    "    # optional argument to omit them.\n",
    "    \n",
    "    if group == True:\n",
    "        result = sel.groupby('Category')['token_next'].value_counts().rename(\"count\").reset_index()\n",
    "    \n",
    "    return (result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confreq(\"اژدها\", group = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eurasia Corpus Tool: Functions and Explanation\n",
      "\n",
      "           \ttool_help: lists functions\n",
      "           \tlist_corpora: lists all of the sub-corpora options\n",
      "           \tfreq: returns the most likely terms matching the search term\n",
      "           \tindex_kwic: key word in context; optional arguments: 'category' to specify a specific corpus                   'exclusion=False' to also include the Persian literature corpus\n",
      "            \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tool_help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpora\n",
      ": ['indo_nar_ext_toks', 'indo_xml_toks', 'khiva_doc_toks', 'md_oldsys_toks', 'oldsys_xml_toks', 'pers_lit_toks', 'presort_xml_toks', 'trans_nar_ext_toks', 'trans_xml_toks']\n"
     ]
    }
   ],
   "source": [
    "list_corpora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next steps:\n",
    "\n",
    "## conditional trigram frequency\n",
    "## reverse conditional frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def multi_dic (term):\n",
    "    \n",
    "    match_freq(term)\n",
    "    \n",
    "    search_term = re.compile(term)\n",
    "    \n",
    "    glos_query_mask = glossary[\"Term\"].str.contains(search_term, na=False)\n",
    "    glos_query = glossary[glos_query_mask][[\"UID\", \"Term\", \"Translation\"]]\n",
    "    glos_query\n",
    "    \n",
    "    \n",
    "    dehkhoda_query_mask = dehkhoda[\"Term\"].str.contains(search_term, na=False)\n",
    "    dehkhoda_query = dehkhoda[dehkhoda_query_mask]\n",
    "    dehkhoda_query    \n",
    "    \n",
    "    melz_query_mask = meltzer[\"Präs.-Stamm\"].str.contains(search_term, na=False)\n",
    "    melz_query = meltzer[melz_query_mask]\n",
    "    melz_query[[\"Präs.-Stamm\", \"Deutsch\"]]\n",
    "    \n",
    "    \n",
    "    result = print (\"Glossary \\n\\n\", glos_query,\"\\n\\n\\n\", \\\n",
    "                    \"Dehkhoda \\n\\n\", dehkhoda_query,\"\\n\\n\\n\",\\\n",
    "                    \"Von_Meltzer \\n\\n\", melz_query[[\"Präs.-Stamm\", \"Deutsch\"]])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Another way of doing max value](https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary):\n",
    "\n",
    "```python\n",
    "def keywithmaxval(d):\n",
    "     \"\"\" a) create a list of the dict's keys and values; \n",
    "         b) return the key with the max value\"\"\"  \n",
    "     v=list(d.values())\n",
    "     k=list(d.keys())\n",
    "     return k[v.index(max(v))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def best_match (term, corpus):\n",
    "    \n",
    "    \"\"\"Takes a search term and frequency dictionary, returns the most frequently\\\n",
    "    appearing match within the specified corpus as [matching term, frequency of appearnace].\"\"\"\n",
    "    \n",
    "    search_term = re.compile(term)\n",
    "    toks = {k:v for (k,v) in corpus.items() if re.match(search_term, k)}\n",
    "    if len(toks) > 0:\n",
    "        match = sorted(toks, key=toks.get, reverse=True)[0]\n",
    "        freq = corpus[match]\n",
    "        pair = [match, freq]\n",
    "    \n",
    "    else:\n",
    "        pair = None\n",
    "    \n",
    "    return pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def match_freq(term):\n",
    "    \n",
    "   \n",
    "    \n",
    "    if best_match(term, combo_freq) is not None:\n",
    "        print (\"Most likely match in corpus:\\n\\n\\t\",\\\n",
    "              best_match(term, combo_freq)[0], \"appearing \", best_match(term, combo_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    search_term = re.compile(term)\n",
    "    toks = {k:v for (k,v) in combo_freq.items() if re.match(search_term, k)}\n",
    "    if len(toks) > 3:\n",
    "        cf2 = sorted(toks, key=toks.get, reverse=True)[1]\n",
    "        cf3 = sorted(toks, key=toks.get, reverse=True)[2]\n",
    "        print (\"\\tfollowed by:\\n\\t\",\\\n",
    "                    cf2, \"appearing \", combo_freq[cf2], \"times, and \\n\\t\",\\\n",
    "                   list(sorted(toks))[2], \"appearing \", combo_freq[list(sorted(toks))[2]], \"times\\n\\n\")\n",
    "    \n",
    "    \n",
    "    print (\"Most likely matches in sub-corpora:\\n\")\n",
    "    \n",
    "    if best_match(term, doc_freq) is not None:\n",
    "           print(\"\\tDocuments:\", best_match(term, doc_freq)[0], \"appearing \", best_match(term, doc_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    if best_match(term, nar_freq) is not None:\n",
    "           print (\"\\tNarrative texts:\", best_match(term, nar_freq)[0], \"apprearing\", best_match(term, nar_freq)[1], \"times;\\n\\n\")\n",
    "\n",
    "    if best_match(term, indo_freq) is not None:\n",
    "           print (\"\\tIndic texts:\", best_match(term, indo_freq)[0], \"appearing \", best_match(term, indo_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    if best_match(term, trans_freq) is not None:\n",
    "           print (\"\\tTransoxania texts:\", best_match(term, trans_freq)[0], \"appearing \", best_match(term, trans_freq)[1], \"times;\\n\")\n",
    "    \n",
    "\n",
    "    print (\"\\nMost likely matches in Persian literature corpus:\\n\\t\")\n",
    "           \n",
    "    if best_match(term, pers_lit_freq) is not None:\n",
    "           print (\"\\t\",best_match(term, pers_lit_freq)[0], \"appearing \", best_match(term, pers_lit_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Conditional Frequency Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def confreq (term, refine=\"none\"):\n",
    "    \n",
    "    \"\"\"Conditional frequency across multiple corpora and sub-corpora.\n",
    "        Takes a search term (no regex) for the first word in the bigram, \n",
    "        as well as an optional regex filter to narrow down the second word\n",
    "        in the bigram sequence.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    if refine == \"none\" and len(combo_cfd[term]) > 0:\n",
    "    \n",
    "        if len(combo_cfd[term]) > 0:\n",
    "            print (term, \" is most commonly followed by:\\n\\n\", combo_cfd[term].most_common(10))\n",
    "\n",
    "        \n",
    "\n",
    "        if len(combo_cfd[term]) > 0:\n",
    "            print(\"\\nWithin sub-corpora:\\n\")\n",
    "\n",
    "            # Still need to fill out sub-corpora\n",
    "\n",
    "            if len(doc_cfd[term]) > 0 :\n",
    "                print (\"\\tDocuments:\", term, \" is most commonly followed by:\\n\\n\\t\", doc_cfd[term].most_common(5))\n",
    "            if len(nar_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tNarrative texts:\", term, \" is most commonly followed by:\\n\\n\\t\", nar_cfd[term].most_common(5))\n",
    "            if len(indo_cfd[term]) > 0 :\n",
    "                print (\"\\n\\n\\tIndic texts:\", term, \" is most commonly followed by:\\n\\n\\t\", indo_cfd[term].most_common(5))\n",
    "            if len(trans_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tTransoxania texts:\", term, \" is most commonly followed by:\\n\\n\\t\", trans_cfd[term].most_common(5))\n",
    "    \n",
    "    # Optional regex refinement of the results:\n",
    "    if refine != \"none\" and len(combo_cfd[term]) > 0:\n",
    "        \n",
    "        filt = re.compile(refine)\n",
    "        \n",
    "        filt_toks = [(x, y) for (x, y) in combo_cfd[term].items() if re.match(refine, x)]\n",
    "        \n",
    "        print (\"With the results filtered by the regex search (\", refine, \"), the most likely words following,\", term, \"are:\\n\\t\", filt_toks)\n",
    "        \n",
    "    elif len(combo_cfd[term]) == 0:\n",
    "            print (\"no results\")\n",
    "        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def regcf (term):\n",
    "    \n",
    "    \"\"\"\n",
    "        From a regex search term finds the most frequent possible word, then returns \n",
    "        conditional frequency (from bigrams) across multiple corpora.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if best_match(term, combo_freq) is not None:\n",
    "        \n",
    "        local_match = best_match(term, combo_freq)[0]\n",
    "        \n",
    "        print (\"The most likly match (based on word frequency) for \", term, \" is \", local_match,\\\n",
    "              \"(with frequency\", best_match(term, combo_freq)[1], \").\\n\")\n",
    "        print (\"Conditional frequency of\", local_match, \"(combined corpus):\\n\\t\", combo_cfd[local_match].most_common(5))\n",
    "        \n",
    "        print (\"\\nSub-Corpora:\\n\")\n",
    "        \n",
    "        print (\"\\n\\tDocuments:\\n\\t\", doc_cfd[local_match].most_common(5))\n",
    "        print (\"\\n\\tNarrative texts:\\n\\t\", nar_cfd[local_match].most_common(5))\n",
    "        \n",
    "        print (\"\\n\\n\\tIndic texts:\\n\\t\", indo_cfd[local_match].most_common(5))\n",
    "        print (\"\\n\\tTransoxania texts:\\n\\t\", trans_cfd[local_match].most_common(5))\n",
    "        \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third term, if first two known:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Document Corpus (Meta-Corpus simply too computationally costly)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def tricfd (first_term, second_term, refine=\"none\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given two words in a row, conditional frequency of the third word in the sequence.\n",
    "    Inputs: two words (in order), and an optional regex filter on the third word.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    #print (\"The pair \", first_term, second_term, \" is most commonly followed by :\\n\")\n",
    "    #output = combo_tricfd[(first_term, second_term)].most_common(10)\n",
    "    #print (output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if refine == \"none\" and len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "    \n",
    "        if len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "            print (\"The pair \", first_term, second_term, \" is most commonly followed by:\\n\\n\", combo_tricfd[(first_term, second_term)].most_common(10))\n",
    "\n",
    "\n",
    "        if len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "            print(\"\\nWithin sub-corpora:\\n\")\n",
    "\n",
    "            # Still need to fill out sub-corpora\n",
    "\n",
    "            if len(doc_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\tDocuments:\\n\\n\\t\", doc_tricfd[(first_term, second_term)].most_common(5))\n",
    "            if len(nar_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\n\\tNarrative texts:\\n\\n\\t\", nar_tricfd[(first_term, second_term)].most_common(5))\n",
    "            if len(indo_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\n\\n\\tIndic texts:\\n\\n\\t\", indo_tricfd[(first_term, second_term)].most_common(5))\n",
    "            if len(trans_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\n\\tTransoxania texts::\\n\\n\\t\", trans_tricfd[(first_term, second_term)].most_common(5))\n",
    "    \n",
    "    # Optional regex refinement of the results:\n",
    "    if refine != \"none\" and len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "        \n",
    "        filt = re.compile(refine)\n",
    "        \n",
    "        filt_toks = [(x, y) for (x, y) in combo_tricfd[(first_term, second_term)].items() if re.match(refine, x)]\n",
    "        \n",
    "        print (\"With the results filtered by the regex search (\", refine, \"), the most likely words following the pair \", first_term, second_term, \"are:\\n\\t\", filt_toks)\n",
    "        \n",
    "    elif len(combo_tricfd[(first_term, second_term)]) == 0:\n",
    "            print (\"no results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversed conditional frequency, i.e. if second word in sequence known but not first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Meta-Corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def revcfd (term, refine=\"none\"):\n",
    "    \n",
    "    \n",
    "    \"\"\"Reverse conditional frequency (bigrams) across multiple corpora and sub-corpora.\n",
    "        Takes a search term (no regex) for the first word in the bigram, \n",
    "        as well as an optional regex filter to narrow down the second word\n",
    "        in the bigram sequence.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    if refine == \"none\" and len(rev_combo_cfd[term]) > 0:\n",
    "    \n",
    "        if len(rev_combo_cfd[term]) > 0:\n",
    "            print (term, \" is most commonly preceded by:\\n\\n\", rev_combo_cfd[term].most_common(10))\n",
    "\n",
    "        \n",
    "\n",
    "        if len(rev_combo_cfd[term]) > 0:\n",
    "            print(\"\\nWithin sub-corpora:\\n\")\n",
    "\n",
    "            # Still need to fill out sub-corpora\n",
    "\n",
    "            if len(rev_doc_cfd[term]) > 0 :\n",
    "                print (\"\\tDocuments:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_doc_cfd[term].most_common(5))\n",
    "            if len(rev_nar_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tNarrative texts:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_nar_cfd[term].most_common(5))\n",
    "            if len(rev_indo_cfd[term]) > 0 :\n",
    "                print (\"\\n\\n\\tIndic texts:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_indo_cfd[term].most_common(5))\n",
    "            if len(rev_trans_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tTransoxania texts:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_trans_cfd[term].most_common(5))\n",
    "    \n",
    "    # Optional regex refinement of the results:\n",
    "    if refine != \"none\" and len(rev_combo_cfd[term]) > 0:\n",
    "        \n",
    "        filt = re.compile(refine)\n",
    "        \n",
    "        filt_toks = [(x, y) for (x, y) in rev_combo_cfd[term].items() if re.match(refine, x)]\n",
    "        \n",
    "        print (\"With the results filtered by the regex search (\", refine, \"), the most likely words preceding,\", term, \"are:\\n\\t\", filt_toks)\n",
    "        \n",
    "    elif len(rev_combo_cfd[term]) == 0:\n",
    "            print (\"no results\")\n",
    "        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----\n",
    "# Graveyard\n",
    "(i.e. code saved for posterity, no longer active)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# for whatever reason you can't just use the concordance method on a string;\n",
    "# you have to convert it to an NLTK Text type one way or another\n",
    "\n",
    "trans_corpus = nltk.Text(raw_combo_toks)\n",
    "\n",
    "#trans_corpus.concordance('خانه')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tokens in corpus regex matching the string:*\n",
    "\n",
    "(obsolete with custom KWIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "toks = [x for x in combo_freq if re.match(r'...خوی', x)]\n",
    "toks[:5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "conc0 = sum([trans_corpus.concordance_list(x) for x in toks], [])\n",
    "conc1 = [c.line for c in conc0]\n",
    "print('\\n'.join(conc1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom KWIC (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(drafting, active version now as a function, saved in markdown for posterity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Creating 5-Grams\n",
    "\n",
    "five_grams = {k:list(nltk.ngrams(v, 5)) for (k,v) in combined_corpus_toks.items() if len(v) >= 5}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Find in Document\n",
    "## This function takes a dictionary of 5-grams as the first argument,\n",
    "## a regex search term as the second argument, and returns the sequence of 5 words\n",
    "\n",
    "def find_doc(d, s):\n",
    "    for v in d:\n",
    "        m = re.match(s, v[2])\n",
    "        if m is not None:\n",
    "            yield ' '.join(v)\n",
    "            \n",
    "\n",
    "# Note: Return sends a specified value back to its caller\n",
    "# whereas Yield can produce a sequence of values.\n",
    "\n",
    "\n",
    "# Example:\n",
    "## list(find_doc(five_grams['al_biruni_card_catalog_suleimanov_fond'], 'ف.'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Find Corpus\n",
    "## Produces a generator object with the KWIC with associated work title\n",
    "\n",
    "def find_corpus(c, s):\n",
    "    for k, d in five_grams.items():\n",
    "        for m in find_doc(d, s):\n",
    "            yield f'{k:50s}: {m}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Formatting\n",
    "\n",
    "def print_align(v, m):\n",
    "    plen = max([sum([len(z)+1 for z in x[:m]]) for x in v])\n",
    "    for x in v:\n",
    "        pre = ' '.join(x[:m])\n",
    "        mid = x[m]\n",
    "        pos = ' '.join(x[m+1:])\n",
    "        print(f'{pre:>{plen}s} \\033[1m{mid}\\033[0m {pos}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print('\\n'.join(find_corpus(five_grams, '^من.قر?$')))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Meta-Corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# ConditionalFreqDist() takes a list of pairs.\n",
    "# Generator variable uses itself up upon assignment, so need to recreate above\n",
    "\n",
    "bigrams_cfd = nltk.ngrams(raw_combo_toks, 2)\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(bigrams_cfd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Conditional Frequency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Meta-Corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "search_term = r\"جهد\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print (search_term, \" is most commonly followed by:\\n\")\n",
    "cfd[search_term].most_common(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Document Corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "bigrams_doc_fd = nltk.ngrams(raw_doc_toks, 2)\n",
    "\n",
    "cfd_doc = nltk.ConditionalFreqDist(bigrams_doc_fd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "search_term = \"بداند\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print (\"\\nin the documents corpus, \", search_term, \" is most commonly followed by: \\n\")\n",
    "cfd_doc[search_term].most_common(5)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
