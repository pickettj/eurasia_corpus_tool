{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Deciphering Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, re, nltk, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import DataFrame, Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set home directory path\n",
    "hdir = os.path.expanduser('~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sister files:\n",
    "- Pickled corpora cleaned in text_cleaning_tokenizing\n",
    "- Corpora stats in corpora_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Importing Corpora\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = hdir + \"/Box/Notes/Digital_Humanities/Corpora/pickled_tokenized_cleaned_corpora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_path + \"/corpora.pkl\", \"rb\") as f:\n",
    "    unsorted_doc_toks,\\\n",
    "                indo_xml_toks, hyd_xml_toks, trans_xml_toks,\\\n",
    "                trans_nar_toks, indo_nar_toks,\\\n",
    "                trans_nar_ext_toks, indo_nar_ext_toks, khiva_doc_toks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_path + \"/meta_corpora.pkl\", \"rb\") as f:\n",
    "    comb_india_nar_toks, comb_trans_nar_toks, nar_corpus_toks, doc_corpus_toks,\\\n",
    "                comb_india_toks, comb_trans_toks, comb_turk_toks,\\\n",
    "                combined_corpus_toks, mega_corpus_toks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test:\n",
    "#\"خان\" in combined_corpus_toks[\"tarikh_i_baljuvan_al_biruni_2663iii_ser412\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Importing Raw Tokens\n",
    "I.e. tokens without parent text designation, i.e. format necessary for many NLTK routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_path + \"/raw_tokens.pkl\", \"rb\") as f:\n",
    "    raw_doc_toks, raw_nar_toks, raw_indo_toks,\\\n",
    "                 raw_trans_toks, raw_lit_toks, raw_combo_toks, raw_turk_toks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test:\n",
    "#indo_nar_toks.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Importing Pre-processed NTLK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_data_path = hdir + \"/Box/Notes/Digital_Humanities/Corpora/pickled_nltk_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK Word Frequencies\n",
    "\n",
    "with open(pickle_data_path + \"/frequencies.pkl\", \"rb\") as f:\n",
    "    combo_freq, pers_lit_freq,\\\n",
    "                indo_freq, trans_freq,\\\n",
    "                nar_freq, doc_freq,\\\n",
    "                turk_freq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK Conditional Frequency Dictionaries (raw tokens)\n",
    "\n",
    "with open(pickle_data_path + \"/cfd.pkl\", \"rb\") as f:\n",
    "    combo_cfd,\\\n",
    "                indo_cfd, trans_cfd,\\\n",
    "                nar_cfd, doc_cfd,\\\n",
    "                turk_cfd,\\\n",
    "                rev_combo_cfd,\\\n",
    "                rev_indo_cfd, rev_trans_cfd,\\\n",
    "                rev_nar_cfd, rev_doc_cfd,\\\n",
    "                rev_turk_cfd = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK 5-grams (tokens by work)\n",
    "\n",
    "with open(pickle_data_path + \"/fivegrams.pkl\", \"rb\") as f:\n",
    "    combo_five_grams,\\\n",
    "                indo_five_grams, trans_five_grams,\\\n",
    "                nar_five_grams, doc_five_grams,\\\n",
    "                turk_five_grams = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three-way Conditional Frequency Dictionaries (raw tokens)\n",
    "\n",
    "with open(pickle_data_path + \"/tri_cfd.pkl\", \"rb\") as f:\n",
    "    combo_tricfd,\\\n",
    "                indo_tricfd, trans_tricfd,\\\n",
    "                nar_tricfd, doc_tricfd,\\\n",
    "                turk_tricfd = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Importing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Von Melzer Persian Lexicon\n",
    "- Glossary\n",
    "- Place Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset path\n",
    "\n",
    "ds_path = hdir + \"/Box/Notes/Digital_Humanities/Datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Von Melzer\n",
    "meltzer = pd.read_csv(ds_path + \"/von_melzer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meltzer[\"Präs.-Stamm\"].sample(5)\n",
    "#meltzer.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations\n",
    "locations = pd.read_csv(ds_path + '/exported_database_data/locations.csv', names=['UID', 'Ar_Names', \\\n",
    "                                                'Lat_Name', 'Nickname', 'Type'])\n",
    "# Social Roles\n",
    "roles = pd.read_csv(ds_path + '/exported_database_data/roles.csv', names=['UID', 'Term', 'Emic', 'Etic', 'Scope'])\n",
    "\n",
    "# Glossary\n",
    "glossary = pd.read_csv(ds_path + '/exported_database_data/glossary.csv', names=['UID', 'Term', \\\n",
    "                                                'Eng_Term', 'Translation', 'Transliteration', 'Scope', 'Tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dehkhoda = pd.read_csv(ds_path + \"/dehkhoda_dictionary.csv\", names=['Term', 'Definition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dehkhoda.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex reminders:\n",
    "- Just the word itself: `^مال$`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = re.compile(r\"ب.د\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Von Melzer Persian Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "melz_query_mask = meltzer[\"Präs.-Stamm\"].str.contains(search_term, na=False)\n",
    "melz_query = meltzer[melz_query_mask]\n",
    "#melz_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Technical Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glos_query_mask = glossary[\"Term\"].str.contains(search_term, na=False)\n",
    "glos_query = glossary[glos_query_mask]\n",
    "#glos_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Social Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_query_mask = roles[\"Emic\"].str.contains(search_term, na=False)\n",
    "roles_query = roles[roles_query_mask]\n",
    "#roles_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Place Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_query_mask = locations[\"Ar_Names\"].str.contains(search_term, na=False)\n",
    "loc_query = locations[loc_query_mask]\n",
    "#loc_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpora_guide ():\n",
    "    print(\n",
    "        \"\\tCombined Token Corpora:\\n\\\n",
    "        \\t Narrative Sources from India: comb_india_nar_toks\\n\\\n",
    "        \\t Narrative Sources from Transoxania: comb_trans_nar_toks\\n\\n\\\n",
    "        \\t All Narrative Sources: nar_corpus_toks\\n\\\n",
    "        \\t All Document Sources: doc_corpus_toks\\n\\n\\\n",
    "        \\t Documents and Narrative Sources: combined_corpus_toks\\n\\\n",
    "        \\t Mega Corpus including Persian lit. corpus: mega_corpus_toks\\n\\n\\n\\\n",
    "        Individual Corpora:\\n\\\n",
    "        \\t External Indic Corpus: indo_nar_ext_toks\\n\\\n",
    "        \\t External Transoxania Corpus: trans_nar_ext_toks\\n\\n\\\n",
    "        \\t Khiva Turkic Document Corpus: khiva_doc_toks\\n\\n\\\n",
    "        \\t Internal India Narrative Corpus: indo_nar_toks\\n\\\n",
    "        \\t Internal Transoxania Narrative orpus: trans_nar_toks\\n\\n\\\n",
    "        \\t XML-stage Transoxania Documents: trans_xml_toks\\n\\\n",
    "        \\t XML-stage Indic Documents: indo_xml_toks\\n\\\n",
    "        \\t XML-stage Hyderabad Documents: hyd_xml_toks\\n\\n\\\n",
    "        \\t\"\n",
    "                 \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Another way of doing max value](https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary):\n",
    "\n",
    "```python\n",
    "def keywithmaxval(d):\n",
    "     \"\"\" a) create a list of the dict's keys and values; \n",
    "         b) return the key with the max value\"\"\"  \n",
    "     v=list(d.values())\n",
    "     k=list(d.keys())\n",
    "     return k[v.index(max(v))]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match (term, corpus):\n",
    "    \n",
    "    \"\"\"Takes a search term and frequency dictionary, returns the most frequently\\\n",
    "    appearing match within the specified corpus as [matching term, frequency of appearnace].\"\"\"\n",
    "    \n",
    "    search_term = re.compile(term)\n",
    "    toks = {k:v for (k,v) in corpus.items() if re.match(search_term, k)}\n",
    "    if len(toks) > 0:\n",
    "        match = sorted(toks, key=toks.get, reverse=True)[0]\n",
    "        freq = corpus[match]\n",
    "        pair = [match, freq]\n",
    "    \n",
    "    else:\n",
    "        pair = None\n",
    "    \n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['داروی', 6]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#help (best_match)\n",
    "\n",
    "#best_match (\"error\", nar_freq)\n",
    "\n",
    "best_match(\"د.رو\", doc_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_freq(term):\n",
    "    \n",
    "   \n",
    "    \n",
    "    if best_match(term, combo_freq) is not None:\n",
    "        print (\"Most likely match in corpus:\\n\\n\\t\",\\\n",
    "              best_match(term, combo_freq)[0], \"appearing \", best_match(term, combo_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    search_term = re.compile(term)\n",
    "    toks = {k:v for (k,v) in combo_freq.items() if re.match(search_term, k)}\n",
    "    if len(toks) > 3:\n",
    "        cf2 = sorted(toks, key=toks.get, reverse=True)[1]\n",
    "        cf3 = sorted(toks, key=toks.get, reverse=True)[2]\n",
    "        print (\"\\tfollowed by:\\n\\t\",\\\n",
    "                    cf2, \"appearing \", combo_freq[cf2], \"times, and \\n\\t\",\\\n",
    "                   list(sorted(toks))[2], \"appearing \", combo_freq[list(sorted(toks))[2]], \"times\\n\\n\")\n",
    "    \n",
    "    \n",
    "    print (\"Most likely matches in sub-corpora:\\n\")\n",
    "    \n",
    "    if best_match(term, doc_freq) is not None:\n",
    "           print(\"\\tDocuments:\", best_match(term, doc_freq)[0], \"appearing \", best_match(term, doc_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    if best_match(term, nar_freq) is not None:\n",
    "           print (\"\\tNarrative texts:\", best_match(term, nar_freq)[0], \"apprearing\", best_match(term, nar_freq)[1], \"times;\\n\\n\")\n",
    "\n",
    "    if best_match(term, indo_freq) is not None:\n",
    "           print (\"\\tIndic texts:\", best_match(term, indo_freq)[0], \"appearing \", best_match(term, indo_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    if best_match(term, trans_freq) is not None:\n",
    "           print (\"\\tTransoxania texts:\", best_match(term, trans_freq)[0], \"appearing \", best_match(term, trans_freq)[1], \"times;\\n\")\n",
    "    \n",
    "\n",
    "    print (\"\\nMost likely matches in Persian literature corpus:\\n\\t\")\n",
    "           \n",
    "    if best_match(term, pers_lit_freq) is not None:\n",
    "           print (\"\\t\",best_match(term, pers_lit_freq)[0], \"appearing \", best_match(term, pers_lit_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#match_freq(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_dic (term):\n",
    "    \n",
    "    match_freq(term)\n",
    "    \n",
    "    search_term = re.compile(term)\n",
    "    \n",
    "    glos_query_mask = glossary[\"Term\"].str.contains(search_term, na=False)\n",
    "    glos_query = glossary[glos_query_mask][[\"UID\", \"Term\", \"Translation\"]]\n",
    "    glos_query\n",
    "    \n",
    "    \n",
    "    dehkhoda_query_mask = dehkhoda[\"Term\"].str.contains(search_term, na=False)\n",
    "    dehkhoda_query = dehkhoda[dehkhoda_query_mask]\n",
    "    dehkhoda_query    \n",
    "    \n",
    "    melz_query_mask = meltzer[\"Präs.-Stamm\"].str.contains(search_term, na=False)\n",
    "    melz_query = meltzer[melz_query_mask]\n",
    "    melz_query[[\"Präs.-Stamm\", \"Deutsch\"]]\n",
    "    \n",
    "    \n",
    "    result = print (\"Glossary \\n\\n\", glos_query,\"\\n\\n\\n\", \\\n",
    "                    \"Dehkhoda \\n\\n\", dehkhoda_query,\"\\n\\n\\n\",\\\n",
    "                    \"Von_Meltzer \\n\\n\", melz_query[[\"Präs.-Stamm\", \"Deutsch\"]])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi_dic (\"دارو+\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom KWIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find in Document\n",
    "\n",
    "def find_doc(d, s):\n",
    "    \n",
    "    \"\"\"This function takes a dictionary of 5-grams as the first argument,\\\n",
    "    a regex search term as the second argument, and returns the sequence of 5 words\"\"\"\n",
    "    \n",
    "    for v in d:\n",
    "        m = re.match(s, v[2])\n",
    "        if m is not None:\n",
    "            yield ' '.join(v)\n",
    "            \n",
    "\n",
    "# Note: Return sends a specified value back to its caller\n",
    "# whereas Yield can produce a sequence of values.\n",
    "\n",
    "\n",
    "# Example:\n",
    "## list(find_doc(five_grams['al_biruni_card_catalog_suleimanov_fond'], 'ف.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Corpus\n",
    "## Produces a generator object with the KWIC with associated work title\n",
    "\n",
    "def find_corpus(c, s):\n",
    "    for k, d in c.items():\n",
    "        for m in find_doc(d, s):\n",
    "            yield f'{k:50s}: {m}'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kwic(term, corpus=combo_five_grams):\n",
    "    \n",
    "    print('\\n'.join(find_corpus(corpus, term)))\n",
    "    \n",
    "    # todo: organize this by best match\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kwic(\"د.رو\", indo_five_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Conditional Frequency Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confreq (term, refine=\"none\"):\n",
    "    \n",
    "    \"\"\"Conditional frequency across multiple corpora and sub-corpora.\n",
    "        Takes a search term (no regex) for the first word in the bigram, \n",
    "        as well as an optional regex filter to narrow down the second word\n",
    "        in the bigram sequence.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    if refine == \"none\" and len(combo_cfd[term]) > 0:\n",
    "    \n",
    "        if len(combo_cfd[term]) > 0:\n",
    "            print (term, \" is most commonly followed by:\\n\\n\", combo_cfd[term].most_common(10))\n",
    "\n",
    "        \n",
    "\n",
    "        if len(combo_cfd[term]) > 0:\n",
    "            print(\"\\nWithin sub-corpora:\\n\")\n",
    "\n",
    "            # Still need to fill out sub-corpora\n",
    "\n",
    "            if len(doc_cfd[term]) > 0 :\n",
    "                print (\"\\tDocuments:\", term, \" is most commonly followed by:\\n\\n\\t\", doc_cfd[term].most_common(5))\n",
    "            if len(nar_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tNarrative texts:\", term, \" is most commonly followed by:\\n\\n\\t\", nar_cfd[term].most_common(5))\n",
    "            if len(indo_cfd[term]) > 0 :\n",
    "                print (\"\\n\\n\\tIndic texts:\", term, \" is most commonly followed by:\\n\\n\\t\", indo_cfd[term].most_common(5))\n",
    "            if len(trans_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tTransoxania texts:\", term, \" is most commonly followed by:\\n\\n\\t\", trans_cfd[term].most_common(5))\n",
    "    \n",
    "    # Optional regex refinement of the results:\n",
    "    if refine != \"none\" and len(combo_cfd[term]) > 0:\n",
    "        \n",
    "        filt = re.compile(refine)\n",
    "        \n",
    "        filt_toks = [(x, y) for (x, y) in combo_cfd[term].items() if re.match(refine, x)]\n",
    "        \n",
    "        print (\"With the results filtered by the regex search (\", refine, \"), the most likely words following,\", term, \"are:\\n\\t\", filt_toks)\n",
    "        \n",
    "    elif len(combo_cfd[term]) == 0:\n",
    "            print (\"no results\")\n",
    "        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confreq(\"قوش\")\n",
    "#help(confreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regcf (term):\n",
    "    \n",
    "    \"\"\"\n",
    "        From a regex search term finds the most frequent possible word, then returns \n",
    "        conditional frequency (from bigrams) across multiple corpora.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if best_match(term, combo_freq) is not None:\n",
    "        \n",
    "        local_match = best_match(term, combo_freq)[0]\n",
    "        \n",
    "        print (\"The most likly match (based on word frequency) for \", term, \" is \", local_match,\\\n",
    "              \"(with frequency\", best_match(term, combo_freq)[1], \").\\n\")\n",
    "        print (\"Conditional frequency of\", local_match, \"(combined corpus):\\n\\t\", combo_cfd[local_match].most_common(5))\n",
    "        \n",
    "        print (\"\\nSub-Corpora:\\n\")\n",
    "        \n",
    "        print (\"\\n\\tDocuments:\\n\\t\", doc_cfd[local_match].most_common(5))\n",
    "        print (\"\\n\\tNarrative texts:\\n\\t\", nar_cfd[local_match].most_common(5))\n",
    "        \n",
    "        print (\"\\n\\n\\tIndic texts:\\n\\t\", indo_cfd[local_match].most_common(5))\n",
    "        print (\"\\n\\tTransoxania texts:\\n\\t\", trans_cfd[local_match].most_common(5))\n",
    "        \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regcf(\"ق.ش\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: functions for 3-part confreq, and reverse confreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third term, if first two known:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Document Corpus (Meta-Corpus simply too computationally costly)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tricfd (first_term, second_term, refine=\"none\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given two words in a row, conditional frequency of the third word in the sequence.\n",
    "    Inputs: two words (in order), and an optional regex filter on the third word.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    #print (\"The pair \", first_term, second_term, \" is most commonly followed by :\\n\")\n",
    "    #output = combo_tricfd[(first_term, second_term)].most_common(10)\n",
    "    #print (output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if refine == \"none\" and len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "    \n",
    "        if len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "            print (\"The pair \", first_term, second_term, \" is most commonly followed by:\\n\\n\", combo_tricfd[(first_term, second_term)].most_common(10))\n",
    "\n",
    "\n",
    "        if len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "            print(\"\\nWithin sub-corpora:\\n\")\n",
    "\n",
    "            # Still need to fill out sub-corpora\n",
    "\n",
    "            if len(doc_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\tDocuments:\\n\\n\\t\", doc_tricfd[(first_term, second_term)].most_common(5))\n",
    "            if len(nar_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\n\\tNarrative texts:\\n\\n\\t\", nar_tricfd[(first_term, second_term)].most_common(5))\n",
    "            if len(indo_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\n\\n\\tIndic texts:\\n\\n\\t\", indo_tricfd[(first_term, second_term)].most_common(5))\n",
    "            if len(trans_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\n\\tTransoxania texts::\\n\\n\\t\", trans_tricfd[(first_term, second_term)].most_common(5))\n",
    "    \n",
    "    # Optional regex refinement of the results:\n",
    "    if refine != \"none\" and len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "        \n",
    "        filt = re.compile(refine)\n",
    "        \n",
    "        filt_toks = [(x, y) for (x, y) in combo_tricfd[(first_term, second_term)].items() if re.match(refine, x)]\n",
    "        \n",
    "        print (\"With the results filtered by the regex search (\", refine, \"), the most likely words following the pair \", first_term, second_term, \"are:\\n\\t\", filt_toks)\n",
    "        \n",
    "    elif len(combo_tricfd[(first_term, second_term)]) == 0:\n",
    "            print (\"no results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tricfd (\"بعد\", \"از\", \"خ+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversed conditional frequency, i.e. if second word in sequence known but not first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Meta-Corpus*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revcfd (term, refine=\"none\"):\n",
    "    \n",
    "    \n",
    "    \"\"\"Reverse conditional frequency (bigrams) across multiple corpora and sub-corpora.\n",
    "        Takes a search term (no regex) for the first word in the bigram, \n",
    "        as well as an optional regex filter to narrow down the second word\n",
    "        in the bigram sequence.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    if refine == \"none\" and len(rev_combo_cfd[term]) > 0:\n",
    "    \n",
    "        if len(rev_combo_cfd[term]) > 0:\n",
    "            print (term, \" is most commonly preceded by:\\n\\n\", rev_combo_cfd[term].most_common(10))\n",
    "\n",
    "        \n",
    "\n",
    "        if len(rev_combo_cfd[term]) > 0:\n",
    "            print(\"\\nWithin sub-corpora:\\n\")\n",
    "\n",
    "            # Still need to fill out sub-corpora\n",
    "\n",
    "            if len(rev_doc_cfd[term]) > 0 :\n",
    "                print (\"\\tDocuments:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_doc_cfd[term].most_common(5))\n",
    "            if len(rev_nar_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tNarrative texts:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_nar_cfd[term].most_common(5))\n",
    "            if len(rev_indo_cfd[term]) > 0 :\n",
    "                print (\"\\n\\n\\tIndic texts:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_indo_cfd[term].most_common(5))\n",
    "            if len(rev_trans_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tTransoxania texts:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_trans_cfd[term].most_common(5))\n",
    "    \n",
    "    # Optional regex refinement of the results:\n",
    "    if refine != \"none\" and len(rev_combo_cfd[term]) > 0:\n",
    "        \n",
    "        filt = re.compile(refine)\n",
    "        \n",
    "        filt_toks = [(x, y) for (x, y) in rev_combo_cfd[term].items() if re.match(refine, x)]\n",
    "        \n",
    "        print (\"With the results filtered by the regex search (\", refine, \"), the most likely words preceding,\", term, \"are:\\n\\t\", filt_toks)\n",
    "        \n",
    "    elif len(rev_combo_cfd[term]) == 0:\n",
    "            print (\"no results\")\n",
    "        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revcfd(\"قوش\", \"خد\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCombined Token Corpora:\n",
      "        \t Narrative Sources from India: comb_india_nar_toks\n",
      "        \t Narrative Sources from Transoxania: comb_trans_nar_toks\n",
      "\n",
      "        \t All Narrative Sources: nar_corpus_toks\n",
      "        \t All Document Sources: doc_corpus_toks\n",
      "\n",
      "        \t Documents and Narrative Sources: combined_corpus_toks\n",
      "        \t Mega Corpus including Persian lit. corpus: mega_corpus_toks\n",
      "\n",
      "\n",
      "        Individual Corpora:\n",
      "        \t External Indic Corpus: indo_nar_ext_toks\n",
      "        \t External Transoxania Corpus: trans_nar_ext_toks\n",
      "\n",
      "        \t Khiva Turkic Document Corpus: khiva_doc_toks\n",
      "\n",
      "        \t Internal India Narrative Corpus: indo_nar_toks\n",
      "        \t Internal Transoxania Narrative orpus: trans_nar_toks\n",
      "\n",
      "        \t XML-stage Transoxania Documents: trans_xml_toks\n",
      "        \t XML-stage Indic Documents: indo_xml_toks\n",
      "        \t XML-stage Hyderabad Documents: hyd_xml_toks\n",
      "\n",
      "        \t\n"
     ]
    }
   ],
   "source": [
    "corpora_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----\n",
    "# Graveyard\n",
    "(i.e. code saved for posterity, no longer active)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# for whatever reason you can't just use the concordance method on a string;\n",
    "# you have to convert it to an NLTK Text type one way or another\n",
    "\n",
    "trans_corpus = nltk.Text(raw_combo_toks)\n",
    "\n",
    "#trans_corpus.concordance('خانه')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tokens in corpus regex matching the string:*\n",
    "\n",
    "(obsolete with custom KWIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "toks = [x for x in combo_freq if re.match(r'...خوی', x)]\n",
    "toks[:5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "conc0 = sum([trans_corpus.concordance_list(x) for x in toks], [])\n",
    "conc1 = [c.line for c in conc0]\n",
    "print('\\n'.join(conc1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom KWIC (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(drafting, active version now as a function, saved in markdown for posterity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Creating 5-Grams\n",
    "\n",
    "five_grams = {k:list(nltk.ngrams(v, 5)) for (k,v) in combined_corpus_toks.items() if len(v) >= 5}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Find in Document\n",
    "## This function takes a dictionary of 5-grams as the first argument,\n",
    "## a regex search term as the second argument, and returns the sequence of 5 words\n",
    "\n",
    "def find_doc(d, s):\n",
    "    for v in d:\n",
    "        m = re.match(s, v[2])\n",
    "        if m is not None:\n",
    "            yield ' '.join(v)\n",
    "            \n",
    "\n",
    "# Note: Return sends a specified value back to its caller\n",
    "# whereas Yield can produce a sequence of values.\n",
    "\n",
    "\n",
    "# Example:\n",
    "## list(find_doc(five_grams['al_biruni_card_catalog_suleimanov_fond'], 'ف.'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Find Corpus\n",
    "## Produces a generator object with the KWIC with associated work title\n",
    "\n",
    "def find_corpus(c, s):\n",
    "    for k, d in five_grams.items():\n",
    "        for m in find_doc(d, s):\n",
    "            yield f'{k:50s}: {m}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Formatting\n",
    "\n",
    "def print_align(v, m):\n",
    "    plen = max([sum([len(z)+1 for z in x[:m]]) for x in v])\n",
    "    for x in v:\n",
    "        pre = ' '.join(x[:m])\n",
    "        mid = x[m]\n",
    "        pos = ' '.join(x[m+1:])\n",
    "        print(f'{pre:>{plen}s} \\033[1m{mid}\\033[0m {pos}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print('\\n'.join(find_corpus(five_grams, '^من.قر?$')))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Meta-Corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# ConditionalFreqDist() takes a list of pairs.\n",
    "# Generator variable uses itself up upon assignment, so need to recreate above\n",
    "\n",
    "bigrams_cfd = nltk.ngrams(raw_combo_toks, 2)\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(bigrams_cfd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Conditional Frequency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Meta-Corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "search_term = r\"جهد\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print (search_term, \" is most commonly followed by:\\n\")\n",
    "cfd[search_term].most_common(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Document Corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "bigrams_doc_fd = nltk.ngrams(raw_doc_toks, 2)\n",
    "\n",
    "cfd_doc = nltk.ConditionalFreqDist(bigrams_doc_fd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "search_term = \"بداند\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print (\"\\nin the documents corpus, \", search_term, \" is most commonly followed by: \\n\")\n",
    "cfd_doc[search_term].most_common(5)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
