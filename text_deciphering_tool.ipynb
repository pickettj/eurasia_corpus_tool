{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Deciphering Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, re, nltk, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import DataFrame, Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set home directory path\n",
    "hdir = os.path.expanduser('~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sister files:\n",
    "- Pickled corpora cleaned in text_cleaning_tokenizing\n",
    "- Corpora stats in corpora_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Corpora\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = hdir + \"/Dropbox/Active_Directories/Digital_Humanities/Corpora/pickled_tokenized_cleaned_corpora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe corpus\n",
    "\n",
    "df_eurcorp = pd.read_csv (os.path.join(pickle_path,r'eurasia_corpus.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>No</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15027780</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>rumi_moulavi.masnavi</td>\n",
       "      <td>21064</td>\n",
       "      <td>نشان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13358103</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>parvin.divan</td>\n",
       "      <td>3786</td>\n",
       "      <td>دگر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15688942</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>saeb.divan</td>\n",
       "      <td>240737</td>\n",
       "      <td>سر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10215250</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>khaghani.divan</td>\n",
       "      <td>94449</td>\n",
       "      <td>طفل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9302230</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>gorgani.visuramin</td>\n",
       "      <td>110242</td>\n",
       "      <td>اورنگ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Category                  Text      No  Token\n",
       "15027780  pers_lit_toks  rumi_moulavi.masnavi   21064   نشان\n",
       "13358103  pers_lit_toks          parvin.divan    3786    دگر\n",
       "15688942  pers_lit_toks            saeb.divan  240737     سر\n",
       "10215250  pers_lit_toks        khaghani.divan   94449    طفل\n",
       "9302230   pers_lit_toks     gorgani.visuramin  110242  اورنگ"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying import took\n",
    "df_eurcorp.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived code from pickling era*:\n",
    "\n",
    "```python\n",
    "with open(pickle_path + \"/corpora.pkl\", \"rb\") as f:\n",
    "    unsorted_doc_toks,\\\n",
    "                indo_xml_toks, hyd_xml_toks, trans_xml_toks,\\\n",
    "                trans_nar_toks, indo_nar_toks,\\\n",
    "                trans_nar_ext_toks, indo_nar_ext_toks, khiva_doc_toks = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with open(pickle_path + \"/meta_corpora.pkl\", \"rb\") as f:\n",
    "    comb_india_nar_toks, comb_trans_nar_toks, nar_corpus_toks, doc_corpus_toks,\\\n",
    "                comb_india_toks, comb_trans_toks, comb_turk_toks,\\\n",
    "                combined_corpus_toks, mega_corpus_toks = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived II. Importing Raw Tokens; I.e. tokens without parent text designation, i.e. format necessary for many NLTK routines.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with open(pickle_path + \"/raw_tokens.pkl\", \"rb\") as f:\n",
    "    raw_doc_toks, raw_nar_toks, raw_indo_toks,\\\n",
    "                 raw_trans_toks, raw_lit_toks, raw_combo_toks, raw_turk_toks = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived: Importing Pre-processed NTLK Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pickle_data_path = hdir + \"/Box/Notes/Digital_Humanities/Corpora/pickled_nltk_data\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived: NLTK Word Frequencies*\n",
    "\n",
    "```python\n",
    "with open(pickle_data_path + \"/frequencies.pkl\", \"rb\") as f:\n",
    "    combo_freq, pers_lit_freq,\\\n",
    "                indo_freq, trans_freq,\\\n",
    "                nar_freq, doc_freq,\\\n",
    "                turk_freq = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived: NLTK Conditional Frequency Dictionaries (raw tokens)*\n",
    "\n",
    "```python\n",
    "with open(pickle_data_path + \"/cfd.pkl\", \"rb\") as f:\n",
    "    combo_cfd,\\\n",
    "                indo_cfd, trans_cfd,\\\n",
    "                nar_cfd, doc_cfd,\\\n",
    "                turk_cfd,\\\n",
    "                rev_combo_cfd,\\\n",
    "                rev_indo_cfd, rev_trans_cfd,\\\n",
    "                rev_nar_cfd, rev_doc_cfd,\\\n",
    "                rev_turk_cfd = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived: NLTK 5-grams (tokens by work)*\n",
    "\n",
    "```python\n",
    "with open(pickle_data_path + \"/fivegrams.pkl\", \"rb\") as f:\n",
    "    combo_five_grams,\\\n",
    "                indo_five_grams, trans_five_grams,\\\n",
    "                nar_five_grams, doc_five_grams,\\\n",
    "                turk_five_grams = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived: Three-way Conditional Frequency Dictionaries (raw tokens)*\n",
    "\n",
    "```python\n",
    "with open(pickle_data_path + \"/tri_cfd.pkl\", \"rb\") as f:\n",
    "    combo_tricfd,\\\n",
    "                indo_tricfd, trans_tricfd,\\\n",
    "                nar_tricfd, doc_tricfd,\\\n",
    "                turk_tricfd = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Von Melzer Persian Lexicon\n",
    "- Glossary\n",
    "- Place Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset path\n",
    "\n",
    "ds_path = hdir + \"/Dropbox/Active_Directories/Digital_Humanities/Datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Von Melzer\n",
    "meltzer = pd.read_csv(ds_path + \"/von_melzer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Persisch</th>\n",
       "      <th>Präs.-Stamm</th>\n",
       "      <th>Transkription</th>\n",
       "      <th>Deutsch</th>\n",
       "      <th>Bemerkung</th>\n",
       "      <th>Quellenangaben</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28982</th>\n",
       "      <td>28983</td>\n",
       "      <td>II</td>\n",
       "      <td>10198</td>\n",
       "      <td>168</td>\n",
       "      <td>‫جاگیردار‬</td>\n",
       "      <td>ǧā-gīr-dār</td>\n",
       "      <td>(in Indien: Inhaber eines Grundstücks als Ruhe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EdI I 1038b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13900</th>\n",
       "      <td>13901</td>\n",
       "      <td>I</td>\n",
       "      <td>13900</td>\n",
       "      <td>2412</td>\n",
       "      <td>‫بدخواهی‬</td>\n",
       "      <td>bad-ḫāhī</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Böswilligkeit (f.); feindliche Gesin- nung (f.)</td>\n",
       "      <td>Nicolas 1887:II 52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58277</th>\n",
       "      <td>58278</td>\n",
       "      <td>IV</td>\n",
       "      <td>1250</td>\n",
       "      <td>1251</td>\n",
       "      <td>(‫گرفتن )اندر‬</td>\n",
       "      <td>&lt;-‫ &gt;گیر‬gereftan &lt;gīr-&gt; (andar)</td>\n",
       "      <td>sich hängen (an)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tārīḫ-i Ṭabarī 538/10f.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23278</th>\n",
       "      <td>23279</td>\n",
       "      <td>II</td>\n",
       "      <td>4494</td>\n",
       "      <td>126</td>\n",
       "      <td>(‫ﺗاب داشتن )از‬</td>\n",
       "      <td>tāb dāštan (az)</td>\n",
       "      <td>aufgebracht sein; erhitzt sein; erzürnt sein (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ḥāfiẓ (Dīvān 198V.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>3775</td>\n",
       "      <td>I</td>\n",
       "      <td>3774</td>\n",
       "      <td>3775</td>\n",
       "      <td>‫ارابه ساز‬</td>\n",
       "      <td>arābe-sāz</td>\n",
       "      <td>Wagner (m.)</td>\n",
       "      <td>BA I</td>\n",
       "      <td>57a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         UID Volume  Unnamed: 2  Persisch       Präs.-Stamm  \\\n",
       "28982  28983     II       10198       168        ‫جاگیردار‬   \n",
       "13900  13901      I       13900      2412         ‫بدخواهی‬   \n",
       "58277  58278     IV        1250      1251    (‫گرفتن )اندر‬   \n",
       "23278  23279     II        4494       126  (‫ﺗاب داشتن )از‬   \n",
       "3774    3775      I        3774      3775       ‫ارابه ساز‬   \n",
       "\n",
       "                          Transkription  \\\n",
       "28982                        ǧā-gīr-dār   \n",
       "13900                          bad-ḫāhī   \n",
       "58277  <-‫ >گیر‬gereftan <gīr-> (andar)   \n",
       "23278                   tāb dāštan (az)   \n",
       "3774                          arābe-sāz   \n",
       "\n",
       "                                                 Deutsch  \\\n",
       "28982  (in Indien: Inhaber eines Grundstücks als Ruhe...   \n",
       "13900                                                NaN   \n",
       "58277                                   sich hängen (an)   \n",
       "23278  aufgebracht sein; erhitzt sein; erzürnt sein (...   \n",
       "3774                                         Wagner (m.)   \n",
       "\n",
       "                                             Bemerkung  \\\n",
       "28982                                              NaN   \n",
       "13900  Böswilligkeit (f.); feindliche Gesin- nung (f.)   \n",
       "58277                                              NaN   \n",
       "23278                                              NaN   \n",
       "3774                                              BA I   \n",
       "\n",
       "                Quellenangaben  \n",
       "28982              EdI I 1038b  \n",
       "13900       Nicolas 1887:II 52  \n",
       "58277  Tārīḫ-i Ṭabarī 538/10f.  \n",
       "23278      Ḥāfiẓ (Dīvān 198V.)  \n",
       "3774                       57a  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifying import\n",
    "#meltzer[\"Präs.-Stamm\"].sample(5)\n",
    "meltzer.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations\n",
    "locations = pd.read_csv(ds_path + '/exported_database_data/locations.csv', names=['UID', 'Ar_Names', \\\n",
    "                                                'Lat_Name', 'Nickname', 'Type'])\n",
    "# Social Roles\n",
    "roles = pd.read_csv(ds_path + '/exported_database_data/roles.csv', names=['UID', 'Term', 'Emic', 'Etic', 'Scope'])\n",
    "\n",
    "# Glossary\n",
    "glossary = pd.read_csv(ds_path + '/exported_database_data/glossary.csv', names=['UID', 'Term', \\\n",
    "                                                'Eng_Term', 'Translation', 'Transliteration', 'Scope', 'Tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dehkhoda = pd.read_csv(ds_path + \"/dehkhoda_dictionary.csv\", names=['Term', 'Definition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12798</th>\n",
       "      <td>فلات</td>\n",
       "      <td>1 بادیه، بر، بیابان، پشته، خشکی، دشت، صحرا، ن...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11400</th>\n",
       "      <td>صمد</td>\n",
       "      <td>بی‌نیاز، غنی &amp; نیازمند</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13400</th>\n",
       "      <td>کدر</td>\n",
       "      <td>1، بی‌جلا، بی‌رونق، بی‌فروغ، بی‌نور، تار، تیر...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10585</th>\n",
       "      <td>سوگلی</td>\n",
       "      <td>1 برگزیده 2 عزیزدردانه، عزیزکرده 3 محبوب، معشوق</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>بی‌حیایی</td>\n",
       "      <td>بی‌چشم‌ورویی، بی‌شرمی، دریدگی، گستاخی، وقاحت ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Term                                         Definition\n",
       "12798       فلات   1 بادیه، بر، بیابان، پشته، خشکی، دشت، صحرا، ن...\n",
       "11400        صمد                             بی‌نیاز، غنی & نیازمند\n",
       "13400        کدر   1، بی‌جلا، بی‌رونق، بی‌فروغ، بی‌نور، تار، تیر...\n",
       "10585     سوگلی     1 برگزیده 2 عزیزدردانه، عزیزکرده 3 محبوب، معشوق\n",
       "2893   بی‌حیایی    بی‌چشم‌ورویی، بی‌شرمی، دریدگی، گستاخی، وقاحت ..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifying import\n",
    "dehkhoda.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex reminders:\n",
    "- Just the word itself: `^مال$`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = re.compile(r\"ب.د\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Von Melzer Persian Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "melz_query_mask = meltzer[\"Präs.-Stamm\"].str.contains(search_term, na=False)\n",
    "melz_query = meltzer[melz_query_mask]\n",
    "#melz_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Technical Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glos_query_mask = glossary[\"Term\"].str.contains(search_term, na=False)\n",
    "glos_query = glossary[glos_query_mask]\n",
    "#glos_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Social Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_query_mask = roles[\"Emic\"].str.contains(search_term, na=False)\n",
    "roles_query = roles[roles_query_mask]\n",
    "#roles_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Place Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_query_mask = locations[\"Ar_Names\"].str.contains(search_term, na=False)\n",
    "loc_query = locations[loc_query_mask]\n",
    "#loc_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpora_guide ():\n",
    "    print(\n",
    "        \"\\tCombined Token Corpora:\\n\\\n",
    "        \\t Narrative Sources from India: comb_india_nar_toks\\n\\\n",
    "        \\t Narrative Sources from Transoxania: comb_trans_nar_toks\\n\\n\\\n",
    "        \\t All Narrative Sources: nar_corpus_toks\\n\\\n",
    "        \\t All Document Sources: doc_corpus_toks\\n\\n\\\n",
    "        \\t Documents and Narrative Sources: combined_corpus_toks\\n\\\n",
    "        \\t Mega Corpus including Persian lit. corpus: mega_corpus_toks\\n\\n\\n\\\n",
    "        Individual Corpora:\\n\\\n",
    "        \\t External Indic Corpus: indo_nar_ext_toks\\n\\\n",
    "        \\t External Transoxania Corpus: trans_nar_ext_toks\\n\\n\\\n",
    "        \\t Khiva Turkic Document Corpus: khiva_doc_toks\\n\\n\\\n",
    "        \\t Internal India Narrative Corpus: indo_nar_toks\\n\\\n",
    "        \\t Internal Transoxania Narrative orpus: trans_nar_toks\\n\\n\\\n",
    "        \\t XML-stage Transoxania Documents: trans_xml_toks\\n\\\n",
    "        \\t XML-stage Indic Documents: indo_xml_toks\\n\\\n",
    "        \\t XML-stage Hyderabad Documents: hyd_xml_toks\\n\\n\\\n",
    "        \\t\"\n",
    "                 \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Another way of doing max value](https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary):\n",
    "\n",
    "```python\n",
    "def keywithmaxval(d):\n",
    "     \"\"\" a) create a list of the dict's keys and values; \n",
    "         b) return the key with the max value\"\"\"  \n",
    "     v=list(d.values())\n",
    "     k=list(d.keys())\n",
    "     return k[v.index(max(v))]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match (term, corpus):\n",
    "    \n",
    "    \"\"\"Takes a search term and frequency dictionary, returns the most frequently\\\n",
    "    appearing match within the specified corpus as [matching term, frequency of appearnace].\"\"\"\n",
    "    \n",
    "    search_term = re.compile(term)\n",
    "    toks = {k:v for (k,v) in corpus.items() if re.match(search_term, k)}\n",
    "    if len(toks) > 0:\n",
    "        match = sorted(toks, key=toks.get, reverse=True)[0]\n",
    "        freq = corpus[match]\n",
    "        pair = [match, freq]\n",
    "    \n",
    "    else:\n",
    "        pair = None\n",
    "    \n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help (best_match)\n",
    "\n",
    "#best_match (\"error\", nar_freq)\n",
    "\n",
    "#best_match(\"د.رو\", doc_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_freq(term):\n",
    "    \n",
    "   \n",
    "    \n",
    "    if best_match(term, combo_freq) is not None:\n",
    "        print (\"Most likely match in corpus:\\n\\n\\t\",\\\n",
    "              best_match(term, combo_freq)[0], \"appearing \", best_match(term, combo_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    search_term = re.compile(term)\n",
    "    toks = {k:v for (k,v) in combo_freq.items() if re.match(search_term, k)}\n",
    "    if len(toks) > 3:\n",
    "        cf2 = sorted(toks, key=toks.get, reverse=True)[1]\n",
    "        cf3 = sorted(toks, key=toks.get, reverse=True)[2]\n",
    "        print (\"\\tfollowed by:\\n\\t\",\\\n",
    "                    cf2, \"appearing \", combo_freq[cf2], \"times, and \\n\\t\",\\\n",
    "                   list(sorted(toks))[2], \"appearing \", combo_freq[list(sorted(toks))[2]], \"times\\n\\n\")\n",
    "    \n",
    "    \n",
    "    print (\"Most likely matches in sub-corpora:\\n\")\n",
    "    \n",
    "    if best_match(term, doc_freq) is not None:\n",
    "           print(\"\\tDocuments:\", best_match(term, doc_freq)[0], \"appearing \", best_match(term, doc_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    if best_match(term, nar_freq) is not None:\n",
    "           print (\"\\tNarrative texts:\", best_match(term, nar_freq)[0], \"apprearing\", best_match(term, nar_freq)[1], \"times;\\n\\n\")\n",
    "\n",
    "    if best_match(term, indo_freq) is not None:\n",
    "           print (\"\\tIndic texts:\", best_match(term, indo_freq)[0], \"appearing \", best_match(term, indo_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    if best_match(term, trans_freq) is not None:\n",
    "           print (\"\\tTransoxania texts:\", best_match(term, trans_freq)[0], \"appearing \", best_match(term, trans_freq)[1], \"times;\\n\")\n",
    "    \n",
    "\n",
    "    print (\"\\nMost likely matches in Persian literature corpus:\\n\\t\")\n",
    "           \n",
    "    if best_match(term, pers_lit_freq) is not None:\n",
    "           print (\"\\t\",best_match(term, pers_lit_freq)[0], \"appearing \", best_match(term, pers_lit_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#match_freq(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_dic (term):\n",
    "    \n",
    "    match_freq(term)\n",
    "    \n",
    "    search_term = re.compile(term)\n",
    "    \n",
    "    glos_query_mask = glossary[\"Term\"].str.contains(search_term, na=False)\n",
    "    glos_query = glossary[glos_query_mask][[\"UID\", \"Term\", \"Translation\"]]\n",
    "    glos_query\n",
    "    \n",
    "    \n",
    "    dehkhoda_query_mask = dehkhoda[\"Term\"].str.contains(search_term, na=False)\n",
    "    dehkhoda_query = dehkhoda[dehkhoda_query_mask]\n",
    "    dehkhoda_query    \n",
    "    \n",
    "    melz_query_mask = meltzer[\"Präs.-Stamm\"].str.contains(search_term, na=False)\n",
    "    melz_query = meltzer[melz_query_mask]\n",
    "    melz_query[[\"Präs.-Stamm\", \"Deutsch\"]]\n",
    "    \n",
    "    \n",
    "    result = print (\"Glossary \\n\\n\", glos_query,\"\\n\\n\\n\", \\\n",
    "                    \"Dehkhoda \\n\\n\", dehkhoda_query,\"\\n\\n\\n\",\\\n",
    "                    \"Von_Meltzer \\n\\n\", melz_query[[\"Präs.-Stamm\", \"Deutsch\"]])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi_dic (\"دارو+\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom KWIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New KWIC: copy pahlavi version, *but* add the sub-corpus as an additional argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find in Document\n",
    "\n",
    "def find_doc(d, s):\n",
    "    \n",
    "    \"\"\"This function takes a dictionary of 5-grams as the first argument,\\\n",
    "    a regex search term as the second argument, and returns the sequence of 5 words\"\"\"\n",
    "    \n",
    "    for v in d:\n",
    "        m = re.match(s, v[2])\n",
    "        if m is not None:\n",
    "            yield ' '.join(v)\n",
    "            \n",
    "\n",
    "# Note: Return sends a specified value back to its caller\n",
    "# whereas Yield can produce a sequence of values.\n",
    "\n",
    "\n",
    "# Example:\n",
    "## list(find_doc(five_grams['al_biruni_card_catalog_suleimanov_fond'], 'ف.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Corpus\n",
    "## Produces a generator object with the KWIC with associated work title\n",
    "\n",
    "def find_corpus(c, s):\n",
    "    for k, d in c.items():\n",
    "        for m in find_doc(d, s):\n",
    "            yield f'{k:50s}: {m}'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kwic(term, corpus=combo_five_grams):\n",
    "    \n",
    "    print('\\n'.join(find_corpus(corpus, term)))\n",
    "    \n",
    "    # todo: organize this by best match\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kwic(\"د.رو\", indo_five_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Conditional Frequency Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confreq (term, refine=\"none\"):\n",
    "    \n",
    "    \"\"\"Conditional frequency across multiple corpora and sub-corpora.\n",
    "        Takes a search term (no regex) for the first word in the bigram, \n",
    "        as well as an optional regex filter to narrow down the second word\n",
    "        in the bigram sequence.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    if refine == \"none\" and len(combo_cfd[term]) > 0:\n",
    "    \n",
    "        if len(combo_cfd[term]) > 0:\n",
    "            print (term, \" is most commonly followed by:\\n\\n\", combo_cfd[term].most_common(10))\n",
    "\n",
    "        \n",
    "\n",
    "        if len(combo_cfd[term]) > 0:\n",
    "            print(\"\\nWithin sub-corpora:\\n\")\n",
    "\n",
    "            # Still need to fill out sub-corpora\n",
    "\n",
    "            if len(doc_cfd[term]) > 0 :\n",
    "                print (\"\\tDocuments:\", term, \" is most commonly followed by:\\n\\n\\t\", doc_cfd[term].most_common(5))\n",
    "            if len(nar_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tNarrative texts:\", term, \" is most commonly followed by:\\n\\n\\t\", nar_cfd[term].most_common(5))\n",
    "            if len(indo_cfd[term]) > 0 :\n",
    "                print (\"\\n\\n\\tIndic texts:\", term, \" is most commonly followed by:\\n\\n\\t\", indo_cfd[term].most_common(5))\n",
    "            if len(trans_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tTransoxania texts:\", term, \" is most commonly followed by:\\n\\n\\t\", trans_cfd[term].most_common(5))\n",
    "    \n",
    "    # Optional regex refinement of the results:\n",
    "    if refine != \"none\" and len(combo_cfd[term]) > 0:\n",
    "        \n",
    "        filt = re.compile(refine)\n",
    "        \n",
    "        filt_toks = [(x, y) for (x, y) in combo_cfd[term].items() if re.match(refine, x)]\n",
    "        \n",
    "        print (\"With the results filtered by the regex search (\", refine, \"), the most likely words following,\", term, \"are:\\n\\t\", filt_toks)\n",
    "        \n",
    "    elif len(combo_cfd[term]) == 0:\n",
    "            print (\"no results\")\n",
    "        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confreq(\"قوش\")\n",
    "#help(confreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regcf (term):\n",
    "    \n",
    "    \"\"\"\n",
    "        From a regex search term finds the most frequent possible word, then returns \n",
    "        conditional frequency (from bigrams) across multiple corpora.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if best_match(term, combo_freq) is not None:\n",
    "        \n",
    "        local_match = best_match(term, combo_freq)[0]\n",
    "        \n",
    "        print (\"The most likly match (based on word frequency) for \", term, \" is \", local_match,\\\n",
    "              \"(with frequency\", best_match(term, combo_freq)[1], \").\\n\")\n",
    "        print (\"Conditional frequency of\", local_match, \"(combined corpus):\\n\\t\", combo_cfd[local_match].most_common(5))\n",
    "        \n",
    "        print (\"\\nSub-Corpora:\\n\")\n",
    "        \n",
    "        print (\"\\n\\tDocuments:\\n\\t\", doc_cfd[local_match].most_common(5))\n",
    "        print (\"\\n\\tNarrative texts:\\n\\t\", nar_cfd[local_match].most_common(5))\n",
    "        \n",
    "        print (\"\\n\\n\\tIndic texts:\\n\\t\", indo_cfd[local_match].most_common(5))\n",
    "        print (\"\\n\\tTransoxania texts:\\n\\t\", trans_cfd[local_match].most_common(5))\n",
    "        \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regcf(\"ق.ش\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: functions for 3-part confreq, and reverse confreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third term, if first two known:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Document Corpus (Meta-Corpus simply too computationally costly)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tricfd (first_term, second_term, refine=\"none\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given two words in a row, conditional frequency of the third word in the sequence.\n",
    "    Inputs: two words (in order), and an optional regex filter on the third word.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    #print (\"The pair \", first_term, second_term, \" is most commonly followed by :\\n\")\n",
    "    #output = combo_tricfd[(first_term, second_term)].most_common(10)\n",
    "    #print (output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if refine == \"none\" and len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "    \n",
    "        if len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "            print (\"The pair \", first_term, second_term, \" is most commonly followed by:\\n\\n\", combo_tricfd[(first_term, second_term)].most_common(10))\n",
    "\n",
    "\n",
    "        if len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "            print(\"\\nWithin sub-corpora:\\n\")\n",
    "\n",
    "            # Still need to fill out sub-corpora\n",
    "\n",
    "            if len(doc_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\tDocuments:\\n\\n\\t\", doc_tricfd[(first_term, second_term)].most_common(5))\n",
    "            if len(nar_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\n\\tNarrative texts:\\n\\n\\t\", nar_tricfd[(first_term, second_term)].most_common(5))\n",
    "            if len(indo_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\n\\n\\tIndic texts:\\n\\n\\t\", indo_tricfd[(first_term, second_term)].most_common(5))\n",
    "            if len(trans_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\n\\tTransoxania texts::\\n\\n\\t\", trans_tricfd[(first_term, second_term)].most_common(5))\n",
    "    \n",
    "    # Optional regex refinement of the results:\n",
    "    if refine != \"none\" and len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "        \n",
    "        filt = re.compile(refine)\n",
    "        \n",
    "        filt_toks = [(x, y) for (x, y) in combo_tricfd[(first_term, second_term)].items() if re.match(refine, x)]\n",
    "        \n",
    "        print (\"With the results filtered by the regex search (\", refine, \"), the most likely words following the pair \", first_term, second_term, \"are:\\n\\t\", filt_toks)\n",
    "        \n",
    "    elif len(combo_tricfd[(first_term, second_term)]) == 0:\n",
    "            print (\"no results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tricfd (\"بعد\", \"از\", \"خ+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversed conditional frequency, i.e. if second word in sequence known but not first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Meta-Corpus*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revcfd (term, refine=\"none\"):\n",
    "    \n",
    "    \n",
    "    \"\"\"Reverse conditional frequency (bigrams) across multiple corpora and sub-corpora.\n",
    "        Takes a search term (no regex) for the first word in the bigram, \n",
    "        as well as an optional regex filter to narrow down the second word\n",
    "        in the bigram sequence.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    if refine == \"none\" and len(rev_combo_cfd[term]) > 0:\n",
    "    \n",
    "        if len(rev_combo_cfd[term]) > 0:\n",
    "            print (term, \" is most commonly preceded by:\\n\\n\", rev_combo_cfd[term].most_common(10))\n",
    "\n",
    "        \n",
    "\n",
    "        if len(rev_combo_cfd[term]) > 0:\n",
    "            print(\"\\nWithin sub-corpora:\\n\")\n",
    "\n",
    "            # Still need to fill out sub-corpora\n",
    "\n",
    "            if len(rev_doc_cfd[term]) > 0 :\n",
    "                print (\"\\tDocuments:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_doc_cfd[term].most_common(5))\n",
    "            if len(rev_nar_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tNarrative texts:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_nar_cfd[term].most_common(5))\n",
    "            if len(rev_indo_cfd[term]) > 0 :\n",
    "                print (\"\\n\\n\\tIndic texts:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_indo_cfd[term].most_common(5))\n",
    "            if len(rev_trans_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tTransoxania texts:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_trans_cfd[term].most_common(5))\n",
    "    \n",
    "    # Optional regex refinement of the results:\n",
    "    if refine != \"none\" and len(rev_combo_cfd[term]) > 0:\n",
    "        \n",
    "        filt = re.compile(refine)\n",
    "        \n",
    "        filt_toks = [(x, y) for (x, y) in rev_combo_cfd[term].items() if re.match(refine, x)]\n",
    "        \n",
    "        print (\"With the results filtered by the regex search (\", refine, \"), the most likely words preceding,\", term, \"are:\\n\\t\", filt_toks)\n",
    "        \n",
    "    elif len(rev_combo_cfd[term]) == 0:\n",
    "            print (\"no results\")\n",
    "        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revcfd(\"قوش\", \"خد\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCombined Token Corpora:\n",
      "        \t Narrative Sources from India: comb_india_nar_toks\n",
      "        \t Narrative Sources from Transoxania: comb_trans_nar_toks\n",
      "\n",
      "        \t All Narrative Sources: nar_corpus_toks\n",
      "        \t All Document Sources: doc_corpus_toks\n",
      "\n",
      "        \t Documents and Narrative Sources: combined_corpus_toks\n",
      "        \t Mega Corpus including Persian lit. corpus: mega_corpus_toks\n",
      "\n",
      "\n",
      "        Individual Corpora:\n",
      "        \t External Indic Corpus: indo_nar_ext_toks\n",
      "        \t External Transoxania Corpus: trans_nar_ext_toks\n",
      "\n",
      "        \t Khiva Turkic Document Corpus: khiva_doc_toks\n",
      "\n",
      "        \t Internal India Narrative Corpus: indo_nar_toks\n",
      "        \t Internal Transoxania Narrative orpus: trans_nar_toks\n",
      "\n",
      "        \t XML-stage Transoxania Documents: trans_xml_toks\n",
      "        \t XML-stage Indic Documents: indo_xml_toks\n",
      "        \t XML-stage Hyderabad Documents: hyd_xml_toks\n",
      "\n",
      "        \t\n"
     ]
    }
   ],
   "source": [
    "corpora_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----\n",
    "# Graveyard\n",
    "(i.e. code saved for posterity, no longer active)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# for whatever reason you can't just use the concordance method on a string;\n",
    "# you have to convert it to an NLTK Text type one way or another\n",
    "\n",
    "trans_corpus = nltk.Text(raw_combo_toks)\n",
    "\n",
    "#trans_corpus.concordance('خانه')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tokens in corpus regex matching the string:*\n",
    "\n",
    "(obsolete with custom KWIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "toks = [x for x in combo_freq if re.match(r'...خوی', x)]\n",
    "toks[:5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "conc0 = sum([trans_corpus.concordance_list(x) for x in toks], [])\n",
    "conc1 = [c.line for c in conc0]\n",
    "print('\\n'.join(conc1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom KWIC (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(drafting, active version now as a function, saved in markdown for posterity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Creating 5-Grams\n",
    "\n",
    "five_grams = {k:list(nltk.ngrams(v, 5)) for (k,v) in combined_corpus_toks.items() if len(v) >= 5}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Find in Document\n",
    "## This function takes a dictionary of 5-grams as the first argument,\n",
    "## a regex search term as the second argument, and returns the sequence of 5 words\n",
    "\n",
    "def find_doc(d, s):\n",
    "    for v in d:\n",
    "        m = re.match(s, v[2])\n",
    "        if m is not None:\n",
    "            yield ' '.join(v)\n",
    "            \n",
    "\n",
    "# Note: Return sends a specified value back to its caller\n",
    "# whereas Yield can produce a sequence of values.\n",
    "\n",
    "\n",
    "# Example:\n",
    "## list(find_doc(five_grams['al_biruni_card_catalog_suleimanov_fond'], 'ف.'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Find Corpus\n",
    "## Produces a generator object with the KWIC with associated work title\n",
    "\n",
    "def find_corpus(c, s):\n",
    "    for k, d in five_grams.items():\n",
    "        for m in find_doc(d, s):\n",
    "            yield f'{k:50s}: {m}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Formatting\n",
    "\n",
    "def print_align(v, m):\n",
    "    plen = max([sum([len(z)+1 for z in x[:m]]) for x in v])\n",
    "    for x in v:\n",
    "        pre = ' '.join(x[:m])\n",
    "        mid = x[m]\n",
    "        pos = ' '.join(x[m+1:])\n",
    "        print(f'{pre:>{plen}s} \\033[1m{mid}\\033[0m {pos}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print('\\n'.join(find_corpus(five_grams, '^من.قر?$')))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Meta-Corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# ConditionalFreqDist() takes a list of pairs.\n",
    "# Generator variable uses itself up upon assignment, so need to recreate above\n",
    "\n",
    "bigrams_cfd = nltk.ngrams(raw_combo_toks, 2)\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(bigrams_cfd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Conditional Frequency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Meta-Corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "search_term = r\"جهد\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print (search_term, \" is most commonly followed by:\\n\")\n",
    "cfd[search_term].most_common(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Document Corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "bigrams_doc_fd = nltk.ngrams(raw_doc_toks, 2)\n",
    "\n",
    "cfd_doc = nltk.ConditionalFreqDist(bigrams_doc_fd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "search_term = \"بداند\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print (\"\\nin the documents corpus, \", search_term, \" is most commonly followed by: \\n\")\n",
    "cfd_doc[search_term].most_common(5)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
