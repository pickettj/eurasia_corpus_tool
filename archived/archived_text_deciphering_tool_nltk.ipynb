{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Deciphering Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, re, nltk, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas import DataFrame, Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general function in Pandas to set maximum number of\n",
    "\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set home directory path\n",
    "hdir = os.path.expanduser('~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sister files:\n",
    "- Pickled corpora cleaned in text_cleaning_tokenizing (optional: run `text_cleaning_tokenizing.py` - may take a few minutes to execute)\n",
    "- Corpora stats in corpora_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Corpora\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = hdir + \"/Dropbox/Active_Directories/Digital_Humanities/Corpora/pickled_tokenized_cleaned_corpora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe corpus\n",
    "\n",
    "df_eurcorp = pd.read_csv (os.path.join(pickle_path,r'eurasia_corpus.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>No</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17622766</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>seyf.divan</td>\n",
       "      <td>24958</td>\n",
       "      <td>آن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14141394</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>rumi_moulavi.divan</td>\n",
       "      <td>54220</td>\n",
       "      <td>در</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14946165</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>moulavi.masnavi</td>\n",
       "      <td>262446</td>\n",
       "      <td>و</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15596612</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>saeb.divan</td>\n",
       "      <td>146343</td>\n",
       "      <td>چون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994724</th>\n",
       "      <td>indo_nar_ext_toks</td>\n",
       "      <td>mu_vol2</td>\n",
       "      <td>31094</td>\n",
       "      <td>کرده</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Category                Text      No Token\n",
       "17622766      pers_lit_toks          seyf.divan   24958    آن\n",
       "14141394      pers_lit_toks  rumi_moulavi.divan   54220    در\n",
       "14946165      pers_lit_toks     moulavi.masnavi  262446     و\n",
       "15596612      pers_lit_toks          saeb.divan  146343   چون\n",
       "994724    indo_nar_ext_toks             mu_vol2   31094  کرده"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying import took\n",
    "df_eurcorp.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived code from pickling era*:\n",
    "\n",
    "```python\n",
    "with open(pickle_path + \"/corpora.pkl\", \"rb\") as f:\n",
    "    unsorted_doc_toks,\\\n",
    "                indo_xml_toks, hyd_xml_toks, trans_xml_toks,\\\n",
    "                trans_nar_toks, indo_nar_toks,\\\n",
    "                trans_nar_ext_toks, indo_nar_ext_toks, khiva_doc_toks = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with open(pickle_path + \"/meta_corpora.pkl\", \"rb\") as f:\n",
    "    comb_india_nar_toks, comb_trans_nar_toks, nar_corpus_toks, doc_corpus_toks,\\\n",
    "                comb_india_toks, comb_trans_toks, comb_turk_toks,\\\n",
    "                combined_corpus_toks, mega_corpus_toks = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived II. Importing Raw Tokens; I.e. tokens without parent text designation, i.e. format necessary for many NLTK routines.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with open(pickle_path + \"/raw_tokens.pkl\", \"rb\") as f:\n",
    "    raw_doc_toks, raw_nar_toks, raw_indo_toks,\\\n",
    "                 raw_trans_toks, raw_lit_toks, raw_combo_toks, raw_turk_toks = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived: Importing Pre-processed NTLK Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pickle_data_path = hdir + \"/Box/Notes/Digital_Humanities/Corpora/pickled_nltk_data\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived: NLTK Word Frequencies*\n",
    "\n",
    "```python\n",
    "with open(pickle_data_path + \"/frequencies.pkl\", \"rb\") as f:\n",
    "    combo_freq, pers_lit_freq,\\\n",
    "                indo_freq, trans_freq,\\\n",
    "                nar_freq, doc_freq,\\\n",
    "                turk_freq = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived: NLTK Conditional Frequency Dictionaries (raw tokens)*\n",
    "\n",
    "```python\n",
    "with open(pickle_data_path + \"/cfd.pkl\", \"rb\") as f:\n",
    "    combo_cfd,\\\n",
    "                indo_cfd, trans_cfd,\\\n",
    "                nar_cfd, doc_cfd,\\\n",
    "                turk_cfd,\\\n",
    "                rev_combo_cfd,\\\n",
    "                rev_indo_cfd, rev_trans_cfd,\\\n",
    "                rev_nar_cfd, rev_doc_cfd,\\\n",
    "                rev_turk_cfd = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived: NLTK 5-grams (tokens by work)*\n",
    "\n",
    "```python\n",
    "with open(pickle_data_path + \"/fivegrams.pkl\", \"rb\") as f:\n",
    "    combo_five_grams,\\\n",
    "                indo_five_grams, trans_five_grams,\\\n",
    "                nar_five_grams, doc_five_grams,\\\n",
    "                turk_five_grams = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Archived: Three-way Conditional Frequency Dictionaries (raw tokens)*\n",
    "\n",
    "```python\n",
    "with open(pickle_data_path + \"/tri_cfd.pkl\", \"rb\") as f:\n",
    "    combo_tricfd,\\\n",
    "                indo_tricfd, trans_tricfd,\\\n",
    "                nar_tricfd, doc_tricfd,\\\n",
    "                turk_tricfd = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Von Melzer Persian Lexicon\n",
    "- Glossary\n",
    "- Place Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset path\n",
    "\n",
    "ds_path = hdir + \"/Dropbox/Active_Directories/Digital_Humanities/Datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Von Melzer\n",
    "meltzer = pd.read_csv(ds_path + \"/von_melzer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Persisch</th>\n",
       "      <th>Präs.-Stamm</th>\n",
       "      <th>Transkription</th>\n",
       "      <th>Deutsch</th>\n",
       "      <th>Bemerkung</th>\n",
       "      <th>Quellenangaben</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29551</th>\n",
       "      <td>29552</td>\n",
       "      <td>II</td>\n",
       "      <td>10767</td>\n",
       "      <td>737</td>\n",
       "      <td>‫جرم‬</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ǧorm⁺</td>\n",
       "      <td>Beschuldigung (f.)</td>\n",
       "      <td>Rosen 1925:50a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4405</th>\n",
       "      <td>4406</td>\n",
       "      <td>I</td>\n",
       "      <td>4405</td>\n",
       "      <td>4406</td>\n",
       "      <td>‫ﺁرزومند‬</td>\n",
       "      <td>ārzū-mand</td>\n",
       "      <td>begehrlich; begierig</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rūdakī (Dīvān 5/2), Tārīḫ-i Ṭabarī 528/8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44559</th>\n",
       "      <td>44560</td>\n",
       "      <td>III</td>\n",
       "      <td>6966</td>\n",
       "      <td>876</td>\n",
       "      <td>‫سفت‬</td>\n",
       "      <td>seft</td>\n",
       "      <td>grob; rauh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FR I 27/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55413</th>\n",
       "      <td>55414</td>\n",
       "      <td>III</td>\n",
       "      <td>17820</td>\n",
       "      <td>538</td>\n",
       "      <td>‫ﻗدح‬</td>\n",
       "      <td>qadh⁺</td>\n",
       "      <td>Tadel (m.)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Armaġān XVI 68/13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10725</th>\n",
       "      <td>10726</td>\n",
       "      <td>I</td>\n",
       "      <td>10725</td>\n",
       "      <td>10726</td>\n",
       "      <td>‫اوﻟو‬</td>\n",
       "      <td>olū⁺</td>\n",
       "      <td>Herren</td>\n",
       "      <td>(vor ‫ال‬- al-: olo)</td>\n",
       "      <td>Wahrmund 1898:I 143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         UID Volume  Unnamed: 2  Persisch Präs.-Stamm Transkription  \\\n",
       "29551  29552     II       10767       737       ‫جرم‬           NaN   \n",
       "4405    4406      I        4405      4406   ‫ﺁرزومند‬     ārzū-mand   \n",
       "44559  44560    III        6966       876       ‫سفت‬          seft   \n",
       "55413  55414    III       17820       538       ‫ﻗدح‬         qadh⁺   \n",
       "10725  10726      I       10725     10726      ‫اوﻟو‬          olū⁺   \n",
       "\n",
       "                    Deutsch             Bemerkung  \\\n",
       "29551                 ǧorm⁺    Beschuldigung (f.)   \n",
       "4405   begehrlich; begierig                   NaN   \n",
       "44559            grob; rauh                   NaN   \n",
       "55413            Tadel (m.)                   NaN   \n",
       "10725                Herren  (vor ‫ال‬- al-: olo)   \n",
       "\n",
       "                                 Quellenangaben  \n",
       "29551                            Rosen 1925:50a  \n",
       "4405   Rūdakī (Dīvān 5/2), Tārīḫ-i Ṭabarī 528/8  \n",
       "44559                                FR I 27/21  \n",
       "55413                         Armaġān XVI 68/13  \n",
       "10725                       Wahrmund 1898:I 143  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifying import\n",
    "#meltzer[\"Präs.-Stamm\"].sample(5)\n",
    "meltzer.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations\n",
    "locations = pd.read_csv(ds_path + '/exported_database_data/locations.csv', names=['UID', 'Ar_Names', \\\n",
    "                                                'Lat_Name', 'Nickname', 'Type'])\n",
    "# Social Roles\n",
    "roles = pd.read_csv(ds_path + '/exported_database_data/roles.csv', names=['UID', 'Term', 'Emic', 'Etic', 'Scope'])\n",
    "\n",
    "# Glossary\n",
    "glossary = pd.read_csv(ds_path + '/exported_database_data/glossary.csv', names=['UID', 'Term', \\\n",
    "                                                'Eng_Term', 'Translation', 'Transliteration', 'Scope', 'Tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dehkhoda = pd.read_csv(ds_path + \"/dehkhoda_dictionary.csv\", names=['Term', 'Definition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4885</th>\n",
       "      <td>تک‌رو، تکرو</td>\n",
       "      <td>1   خودسر، خودمحور 2 جمع‌گریز، جماعت‌گریز 2 ت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6597</th>\n",
       "      <td>حظ</td>\n",
       "      <td>1 التذاذ، کیف، لذت، خوشی 2 بهره، سهم، نصیب 3 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>بدخو</td>\n",
       "      <td>آتشی‌مزاج، اخمو، بداخلاق، خشن، عصبی، ترش‌رو، ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3824</th>\n",
       "      <td>تاب خوردن</td>\n",
       "      <td>1 درپیچ وتاب شدن 2 تاب بازی کردن 3 دور زدن 4 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8182</th>\n",
       "      <td>درون</td>\n",
       "      <td>1 اندر، اندرون، تو، داخل 2 باطن، نهاد، وجدان ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Term                                         Definition\n",
       "4885  تک‌رو، تکرو    1   خودسر، خودمحور 2 جمع‌گریز، جماعت‌گریز 2 ت...\n",
       "6597           حظ    1 التذاذ، کیف، لذت، خوشی 2 بهره، سهم، نصیب 3 ...\n",
       "1863         بدخو    آتشی‌مزاج، اخمو، بداخلاق، خشن، عصبی، ترش‌رو، ...\n",
       "3824    تاب خوردن    1 درپیچ وتاب شدن 2 تاب بازی کردن 3 دور زدن 4 ...\n",
       "8182          درون   1 اندر، اندرون، تو، داخل 2 باطن، نهاد، وجدان ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verifying import\n",
    "dehkhoda.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex reminders:\n",
    "- Just the word itself: `^مال$`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = re.compile(r\"ب.د\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Von Melzer Persian Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melz_query_mask = meltzer[\"Präs.-Stamm\"].str.contains(search_term, na=False)\n",
    "melz_query = meltzer[melz_query_mask]\n",
    "#melz_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Technical Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glos_query_mask = glossary[\"Term\"].str.contains(search_term, na=False)\n",
    "glos_query = glossary[glos_query_mask]\n",
    "#glos_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Social Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_query_mask = roles[\"Emic\"].str.contains(search_term, na=False)\n",
    "roles_query = roles[roles_query_mask]\n",
    "#roles_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Place Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_query_mask = locations[\"Ar_Names\"].str.contains(search_term, na=False)\n",
    "loc_query = locations[loc_query_mask]\n",
    "#loc_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpora_guide ():\n",
    "    print(\n",
    "        \"\\tCombined Token Corpora:\\n\\\n",
    "        \\t Narrative Sources from India: comb_india_nar_toks\\n\\\n",
    "        \\t Narrative Sources from Transoxania: comb_trans_nar_toks\\n\\n\\\n",
    "        \\t All Narrative Sources: nar_corpus_toks\\n\\\n",
    "        \\t All Document Sources: doc_corpus_toks\\n\\n\\\n",
    "        \\t Documents and Narrative Sources: combined_corpus_toks\\n\\\n",
    "        \\t Mega Corpus including Persian lit. corpus: mega_corpus_toks\\n\\n\\n\\\n",
    "        Individual Corpora:\\n\\\n",
    "        \\t External Indic Corpus: indo_nar_ext_toks\\n\\\n",
    "        \\t External Transoxania Corpus: trans_nar_ext_toks\\n\\n\\\n",
    "        \\t Khiva Turkic Document Corpus: khiva_doc_toks\\n\\n\\\n",
    "        \\t Internal India Narrative Corpus: indo_nar_toks\\n\\\n",
    "        \\t Internal Transoxania Narrative orpus: trans_nar_toks\\n\\n\\\n",
    "        \\t XML-stage Transoxania Documents: trans_xml_toks\\n\\\n",
    "        \\t XML-stage Indic Documents: indo_xml_toks\\n\\\n",
    "        \\t XML-stage Hyderabad Documents: hyd_xml_toks\\n\\n\\\n",
    "        \\t\"\n",
    "                 \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Another way of doing max value](https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary):\n",
    "\n",
    "```python\n",
    "def keywithmaxval(d):\n",
    "     \"\"\" a) create a list of the dict's keys and values; \n",
    "         b) return the key with the max value\"\"\"  \n",
    "     v=list(d.values())\n",
    "     k=list(d.keys())\n",
    "     return k[v.index(max(v))]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_match (term, corpus):\n",
    "    \n",
    "    \"\"\"Takes a search term and frequency dictionary, returns the most frequently\\\n",
    "    appearing match within the specified corpus as [matching term, frequency of appearnace].\"\"\"\n",
    "    \n",
    "    search_term = re.compile(term)\n",
    "    toks = {k:v for (k,v) in corpus.items() if re.match(search_term, k)}\n",
    "    if len(toks) > 0:\n",
    "        match = sorted(toks, key=toks.get, reverse=True)[0]\n",
    "        freq = corpus[match]\n",
    "        pair = [match, freq]\n",
    "    \n",
    "    else:\n",
    "        pair = None\n",
    "    \n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help (best_match)\n",
    "\n",
    "#best_match (\"error\", nar_freq)\n",
    "\n",
    "#best_match(\"د.رو\", doc_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_freq(term):\n",
    "    \n",
    "   \n",
    "    \n",
    "    if best_match(term, combo_freq) is not None:\n",
    "        print (\"Most likely match in corpus:\\n\\n\\t\",\\\n",
    "              best_match(term, combo_freq)[0], \"appearing \", best_match(term, combo_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    search_term = re.compile(term)\n",
    "    toks = {k:v for (k,v) in combo_freq.items() if re.match(search_term, k)}\n",
    "    if len(toks) > 3:\n",
    "        cf2 = sorted(toks, key=toks.get, reverse=True)[1]\n",
    "        cf3 = sorted(toks, key=toks.get, reverse=True)[2]\n",
    "        print (\"\\tfollowed by:\\n\\t\",\\\n",
    "                    cf2, \"appearing \", combo_freq[cf2], \"times, and \\n\\t\",\\\n",
    "                   list(sorted(toks))[2], \"appearing \", combo_freq[list(sorted(toks))[2]], \"times\\n\\n\")\n",
    "    \n",
    "    \n",
    "    print (\"Most likely matches in sub-corpora:\\n\")\n",
    "    \n",
    "    if best_match(term, doc_freq) is not None:\n",
    "           print(\"\\tDocuments:\", best_match(term, doc_freq)[0], \"appearing \", best_match(term, doc_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    if best_match(term, nar_freq) is not None:\n",
    "           print (\"\\tNarrative texts:\", best_match(term, nar_freq)[0], \"apprearing\", best_match(term, nar_freq)[1], \"times;\\n\\n\")\n",
    "\n",
    "    if best_match(term, indo_freq) is not None:\n",
    "           print (\"\\tIndic texts:\", best_match(term, indo_freq)[0], \"appearing \", best_match(term, indo_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    if best_match(term, trans_freq) is not None:\n",
    "           print (\"\\tTransoxania texts:\", best_match(term, trans_freq)[0], \"appearing \", best_match(term, trans_freq)[1], \"times;\\n\")\n",
    "    \n",
    "\n",
    "    print (\"\\nMost likely matches in Persian literature corpus:\\n\\t\")\n",
    "           \n",
    "    if best_match(term, pers_lit_freq) is not None:\n",
    "           print (\"\\t\",best_match(term, pers_lit_freq)[0], \"appearing \", best_match(term, pers_lit_freq)[1], \"times;\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#match_freq(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_dic (term):\n",
    "    \n",
    "    match_freq(term)\n",
    "    \n",
    "    search_term = re.compile(term)\n",
    "    \n",
    "    glos_query_mask = glossary[\"Term\"].str.contains(search_term, na=False)\n",
    "    glos_query = glossary[glos_query_mask][[\"UID\", \"Term\", \"Translation\"]]\n",
    "    glos_query\n",
    "    \n",
    "    \n",
    "    dehkhoda_query_mask = dehkhoda[\"Term\"].str.contains(search_term, na=False)\n",
    "    dehkhoda_query = dehkhoda[dehkhoda_query_mask]\n",
    "    dehkhoda_query    \n",
    "    \n",
    "    melz_query_mask = meltzer[\"Präs.-Stamm\"].str.contains(search_term, na=False)\n",
    "    melz_query = meltzer[melz_query_mask]\n",
    "    melz_query[[\"Präs.-Stamm\", \"Deutsch\"]]\n",
    "    \n",
    "    \n",
    "    result = print (\"Glossary \\n\\n\", glos_query,\"\\n\\n\\n\", \\\n",
    "                    \"Dehkhoda \\n\\n\", dehkhoda_query,\"\\n\\n\\n\",\\\n",
    "                    \"Von_Meltzer \\n\\n\", melz_query[[\"Präs.-Stamm\", \"Deutsch\"]])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi_dic (\"دارو+\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom KWIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more efficent to filter to have version without the literature corpus?\n",
    "# or more efficient to filter it out within the functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New KWIC: copy pahlavi version, *but* add the sub-corpus as an additional argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>No</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7840034</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ferdousi.shahname</td>\n",
       "      <td>507529</td>\n",
       "      <td>کزین</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17270324</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>sanai.divan</td>\n",
       "      <td>168866</td>\n",
       "      <td>اندر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18235481</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>vahshi.nazerumanzur</td>\n",
       "      <td>18357</td>\n",
       "      <td>به</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7703210</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ferdousi.shahname</td>\n",
       "      <td>370705</td>\n",
       "      <td>توایم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12701170</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>nezami.sharafname</td>\n",
       "      <td>19452</td>\n",
       "      <td>نوازست</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Category                 Text      No   Token\n",
       "7840034   pers_lit_toks    ferdousi.shahname  507529    کزین\n",
       "17270324  pers_lit_toks          sanai.divan  168866    اندر\n",
       "18235481  pers_lit_toks  vahshi.nazerumanzur   18357      به\n",
       "7703210   pers_lit_toks    ferdousi.shahname  370705   توایم\n",
       "12701170  pers_lit_toks    nezami.sharafname   19452  نوازست"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eurcorp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_eurcorp.filter(items = [14208103], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Category                Text      No Token\n",
      "14208103  pers_lit_toks  rumi_moulavi.divan  120929    به\n"
     ]
    }
   ],
   "source": [
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120924"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc = int(df['No'])-5\n",
    "loc = int(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[120924,\n",
       " 120925,\n",
       " 120926,\n",
       " 120927,\n",
       " 120928,\n",
       " 120929,\n",
       " 120930,\n",
       " 120931,\n",
       " 120932,\n",
       " 120933,\n",
       " 120934,\n",
       " 120935,\n",
       " 120936,\n",
       " 120937,\n",
       " 120938]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(loc,loc+15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def index_kwic (term, category=None, exclusion=True):\n",
    "    \n",
    "    \"\"\"This function returns a dataframe filtered by the search term.\"\"\"\n",
    "    df_eurcorp_lite = df_eurcorp\n",
    "\n",
    "    \n",
    "    if exclusion:\n",
    "        df_eurcorp_lite = df_eurcorp_lite[df_eurcorp_lite['Category'] != 'pers_lit_toks']\n",
    "        \n",
    "    if category is not None:\n",
    "        df_eurcorp_lite = df_eurcorp_lite[df_eurcorp_lite['Category'] == category]\n",
    "\n",
    "        \n",
    "    result = df_eurcorp_lite[df_eurcorp_lite['Token'].str.match(term)]\n",
    "    return result\n",
    "    \n",
    "    # str.match; the str part is telling match how to behave; .match is a method specific to pandas\n",
    "    \n",
    "    \n",
    "# add regex functionality: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index_kwic('اژدها')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kwic (term):\n",
    "    \n",
    "    for i, item in index_kwic(term).iterrows():\n",
    "        \n",
    "        title = item[\"Text\"]\n",
    "        category = item[\"Category\"]\n",
    "        loc = item['No']\n",
    "        \n",
    "        filtered = df_eurcorp[(df_eurcorp['Text']==title)]\n",
    "        filt_toks = filtered[(filtered['No']>=(loc-5))&(filtered['No']<=(loc+5))]\n",
    "\n",
    "        \n",
    "        #filtered = filtered.sort_values(\"index\")\n",
    "        # probably already sorted, but better to be on the safe side\n",
    "                \n",
    "        text = \" \".join(filt_toks[\"Token\"])\n",
    "        \n",
    "        print(f'{title}: {loc} {category}\\n{text}\\n')\n",
    "        \n",
    "        # task: figure out how to color code results; termcolor package, has to be installed\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# iterrows(): research what this does exactly, has something to do with dataframes being composed of series\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_eurcorp[df_eurcorp['Category'] != 'pers_lit_toks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ain-i_akbari_murty: 40767 indo_nar_ext_toks\n",
      "پت سردار ماران ماری بر اژدها سوار نگارند و وزیررا ماری\n",
      "\n",
      "khumuli: 10273 trans_nar_ext_toks\n",
      "بیضا در صورت عصا از اژدها نفس اماره خلاص یافت بعد\n",
      "\n",
      "khumuli: 10525 trans_nar_ext_toks\n",
      "شیران وحدتند هرچند آدمند عصا اژدها کنند هر چند آدمند عشق\n",
      "\n",
      "tarikh-i_jadida_tashkent_ser725: 20195 trans_nar_ext_toks\n",
      "بآن پهلو بلند چراغرا بصورت اژدها چیده بر صورت دهن اژدها\n",
      "\n",
      "tarikh-i_jadida_tashkent_ser725: 20200 trans_nar_ext_toks\n",
      "اژدها چیده بر صورت دهن اژدها چراغ چیده اندکه بعد از\n",
      "\n",
      "tuhfa-i_taib_ser726: 5479 trans_nar_ext_toks\n",
      "تن بکشتن دادن و سربدهان اژدها نهادن ست باید دانست که\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kwic('اژ.ها$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dic = pd.value_counts(df_eurcorp.Token).to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>163390</th>\n",
       "      <td>نشارلارنکزدین</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170748</th>\n",
       "      <td>سجاعتمندی</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58797</th>\n",
       "      <td>بناوردگاه</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161976</th>\n",
       "      <td>الجکو</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192922</th>\n",
       "      <td>گناهگار</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                index  Token\n",
       "163390  نشارلارنکزدین      1\n",
       "170748      سجاعتمندی      1\n",
       "58797       بناوردگاه      6\n",
       "161976          الجکو      1\n",
       "192922        گناهگار      1"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dic.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = re.compile(r\"اژد.ا$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>اژدها</td>\n",
       "      <td>1018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37592</th>\n",
       "      <td>کاژدها</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132247</th>\n",
       "      <td>نراژدها</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174809</th>\n",
       "      <td>عصااژدها</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234963</th>\n",
       "      <td>اژدرا</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           index  Token\n",
       "2231       اژدها   1018\n",
       "37592     کاژدها     14\n",
       "132247   نراژدها      2\n",
       "174809  عصااژدها      1\n",
       "234963     اژدرا      1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_mask = freq_dic[\"index\"].str.contains(search_term, na=False)\n",
    "query = freq_dic[query_mask]\n",
    "query.head()\n",
    "\n",
    "# turn into a function, just return top hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confreq (term, group=False):\n",
    "    sel = df_eurcorp[df_eurcorp['Token']==term].copy()\n",
    "    sel['index_next'] = sel['No'] + 1\n",
    "    sel = sel.join(\n",
    "        df_eurcorp.set_index(['Text', 'Category', 'No'])['Token'].rename('token_next'),\n",
    "        on=['Text', 'Category', 'index_next']\n",
    "    )\n",
    "    # If there are only 1-frequency results, it will still show them;\n",
    "    # but if there are enough higher frequency results, it will omit the 1-frequency results.\n",
    "    result = sel['token_next'].value_counts()\n",
    "    short_result = [(x,y) for x,y in result.items() if y > 1]\n",
    "    if len(short_result) > 5:\n",
    "        result = short_result\n",
    "    # improvement: create a list of omitted words (e.g. ud, ī, etc.), and make a flag1=False\n",
    "    # optional argument to omit them.\n",
    "    \n",
    "    if group == True:\n",
    "        result = sel.groupby('Category')['token_next'].value_counts().rename(\"count\").reset_index()\n",
    "    \n",
    "    return (result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general function in Pandas to set maximum number of\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>token_next</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>indo_nar_ext_toks</td>\n",
       "      <td>سوار</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>را</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>در</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شد</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بر</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بود</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>و</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>به</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>که</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ز</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دید</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>از</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>سر</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>چون</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گر</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گردد</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شود</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بدین</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>خفته</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>پیکر</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کردار</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ازین</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>با</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بد</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بسپرد</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>تو</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دارد</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ماه</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نیست</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>همی</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>پیکرم</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>پیکری</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کرد</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گفت</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>مور</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گنج</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>آورده</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>اندر</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ای</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>باره</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>باز</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بردرید</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بی</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دل</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دیده</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>روی</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>سازد</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شدم</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>می</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ندانم</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نشود</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نه</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نیامد</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>هر</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>پیکرست</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>چو</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کزو</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کنون</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کو</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گشت</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>یک</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>اژدها</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>باشد</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دیدی</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>مرا</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کجا</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>آتش</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>آموختی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>آمیخته</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>آنک</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>آنکس</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>افتاد</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>افشردنست</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>انداخت</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>این</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بازگشت</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>باشدش</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بانگ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بدو</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بدوزی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>برآرد</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>برآمد</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بردی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>برشدی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>برگ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بریدم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بریده</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بسته</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بسپرم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بشمشیر</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بشکرد</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بشکند</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بغلتید</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بفرمان</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بمازندران</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بنگاشت</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بهمن</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بکندم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بکوشید</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بگذرد</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بگفت</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بگفتار</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بیامد</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بیم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بینی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>تن</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>توبه</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>تیرباران</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>تیز</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>جادویها</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>جان</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>جشنگاه</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>جنگجویی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>جهان</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>جهاندار</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>حذر</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>حزن</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>حیله</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>خبر</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>خوانم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>خود</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>خوی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>داد</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دادم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>داریم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>داشتند</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دام</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دانی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دستی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دلم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دنبال</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دهن</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دور</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>رزم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>رسته</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>رفت</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>رهایی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>روان</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>روح</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>زد</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>زشت</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>زمان</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>زنند</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>زو</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ساخته</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ساختی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>سازی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>سایه</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ستمدیده</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>سوی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>سیاوش</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شاه</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شدی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شورش</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شوم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شکری</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شیرمردی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شیری</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>صورتست</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>صولت</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>عازر</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>عشق</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>غم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>فرو</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>لیک</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ماند</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>مر</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>مرگ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>مشور</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>من</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>مکان</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>مگر</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ناگزیر</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نبردی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نجوید</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نخواهد</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ندانست</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نزنی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نشاید</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نظیری</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نمودی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نمی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نوح</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نکنند</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نگه</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نیارند</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نیز</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>هان</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>هرآنکس</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>هست</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>وگر</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>وگرنه</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>پر</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>پیشت</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>پیل</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>پیکرش</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>چنان</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>چنانت</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>چند</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>چه</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>چونک</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کار</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کردشان</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کز</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کس</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کشت</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کشیدی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کمان</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کمین</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کن</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کنم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کنند</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کنی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کهتران</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کوثر</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کوهها</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کیش</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کین</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گردن</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گرز</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گرچه</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گزیده</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گشتست</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گشته</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گشتی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گه</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>یا</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>یابی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>یار</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>یاران</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>یکی</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>آ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>آسای</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>آمد</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ادعوک</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>افعی</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>اوبار</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>اوفتادی</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>اینک</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بالش</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بایست</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>برد</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>برون</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بنگر</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بودی</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بچه</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>بین</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>تا</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>توسن</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>حد</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>حمله</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>خفتست</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>خوانند</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>خواهم</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>خور</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>خورد</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دار</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دارم</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دستم</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دلان</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>دندان</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ریخت</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>زده</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>زلفی</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>زنیم</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>سریم</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>سنگ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>سوار</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>سیرت</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شکار</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شکل</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شکلت</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>شکلست</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>صاحبدلان</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>صفتش</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>طینت</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>عقل</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>غرد</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>فرستادی</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>فرعون</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>قانع</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>قلعه</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ما</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>مار</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ماری</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>مشکل</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نبرد</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نجاتم</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نروی</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نشان</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نموده</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نمیم</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ننهاد</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نهاد</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نهنگ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نوش</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نکرد</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>نگیرد</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>هرگز</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>هم</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>همپاست</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>هندی</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>پبچان</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>پیش</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>چوب</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>چگونه</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کردن</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کرده</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کش</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>کلید</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>گریخته</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>trans_nar_ext_toks</td>\n",
       "      <td>نفس</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>trans_nar_ext_toks</td>\n",
       "      <td>نهادن</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>trans_nar_ext_toks</td>\n",
       "      <td>چراغ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>trans_nar_ext_toks</td>\n",
       "      <td>چیده</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>trans_nar_ext_toks</td>\n",
       "      <td>کنند</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Category token_next  count\n",
       "0     indo_nar_ext_toks       سوار      1\n",
       "1         pers_lit_toks         را    132\n",
       "2         pers_lit_toks         در     28\n",
       "3         pers_lit_toks         شد     27\n",
       "4         pers_lit_toks         بر     24\n",
       "5         pers_lit_toks        بود     19\n",
       "6         pers_lit_toks          و     19\n",
       "7         pers_lit_toks         به     18\n",
       "8         pers_lit_toks         که     17\n",
       "9         pers_lit_toks          ز     16\n",
       "10        pers_lit_toks        دید     12\n",
       "11        pers_lit_toks         از     11\n",
       "12        pers_lit_toks         سر     10\n",
       "13        pers_lit_toks        چون     10\n",
       "14        pers_lit_toks         گر      9\n",
       "15        pers_lit_toks       گردد      9\n",
       "16        pers_lit_toks        شود      8\n",
       "17        pers_lit_toks       بدین      7\n",
       "18        pers_lit_toks       خفته      7\n",
       "19        pers_lit_toks       پیکر      7\n",
       "20        pers_lit_toks      کردار      7\n",
       "21        pers_lit_toks       ازین      6\n",
       "22        pers_lit_toks         با      6\n",
       "23        pers_lit_toks         بد      6\n",
       "24        pers_lit_toks      بسپرد      6\n",
       "25        pers_lit_toks         تو      6\n",
       "26        pers_lit_toks       دارد      6\n",
       "27        pers_lit_toks        ماه      6\n",
       "28        pers_lit_toks       نیست      6\n",
       "29        pers_lit_toks        همی      6\n",
       "30        pers_lit_toks      پیکرم      6\n",
       "31        pers_lit_toks      پیکری      6\n",
       "32        pers_lit_toks        کرد      6\n",
       "33        pers_lit_toks        گفت      6\n",
       "34        pers_lit_toks        مور      5\n",
       "35        pers_lit_toks        گنج      5\n",
       "36        pers_lit_toks      آورده      4\n",
       "37        pers_lit_toks       اندر      4\n",
       "38        pers_lit_toks         ای      4\n",
       "39        pers_lit_toks       باره      4\n",
       "40        pers_lit_toks        باز      4\n",
       "41        pers_lit_toks     بردرید      4\n",
       "42        pers_lit_toks         بی      4\n",
       "43        pers_lit_toks         دل      4\n",
       "44        pers_lit_toks       دیده      4\n",
       "45        pers_lit_toks        روی      4\n",
       "46        pers_lit_toks       سازد      4\n",
       "47        pers_lit_toks        شدم      4\n",
       "48        pers_lit_toks         می      4\n",
       "49        pers_lit_toks      ندانم      4\n",
       "50        pers_lit_toks       نشود      4\n",
       "51        pers_lit_toks         نه      4\n",
       "52        pers_lit_toks      نیامد      4\n",
       "53        pers_lit_toks         هر      4\n",
       "54        pers_lit_toks     پیکرست      4\n",
       "55        pers_lit_toks         چو      4\n",
       "56        pers_lit_toks        کزو      4\n",
       "57        pers_lit_toks       کنون      4\n",
       "58        pers_lit_toks         کو      4\n",
       "59        pers_lit_toks        گشت      4\n",
       "60        pers_lit_toks         یک      4\n",
       "61        pers_lit_toks      اژدها      3\n",
       "62        pers_lit_toks       باشد      3\n",
       "63        pers_lit_toks       دیدی      3\n",
       "64        pers_lit_toks        مرا      3\n",
       "65        pers_lit_toks        کجا      3\n",
       "66        pers_lit_toks        آتش      2\n",
       "67        pers_lit_toks     آموختی      2\n",
       "68        pers_lit_toks     آمیخته      2\n",
       "69        pers_lit_toks        آنک      2\n",
       "70        pers_lit_toks       آنکس      2\n",
       "71        pers_lit_toks      افتاد      2\n",
       "72        pers_lit_toks   افشردنست      2\n",
       "73        pers_lit_toks     انداخت      2\n",
       "74        pers_lit_toks        این      2\n",
       "75        pers_lit_toks     بازگشت      2\n",
       "76        pers_lit_toks      باشدش      2\n",
       "77        pers_lit_toks       بانگ      2\n",
       "78        pers_lit_toks        بدو      2\n",
       "79        pers_lit_toks      بدوزی      2\n",
       "80        pers_lit_toks      برآرد      2\n",
       "81        pers_lit_toks      برآمد      2\n",
       "82        pers_lit_toks       بردی      2\n",
       "83        pers_lit_toks      برشدی      2\n",
       "84        pers_lit_toks        برگ      2\n",
       "85        pers_lit_toks      بریدم      2\n",
       "86        pers_lit_toks      بریده      2\n",
       "87        pers_lit_toks       بسته      2\n",
       "88        pers_lit_toks      بسپرم      2\n",
       "89        pers_lit_toks     بشمشیر      2\n",
       "90        pers_lit_toks      بشکرد      2\n",
       "91        pers_lit_toks      بشکند      2\n",
       "92        pers_lit_toks     بغلتید      2\n",
       "93        pers_lit_toks     بفرمان      2\n",
       "94        pers_lit_toks  بمازندران      2\n",
       "95        pers_lit_toks     بنگاشت      2\n",
       "96        pers_lit_toks       بهمن      2\n",
       "97        pers_lit_toks      بکندم      2\n",
       "98        pers_lit_toks     بکوشید      2\n",
       "99        pers_lit_toks      بگذرد      2\n",
       "100       pers_lit_toks       بگفت      2\n",
       "101       pers_lit_toks     بگفتار      2\n",
       "102       pers_lit_toks      بیامد      2\n",
       "103       pers_lit_toks        بیم      2\n",
       "104       pers_lit_toks       بینی      2\n",
       "105       pers_lit_toks         تن      2\n",
       "106       pers_lit_toks       توبه      2\n",
       "107       pers_lit_toks   تیرباران      2\n",
       "108       pers_lit_toks        تیز      2\n",
       "109       pers_lit_toks    جادویها      2\n",
       "110       pers_lit_toks        جان      2\n",
       "111       pers_lit_toks     جشنگاه      2\n",
       "112       pers_lit_toks    جنگجویی      2\n",
       "113       pers_lit_toks       جهان      2\n",
       "114       pers_lit_toks    جهاندار      2\n",
       "115       pers_lit_toks        حذر      2\n",
       "116       pers_lit_toks        حزن      2\n",
       "117       pers_lit_toks       حیله      2\n",
       "118       pers_lit_toks        خبر      2\n",
       "119       pers_lit_toks      خوانم      2\n",
       "120       pers_lit_toks        خود      2\n",
       "121       pers_lit_toks        خوی      2\n",
       "122       pers_lit_toks        داد      2\n",
       "123       pers_lit_toks       دادم      2\n",
       "124       pers_lit_toks      داریم      2\n",
       "125       pers_lit_toks     داشتند      2\n",
       "126       pers_lit_toks        دام      2\n",
       "127       pers_lit_toks       دانی      2\n",
       "128       pers_lit_toks       دستی      2\n",
       "129       pers_lit_toks        دلم      2\n",
       "130       pers_lit_toks         دم      2\n",
       "131       pers_lit_toks      دنبال      2\n",
       "132       pers_lit_toks        دهن      2\n",
       "133       pers_lit_toks        دور      2\n",
       "134       pers_lit_toks        رزم      2\n",
       "135       pers_lit_toks       رسته      2\n",
       "136       pers_lit_toks        رفت      2\n",
       "137       pers_lit_toks      رهایی      2\n",
       "138       pers_lit_toks       روان      2\n",
       "139       pers_lit_toks        روح      2\n",
       "140       pers_lit_toks         زد      2\n",
       "141       pers_lit_toks        زشت      2\n",
       "142       pers_lit_toks       زمان      2\n",
       "143       pers_lit_toks       زنند      2\n",
       "144       pers_lit_toks         زو      2\n",
       "145       pers_lit_toks      ساخته      2\n",
       "146       pers_lit_toks      ساختی      2\n",
       "147       pers_lit_toks       سازی      2\n",
       "148       pers_lit_toks       سایه      2\n",
       "149       pers_lit_toks    ستمدیده      2\n",
       "150       pers_lit_toks        سوی      2\n",
       "151       pers_lit_toks      سیاوش      2\n",
       "152       pers_lit_toks        شاه      2\n",
       "153       pers_lit_toks        شدی      2\n",
       "154       pers_lit_toks       شورش      2\n",
       "155       pers_lit_toks        شوم      2\n",
       "156       pers_lit_toks       شکری      2\n",
       "157       pers_lit_toks    شیرمردی      2\n",
       "158       pers_lit_toks       شیری      2\n",
       "159       pers_lit_toks     صورتست      2\n",
       "160       pers_lit_toks       صولت      2\n",
       "161       pers_lit_toks       عازر      2\n",
       "162       pers_lit_toks        عشق      2\n",
       "163       pers_lit_toks         غم      2\n",
       "164       pers_lit_toks        فرو      2\n",
       "165       pers_lit_toks        لیک      2\n",
       "166       pers_lit_toks       ماند      2\n",
       "167       pers_lit_toks         مر      2\n",
       "168       pers_lit_toks        مرگ      2\n",
       "169       pers_lit_toks       مشور      2\n",
       "170       pers_lit_toks         من      2\n",
       "171       pers_lit_toks       مکان      2\n",
       "172       pers_lit_toks        مگر      2\n",
       "173       pers_lit_toks     ناگزیر      2\n",
       "174       pers_lit_toks      نبردی      2\n",
       "175       pers_lit_toks      نجوید      2\n",
       "176       pers_lit_toks     نخواهد      2\n",
       "177       pers_lit_toks     ندانست      2\n",
       "178       pers_lit_toks       نزنی      2\n",
       "179       pers_lit_toks      نشاید      2\n",
       "180       pers_lit_toks      نظیری      2\n",
       "181       pers_lit_toks      نمودی      2\n",
       "182       pers_lit_toks        نمی      2\n",
       "183       pers_lit_toks        نوح      2\n",
       "184       pers_lit_toks      نکنند      2\n",
       "185       pers_lit_toks        نگه      2\n",
       "186       pers_lit_toks     نیارند      2\n",
       "187       pers_lit_toks        نیز      2\n",
       "188       pers_lit_toks        هان      2\n",
       "189       pers_lit_toks     هرآنکس      2\n",
       "190       pers_lit_toks        هست      2\n",
       "191       pers_lit_toks        وگر      2\n",
       "192       pers_lit_toks      وگرنه      2\n",
       "193       pers_lit_toks         پر      2\n",
       "194       pers_lit_toks       پیشت      2\n",
       "195       pers_lit_toks        پیل      2\n",
       "196       pers_lit_toks      پیکرش      2\n",
       "197       pers_lit_toks       چنان      2\n",
       "198       pers_lit_toks      چنانت      2\n",
       "199       pers_lit_toks        چند      2\n",
       "200       pers_lit_toks         چه      2\n",
       "201       pers_lit_toks       چونک      2\n",
       "202       pers_lit_toks        کار      2\n",
       "203       pers_lit_toks     کردشان      2\n",
       "204       pers_lit_toks         کز      2\n",
       "205       pers_lit_toks         کس      2\n",
       "206       pers_lit_toks        کشت      2\n",
       "207       pers_lit_toks      کشیدی      2\n",
       "208       pers_lit_toks       کمان      2\n",
       "209       pers_lit_toks       کمین      2\n",
       "210       pers_lit_toks         کن      2\n",
       "211       pers_lit_toks        کنم      2\n",
       "212       pers_lit_toks       کنند      2\n",
       "213       pers_lit_toks        کنی      2\n",
       "214       pers_lit_toks     کهتران      2\n",
       "215       pers_lit_toks       کوثر      2\n",
       "216       pers_lit_toks      کوهها      2\n",
       "217       pers_lit_toks        کیش      2\n",
       "218       pers_lit_toks        کین      2\n",
       "219       pers_lit_toks       گردن      2\n",
       "220       pers_lit_toks        گرز      2\n",
       "221       pers_lit_toks       گرچه      2\n",
       "222       pers_lit_toks      گزیده      2\n",
       "223       pers_lit_toks      گشتست      2\n",
       "224       pers_lit_toks       گشته      2\n",
       "225       pers_lit_toks       گشتی      2\n",
       "226       pers_lit_toks         گه      2\n",
       "227       pers_lit_toks         یا      2\n",
       "228       pers_lit_toks       یابی      2\n",
       "229       pers_lit_toks        یار      2\n",
       "230       pers_lit_toks      یاران      2\n",
       "231       pers_lit_toks        یکی      2\n",
       "232       pers_lit_toks          آ      1\n",
       "233       pers_lit_toks       آسای      1\n",
       "234       pers_lit_toks        آمد      1\n",
       "235       pers_lit_toks      ادعوک      1\n",
       "236       pers_lit_toks       افعی      1\n",
       "237       pers_lit_toks      اوبار      1\n",
       "238       pers_lit_toks    اوفتادی      1\n",
       "239       pers_lit_toks       اینک      1\n",
       "240       pers_lit_toks       بالش      1\n",
       "241       pers_lit_toks      بایست      1\n",
       "242       pers_lit_toks        برد      1\n",
       "243       pers_lit_toks       برون      1\n",
       "244       pers_lit_toks       بنگر      1\n",
       "245       pers_lit_toks       بودی      1\n",
       "246       pers_lit_toks        بچه      1\n",
       "247       pers_lit_toks        بین      1\n",
       "248       pers_lit_toks         تا      1\n",
       "249       pers_lit_toks       توسن      1\n",
       "250       pers_lit_toks         حد      1\n",
       "251       pers_lit_toks       حمله      1\n",
       "252       pers_lit_toks      خفتست      1\n",
       "253       pers_lit_toks     خوانند      1\n",
       "254       pers_lit_toks      خواهم      1\n",
       "255       pers_lit_toks        خور      1\n",
       "256       pers_lit_toks       خورد      1\n",
       "257       pers_lit_toks        دار      1\n",
       "258       pers_lit_toks       دارم      1\n",
       "259       pers_lit_toks       دستم      1\n",
       "260       pers_lit_toks       دلان      1\n",
       "261       pers_lit_toks      دندان      1\n",
       "262       pers_lit_toks       ریخت      1\n",
       "263       pers_lit_toks        زده      1\n",
       "264       pers_lit_toks       زلفی      1\n",
       "265       pers_lit_toks       زنیم      1\n",
       "266       pers_lit_toks       سریم      1\n",
       "267       pers_lit_toks        سنگ      1\n",
       "268       pers_lit_toks       سوار      1\n",
       "269       pers_lit_toks       سیرت      1\n",
       "270       pers_lit_toks       شکار      1\n",
       "271       pers_lit_toks        شکل      1\n",
       "272       pers_lit_toks       شکلت      1\n",
       "273       pers_lit_toks      شکلست      1\n",
       "274       pers_lit_toks   صاحبدلان      1\n",
       "275       pers_lit_toks       صفتش      1\n",
       "276       pers_lit_toks       طینت      1\n",
       "277       pers_lit_toks        عقل      1\n",
       "278       pers_lit_toks        غرد      1\n",
       "279       pers_lit_toks    فرستادی      1\n",
       "280       pers_lit_toks      فرعون      1\n",
       "281       pers_lit_toks       قانع      1\n",
       "282       pers_lit_toks       قلعه      1\n",
       "283       pers_lit_toks         ما      1\n",
       "284       pers_lit_toks        مار      1\n",
       "285       pers_lit_toks       ماری      1\n",
       "286       pers_lit_toks       مشکل      1\n",
       "287       pers_lit_toks       نبرد      1\n",
       "288       pers_lit_toks      نجاتم      1\n",
       "289       pers_lit_toks       نروی      1\n",
       "290       pers_lit_toks       نشان      1\n",
       "291       pers_lit_toks      نموده      1\n",
       "292       pers_lit_toks       نمیم      1\n",
       "293       pers_lit_toks      ننهاد      1\n",
       "294       pers_lit_toks       نهاد      1\n",
       "295       pers_lit_toks       نهنگ      1\n",
       "296       pers_lit_toks        نوش      1\n",
       "297       pers_lit_toks       نکرد      1\n",
       "298       pers_lit_toks      نگیرد      1\n",
       "299       pers_lit_toks       هرگز      1\n",
       "300       pers_lit_toks         هم      1\n",
       "301       pers_lit_toks     همپاست      1\n",
       "302       pers_lit_toks       هندی      1\n",
       "303       pers_lit_toks      پبچان      1\n",
       "304       pers_lit_toks        پیش      1\n",
       "305       pers_lit_toks        چوب      1\n",
       "306       pers_lit_toks      چگونه      1\n",
       "307       pers_lit_toks       کردن      1\n",
       "308       pers_lit_toks       کرده      1\n",
       "309       pers_lit_toks         کش      1\n",
       "310       pers_lit_toks       کلید      1\n",
       "311       pers_lit_toks     گریخته      1\n",
       "312  trans_nar_ext_toks        نفس      1\n",
       "313  trans_nar_ext_toks      نهادن      1\n",
       "314  trans_nar_ext_toks       چراغ      1\n",
       "315  trans_nar_ext_toks       چیده      1\n",
       "316  trans_nar_ext_toks       کنند      1"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confreq(\"اژدها\", group = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find in Document\n",
    "\n",
    "def find_doc(d, s):\n",
    "    \n",
    "    \"\"\"This function takes a dictionary of 5-grams as the first argument,\\\n",
    "    a regex search term as the second argument, and returns the sequence of 5 words\"\"\"\n",
    "    \n",
    "    for v in d:\n",
    "        m = re.match(s, v[2])\n",
    "        if m is not None:\n",
    "            yield ' '.join(v)\n",
    "            \n",
    "\n",
    "# Note: Return sends a specified value back to its caller\n",
    "# whereas Yield can produce a sequence of values.\n",
    "\n",
    "\n",
    "# Example:\n",
    "## list(find_doc(five_grams['al_biruni_card_catalog_suleimanov_fond'], 'ف.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Corpus\n",
    "## Produces a generator object with the KWIC with associated work title\n",
    "\n",
    "def find_corpus(c, s):\n",
    "    for k, d in c.items():\n",
    "        for m in find_doc(d, s):\n",
    "            yield f'{k:50s}: {m}'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kwic(term, corpus=combo_five_grams):\n",
    "    \n",
    "    print('\\n'.join(find_corpus(corpus, term)))\n",
    "    \n",
    "    # todo: organize this by best match\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kwic(\"د.رو\", indo_five_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Conditional Frequency Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confreq (term, refine=\"none\"):\n",
    "    \n",
    "    \"\"\"Conditional frequency across multiple corpora and sub-corpora.\n",
    "        Takes a search term (no regex) for the first word in the bigram, \n",
    "        as well as an optional regex filter to narrow down the second word\n",
    "        in the bigram sequence.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    if refine == \"none\" and len(combo_cfd[term]) > 0:\n",
    "    \n",
    "        if len(combo_cfd[term]) > 0:\n",
    "            print (term, \" is most commonly followed by:\\n\\n\", combo_cfd[term].most_common(10))\n",
    "\n",
    "        \n",
    "\n",
    "        if len(combo_cfd[term]) > 0:\n",
    "            print(\"\\nWithin sub-corpora:\\n\")\n",
    "\n",
    "            # Still need to fill out sub-corpora\n",
    "\n",
    "            if len(doc_cfd[term]) > 0 :\n",
    "                print (\"\\tDocuments:\", term, \" is most commonly followed by:\\n\\n\\t\", doc_cfd[term].most_common(5))\n",
    "            if len(nar_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tNarrative texts:\", term, \" is most commonly followed by:\\n\\n\\t\", nar_cfd[term].most_common(5))\n",
    "            if len(indo_cfd[term]) > 0 :\n",
    "                print (\"\\n\\n\\tIndic texts:\", term, \" is most commonly followed by:\\n\\n\\t\", indo_cfd[term].most_common(5))\n",
    "            if len(trans_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tTransoxania texts:\", term, \" is most commonly followed by:\\n\\n\\t\", trans_cfd[term].most_common(5))\n",
    "    \n",
    "    # Optional regex refinement of the results:\n",
    "    if refine != \"none\" and len(combo_cfd[term]) > 0:\n",
    "        \n",
    "        filt = re.compile(refine)\n",
    "        \n",
    "        filt_toks = [(x, y) for (x, y) in combo_cfd[term].items() if re.match(refine, x)]\n",
    "        \n",
    "        print (\"With the results filtered by the regex search (\", refine, \"), the most likely words following,\", term, \"are:\\n\\t\", filt_toks)\n",
    "        \n",
    "    elif len(combo_cfd[term]) == 0:\n",
    "            print (\"no results\")\n",
    "        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confreq(\"قوش\")\n",
    "#help(confreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regcf (term):\n",
    "    \n",
    "    \"\"\"\n",
    "        From a regex search term finds the most frequent possible word, then returns \n",
    "        conditional frequency (from bigrams) across multiple corpora.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if best_match(term, combo_freq) is not None:\n",
    "        \n",
    "        local_match = best_match(term, combo_freq)[0]\n",
    "        \n",
    "        print (\"The most likly match (based on word frequency) for \", term, \" is \", local_match,\\\n",
    "              \"(with frequency\", best_match(term, combo_freq)[1], \").\\n\")\n",
    "        print (\"Conditional frequency of\", local_match, \"(combined corpus):\\n\\t\", combo_cfd[local_match].most_common(5))\n",
    "        \n",
    "        print (\"\\nSub-Corpora:\\n\")\n",
    "        \n",
    "        print (\"\\n\\tDocuments:\\n\\t\", doc_cfd[local_match].most_common(5))\n",
    "        print (\"\\n\\tNarrative texts:\\n\\t\", nar_cfd[local_match].most_common(5))\n",
    "        \n",
    "        print (\"\\n\\n\\tIndic texts:\\n\\t\", indo_cfd[local_match].most_common(5))\n",
    "        print (\"\\n\\tTransoxania texts:\\n\\t\", trans_cfd[local_match].most_common(5))\n",
    "        \n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regcf(\"ق.ش\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: functions for 3-part confreq, and reverse confreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third term, if first two known:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Document Corpus (Meta-Corpus simply too computationally costly)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tricfd (first_term, second_term, refine=\"none\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given two words in a row, conditional frequency of the third word in the sequence.\n",
    "    Inputs: two words (in order), and an optional regex filter on the third word.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    #print (\"The pair \", first_term, second_term, \" is most commonly followed by :\\n\")\n",
    "    #output = combo_tricfd[(first_term, second_term)].most_common(10)\n",
    "    #print (output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if refine == \"none\" and len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "    \n",
    "        if len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "            print (\"The pair \", first_term, second_term, \" is most commonly followed by:\\n\\n\", combo_tricfd[(first_term, second_term)].most_common(10))\n",
    "\n",
    "\n",
    "        if len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "            print(\"\\nWithin sub-corpora:\\n\")\n",
    "\n",
    "            # Still need to fill out sub-corpora\n",
    "\n",
    "            if len(doc_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\tDocuments:\\n\\n\\t\", doc_tricfd[(first_term, second_term)].most_common(5))\n",
    "            if len(nar_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\n\\tNarrative texts:\\n\\n\\t\", nar_tricfd[(first_term, second_term)].most_common(5))\n",
    "            if len(indo_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\n\\n\\tIndic texts:\\n\\n\\t\", indo_tricfd[(first_term, second_term)].most_common(5))\n",
    "            if len(trans_tricfd[(first_term, second_term)]) > 0 :\n",
    "                print (\"\\n\\tTransoxania texts::\\n\\n\\t\", trans_tricfd[(first_term, second_term)].most_common(5))\n",
    "    \n",
    "    # Optional regex refinement of the results:\n",
    "    if refine != \"none\" and len(combo_tricfd[(first_term, second_term)]) > 0:\n",
    "        \n",
    "        filt = re.compile(refine)\n",
    "        \n",
    "        filt_toks = [(x, y) for (x, y) in combo_tricfd[(first_term, second_term)].items() if re.match(refine, x)]\n",
    "        \n",
    "        print (\"With the results filtered by the regex search (\", refine, \"), the most likely words following the pair \", first_term, second_term, \"are:\\n\\t\", filt_toks)\n",
    "        \n",
    "    elif len(combo_tricfd[(first_term, second_term)]) == 0:\n",
    "            print (\"no results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tricfd (\"بعد\", \"از\", \"خ+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversed conditional frequency, i.e. if second word in sequence known but not first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Meta-Corpus*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revcfd (term, refine=\"none\"):\n",
    "    \n",
    "    \n",
    "    \"\"\"Reverse conditional frequency (bigrams) across multiple corpora and sub-corpora.\n",
    "        Takes a search term (no regex) for the first word in the bigram, \n",
    "        as well as an optional regex filter to narrow down the second word\n",
    "        in the bigram sequence.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    if refine == \"none\" and len(rev_combo_cfd[term]) > 0:\n",
    "    \n",
    "        if len(rev_combo_cfd[term]) > 0:\n",
    "            print (term, \" is most commonly preceded by:\\n\\n\", rev_combo_cfd[term].most_common(10))\n",
    "\n",
    "        \n",
    "\n",
    "        if len(rev_combo_cfd[term]) > 0:\n",
    "            print(\"\\nWithin sub-corpora:\\n\")\n",
    "\n",
    "            # Still need to fill out sub-corpora\n",
    "\n",
    "            if len(rev_doc_cfd[term]) > 0 :\n",
    "                print (\"\\tDocuments:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_doc_cfd[term].most_common(5))\n",
    "            if len(rev_nar_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tNarrative texts:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_nar_cfd[term].most_common(5))\n",
    "            if len(rev_indo_cfd[term]) > 0 :\n",
    "                print (\"\\n\\n\\tIndic texts:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_indo_cfd[term].most_common(5))\n",
    "            if len(rev_trans_cfd[term]) > 0 :\n",
    "                print (\"\\n\\tTransoxania texts:\", term, \" is most commonly preceded by:\\n\\n\\t\", rev_trans_cfd[term].most_common(5))\n",
    "    \n",
    "    # Optional regex refinement of the results:\n",
    "    if refine != \"none\" and len(rev_combo_cfd[term]) > 0:\n",
    "        \n",
    "        filt = re.compile(refine)\n",
    "        \n",
    "        filt_toks = [(x, y) for (x, y) in rev_combo_cfd[term].items() if re.match(refine, x)]\n",
    "        \n",
    "        print (\"With the results filtered by the regex search (\", refine, \"), the most likely words preceding,\", term, \"are:\\n\\t\", filt_toks)\n",
    "        \n",
    "    elif len(rev_combo_cfd[term]) == 0:\n",
    "            print (\"no results\")\n",
    "        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revcfd(\"قوش\", \"خد\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----\n",
    "# Graveyard\n",
    "(i.e. code saved for posterity, no longer active)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# for whatever reason you can't just use the concordance method on a string;\n",
    "# you have to convert it to an NLTK Text type one way or another\n",
    "\n",
    "trans_corpus = nltk.Text(raw_combo_toks)\n",
    "\n",
    "#trans_corpus.concordance('خانه')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tokens in corpus regex matching the string:*\n",
    "\n",
    "(obsolete with custom KWIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "toks = [x for x in combo_freq if re.match(r'...خوی', x)]\n",
    "toks[:5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "conc0 = sum([trans_corpus.concordance_list(x) for x in toks], [])\n",
    "conc1 = [c.line for c in conc0]\n",
    "print('\\n'.join(conc1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom KWIC (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(drafting, active version now as a function, saved in markdown for posterity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Creating 5-Grams\n",
    "\n",
    "five_grams = {k:list(nltk.ngrams(v, 5)) for (k,v) in combined_corpus_toks.items() if len(v) >= 5}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Find in Document\n",
    "## This function takes a dictionary of 5-grams as the first argument,\n",
    "## a regex search term as the second argument, and returns the sequence of 5 words\n",
    "\n",
    "def find_doc(d, s):\n",
    "    for v in d:\n",
    "        m = re.match(s, v[2])\n",
    "        if m is not None:\n",
    "            yield ' '.join(v)\n",
    "            \n",
    "\n",
    "# Note: Return sends a specified value back to its caller\n",
    "# whereas Yield can produce a sequence of values.\n",
    "\n",
    "\n",
    "# Example:\n",
    "## list(find_doc(five_grams['al_biruni_card_catalog_suleimanov_fond'], 'ف.'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Find Corpus\n",
    "## Produces a generator object with the KWIC with associated work title\n",
    "\n",
    "def find_corpus(c, s):\n",
    "    for k, d in five_grams.items():\n",
    "        for m in find_doc(d, s):\n",
    "            yield f'{k:50s}: {m}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Formatting\n",
    "\n",
    "def print_align(v, m):\n",
    "    plen = max([sum([len(z)+1 for z in x[:m]]) for x in v])\n",
    "    for x in v:\n",
    "        pre = ' '.join(x[:m])\n",
    "        mid = x[m]\n",
    "        pos = ' '.join(x[m+1:])\n",
    "        print(f'{pre:>{plen}s} \\033[1m{mid}\\033[0m {pos}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print('\\n'.join(find_corpus(five_grams, '^من.قر?$')))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Meta-Corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# ConditionalFreqDist() takes a list of pairs.\n",
    "# Generator variable uses itself up upon assignment, so need to recreate above\n",
    "\n",
    "bigrams_cfd = nltk.ngrams(raw_combo_toks, 2)\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(bigrams_cfd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Conditional Frequency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Meta-Corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "search_term = r\"جهد\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print (search_term, \" is most commonly followed by:\\n\")\n",
    "cfd[search_term].most_common(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Document Corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "bigrams_doc_fd = nltk.ngrams(raw_doc_toks, 2)\n",
    "\n",
    "cfd_doc = nltk.ConditionalFreqDist(bigrams_doc_fd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "search_term = \"بداند\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print (\"\\nin the documents corpus, \", search_term, \" is most commonly followed by: \\n\")\n",
    "cfd_doc[search_term].most_common(5)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
