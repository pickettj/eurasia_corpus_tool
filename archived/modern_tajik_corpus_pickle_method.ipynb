{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de196cbe",
   "metadata": {},
   "source": [
    "# Comparing Verb Forms in Modern Tajik (Newspaper Corpus) with Bukhari Persian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c71aef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78695319",
   "metadata": {},
   "source": [
    "Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af297b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set home directory path\n",
    "hdir = os.path.expanduser('~')\n",
    "\n",
    "# Tajik corpus directory\n",
    "taj_path = os.path.join(hdir, \"Dropbox/Active_Directories/Digital_Humanities/Corpora/tajik_newspaper_corpus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef43c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold data\n",
    "data = []\n",
    "\n",
    "# Walk through the directory structure\n",
    "for subdir, dirs, files in os.walk(taj_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            subdir_name = os.path.basename(subdir)\n",
    "            data.append({'sub_directory': subdir_name, 'filename': file, 'content': content})\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b955566a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141572"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of Tajik newspaper articles\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c740e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1198cf1e",
   "metadata": {},
   "source": [
    "### Regex search pattern for Tajik newspapers\n",
    "\n",
    "`ме` marks the beginning of the participle (unlike می in Persian, there it is always attached to the verb without a space); then comes the verb participle; which ends with `дагӣ`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fcc37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "taj_medagi_pattern = r'\\Sи\\sме[^ ]*?дагӣ\\s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82f6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame and select only rows where 'content' column matches the pattern\n",
    "filtered_df = df[df['content'].str.contains(taj_medagi_pattern, regex=True, na=False)]\n",
    "\n",
    "# Keep only the columns where 'content' column matches the pattern\n",
    "filtered_df = filtered_df[['sub_directory', 'filename', 'content']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c176b4a0-d153-4843-b702-38e9f5fc8205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_with_pattern(df, pattern):\n",
    "    \"\"\"\n",
    "    Extract sentences containing the regex pattern from the dataframe\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'content' column\n",
    "        pattern: regex pattern to search for\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with additional columns for matched sentences\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        content = row['content']\n",
    "        \n",
    "        # Split content into sentences (simple approach using common sentence endings)\n",
    "        # This handles Cyrillic punctuation as well\n",
    "        sentences = re.split(r'[.!?։]\\s+', content)\n",
    "        \n",
    "        # Find sentences that contain the pattern\n",
    "        matching_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if re.search(pattern, sentence):\n",
    "                # Clean up the sentence (remove extra whitespace)\n",
    "                clean_sentence = ' '.join(sentence.split())\n",
    "                matching_sentences.append(clean_sentence)\n",
    "        \n",
    "        # If we found matching sentences, add them to results\n",
    "        if matching_sentences:\n",
    "            for sentence in matching_sentences:\n",
    "                results.append({\n",
    "                    'sub_directory': row['sub_directory'],\n",
    "                    'filename': row['filename'],\n",
    "                    'matching_sentence': sentence\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ebe07c8-a731-4bf7-8141-7bf2dcaba789",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_df = extract_sentences_with_pattern(df, taj_medagi_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c717541e-bc16-4061-bbfb-d9326ed1a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sub_directory                      filename  \\\n",
      "0                Oila         oila_2022-12-31-2.txt   \n",
      "1  Javonon_Tojikiston      javonon_13_Июн_20173.txt   \n",
      "2  Javonon_Tojikiston     javonon_11_Май_20231.txt   \n",
      "3         osiyoavrupo  osiyoavrupo_2022_03_19_0.txt   \n",
      "4         osiyoavrupo  osiyoavrupo_2022_03_19_0.txt   \n",
      "\n",
      "                                   matching_sentence  \n",
      "0  Дар муносибатҳои ошқонаи Ханда ва Карим ҳанӯз ...  \n",
      "1  Акнун қатъӣ талаб кард: - Ман ба ту чизи мехӯр...  \n",
      "2  Аммо волидон бо ҳар баҳона дар маҷлисҳои падар...  \n",
      "3  Ҳанӯз пеш аз фарорасии Наврӯз қазоқҳо анъанае ...  \n",
      "4  Ба касоне, ки хусумат доштанд, сулҳу оштиро ба...  \n"
     ]
    }
   ],
   "source": [
    "print(pat_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84fa3621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf1476fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141572"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0032c007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of total articles to articles with 'и медагӣ': 0.00\n"
     ]
    }
   ],
   "source": [
    "ratio=len(filtered_df)/len(df)\n",
    "print(f\"Ratio of total articles to articles with 'и медагӣ': {ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e89f213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def export_sentences_to_file(df, hdir=None):\n",
    "    \"\"\"Export all sentences to a text file with timestamp\"\"\"\n",
    "    import datetime\n",
    "    \n",
    "    # Use home directory if not provided\n",
    "    if hdir is None:\n",
    "        hdir = os.path.expanduser('~')\n",
    "    \n",
    "    # Create timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"taj_news_export_{timestamp}.txt\"\n",
    "    \n",
    "    # Create full path\n",
    "    full_path = os.path.join(hdir, \"Dropbox/Active_Directories/Inbox\", filename)\n",
    "    \n",
    "    # Export the sentences\n",
    "    with open(full_path, 'w', encoding='utf-8') as f:\n",
    "        for i, (idx, row) in enumerate(df.iterrows()):\n",
    "            f.write(f\"--- Example {i+1} ---\\n\")\n",
    "            f.write(f\"File: {row['filename']}\\n\")\n",
    "            f.write(f\"Directory: {row['sub_directory']}\\n\")\n",
    "            f.write(f\"Sentence: {row['matching_sentence']}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"Exported {len(df)} sentences to {full_path}\")\n",
    "    return full_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e65af8cc-dfce-4401-ae7a-e941bb53c301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 18 sentences to /Users/pickettj/Dropbox/Active_Directories/Inbox/taj_news_export_20250815_095448.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/pickettj/Dropbox/Active_Directories/Inbox/taj_news_export_20250815_095448.txt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_sentences_to_file(pat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59917326",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = hdir + \"/Dropbox/Active_Directories/Digital_Humanities/Corpora/pickled_tokenized_cleaned_corpora\"\n",
    "\n",
    "df_eurcorp = pd.read_csv (os.path.join(pickle_path,r'eurasia_corpus.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ad61fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>No</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1357385</th>\n",
       "      <td>indo_nar_ext_toks</td>\n",
       "      <td>ain-i_akbari_murty</td>\n",
       "      <td>51235</td>\n",
       "      <td>از</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6311361</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>attar.ma</td>\n",
       "      <td>37544</td>\n",
       "      <td>خود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9491062</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>hojviri.kashfol-mahjoob</td>\n",
       "      <td>19777</td>\n",
       "      <td>می</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352423</th>\n",
       "      <td>indo_nar_ext_toks</td>\n",
       "      <td>sjn2</td>\n",
       "      <td>53215</td>\n",
       "      <td>ازیشان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8986365</th>\n",
       "      <td>pers_lit_toks</td>\n",
       "      <td>ghaani.divan</td>\n",
       "      <td>191497</td>\n",
       "      <td>جسم</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Category                     Text      No   Token\n",
       "1357385  indo_nar_ext_toks       ain-i_akbari_murty   51235      از\n",
       "6311361      pers_lit_toks                 attar.ma   37544     خود\n",
       "9491062      pers_lit_toks  hojviri.kashfol-mahjoob   19777      می\n",
       "352423   indo_nar_ext_toks                     sjn2   53215  ازیشان\n",
       "8986365      pers_lit_toks             ghaani.divan  191497     جسم"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eurcorp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd67cf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['indo_nar_ext_toks' 'trans_nar_ext_toks' 'khiva_doc_toks'\n",
      " 'oldsys_xml_toks' 'indo_nar' 'trans_xml_toks' 'indo_xml_toks'\n",
      " 'presort_xml_toks' 'md_oldsys_toks' 'pers_lit_toks']\n"
     ]
    }
   ],
   "source": [
    "unique_categories = df_eurcorp['Category'].unique()\n",
    "print(unique_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fc7ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_keep = ['trans_xml_toks', 'presort_xml_toks', 'md_oldsys_toks']\n",
    "trans_docs = df_eurcorp[df_eurcorp['Category'].isin(categories_to_keep)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaf9ea61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>No</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5274612</th>\n",
       "      <td>md_oldsys_toks</td>\n",
       "      <td>apsa_119</td>\n",
       "      <td>18</td>\n",
       "      <td>اعلام</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5257004</th>\n",
       "      <td>presort_xml_toks</td>\n",
       "      <td>ser972</td>\n",
       "      <td>9</td>\n",
       "      <td>تعالی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5282138</th>\n",
       "      <td>md_oldsys_toks</td>\n",
       "      <td>tsgaruz_i126_1_1867_5_93</td>\n",
       "      <td>36</td>\n",
       "      <td>تصدق</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5266172</th>\n",
       "      <td>presort_xml_toks</td>\n",
       "      <td>ser105</td>\n",
       "      <td>30</td>\n",
       "      <td>بنیاد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5284842</th>\n",
       "      <td>md_oldsys_toks</td>\n",
       "      <td>tsgaruz_ i126-1-1904-4_ser518</td>\n",
       "      <td>443</td>\n",
       "      <td>جای</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Category                           Text   No  Token\n",
       "5274612    md_oldsys_toks                       apsa_119   18  اعلام\n",
       "5257004  presort_xml_toks                         ser972    9  تعالی\n",
       "5282138    md_oldsys_toks       tsgaruz_i126_1_1867_5_93   36   تصدق\n",
       "5266172  presort_xml_toks                         ser105   30  بنیاد\n",
       "5284842    md_oldsys_toks  tsgaruz_ i126-1-1904-4_ser518  443    جای"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_docs.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679823d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сози мекардаги\n",
    "# ساز می کرده گی\n",
    "\n",
    "# regex: limit the length that the word can be so that you don't get the whole document, maybe 20 character limit on stuff that is not dagi\n",
    "\n",
    "# state that they are (or are not) comparable in terms of length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a017b3e9-ed2f-42a6-8d1d-ae07657539a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Persian می...گی patterns...\n",
      "Found 6 matches\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def find_persian_medagi_pattern(df):\n",
    "    \"\"\"\n",
    "    Find Persian می...گی patterns in tokenized corpus\n",
    "    Handles both single-token and multi-token cases\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['Category', 'Text', 'No', 'Token']\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with matched patterns and context\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Group by text to work with sequences of tokens\n",
    "    for text_name, text_group in df.groupby('Text'):\n",
    "        tokens = text_group.sort_values('No')['Token'].tolist()\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            # Case 1: Single token containing می...گی pattern\n",
    "            single_token_pattern = r'می.*?گی'\n",
    "            if re.search(single_token_pattern, token):\n",
    "                # Get context (5 tokens before and after)\n",
    "                start_idx = max(0, i - 5)\n",
    "                end_idx = min(len(tokens), i + 6)\n",
    "                context_tokens = tokens[start_idx:end_idx]\n",
    "                \n",
    "                results.append({\n",
    "                    'text_name': text_name,\n",
    "                    'pattern_type': 'single_token',\n",
    "                    'matched_token': token,\n",
    "                    'token_position': i,\n",
    "                    'context': ' '.join(context_tokens),\n",
    "                    'match_detail': token\n",
    "                })\n",
    "            \n",
    "            # Case 2: Multi-token pattern می + ... + گی\n",
    "            elif token == 'می':\n",
    "                # Look ahead for گی within reasonable distance (up to 3 tokens)\n",
    "                for j in range(i + 1, min(i + 4, len(tokens))):\n",
    "                    if tokens[j].endswith('گی'):\n",
    "                        # Found a match\n",
    "                        matched_sequence = tokens[i:j+1]\n",
    "                        \n",
    "                        # Get broader context\n",
    "                        start_idx = max(0, i - 5)\n",
    "                        end_idx = min(len(tokens), j + 6)\n",
    "                        context_tokens = tokens[start_idx:end_idx]\n",
    "                        \n",
    "                        results.append({\n",
    "                            'text_name': text_name,\n",
    "                            'pattern_type': 'multi_token',\n",
    "                            'matched_token': ' '.join(matched_sequence),\n",
    "                            'token_position': f\"{i}-{j}\",\n",
    "                            'context': ' '.join(context_tokens),\n",
    "                            'match_detail': f\"می + {' + '.join(tokens[i+1:j])} + {tokens[j]}\"\n",
    "                        })\n",
    "                        break  # Stop at first match to avoid overlaps\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def find_alternative_persian_patterns(df):\n",
    "    \"\"\"\n",
    "    Alternative approach: reconstruct text and use regex\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for text_name, text_group in df.groupby('Text'):\n",
    "        # Reconstruct the text from tokens\n",
    "        tokens = text_group.sort_values('No')['Token'].tolist()\n",
    "        reconstructed_text = ' '.join(tokens)\n",
    "        \n",
    "        # Find all می...گی patterns (allowing for spaces)\n",
    "        pattern = r'می\\s+.*?\\s+گی|می.*?گی'\n",
    "        matches = list(re.finditer(pattern, reconstructed_text))\n",
    "        \n",
    "        for match in matches:\n",
    "            # Find which tokens this spans\n",
    "            match_text = match.group()\n",
    "            \n",
    "            results.append({\n",
    "                'text_name': text_name,\n",
    "                'matched_sequence': match_text,\n",
    "                'full_context': reconstructed_text,\n",
    "                'match_start': match.start(),\n",
    "                'match_end': match.end()\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Usage with your trans_docs dataframe:\n",
    "# Method 1: Token-by-token approach\n",
    "persian_matches = find_persian_medagi_pattern(trans_docs)\n",
    "\n",
    "# Method 2: Text reconstruction approach  \n",
    "# persian_alt_matches = find_alternative_persian_patterns(trans_docs)\n",
    "\n",
    "# Display results\n",
    "def display_persian_matches(matches_df, num_to_show=10):\n",
    "    \"\"\"Display Persian matches in readable format\"\"\"\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    print(f\"Found {len(matches_df)} Persian می...گی patterns\\n\")\n",
    "    \n",
    "    for i, (idx, row) in enumerate(matches_df.iterrows()):\n",
    "        if i >= num_to_show:\n",
    "            break\n",
    "        print(f\"--- Match {i+1} ---\")\n",
    "        print(f\"Text: {row['text_name']}\")\n",
    "        print(f\"Type: {row['pattern_type']}\")\n",
    "        print(f\"Match: {row['matched_token']}\")\n",
    "        print(f\"Context: {row['context']}\")\n",
    "        if 'match_detail' in row:\n",
    "            print(f\"Detail: {row['match_detail']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Export function for Persian matches\n",
    "def export_persian_sentences_to_file(df, hdir=None):\n",
    "    \"\"\"Export Persian sentences to text file with timestamp\"\"\"\n",
    "    import datetime\n",
    "    \n",
    "    if hdir is None:\n",
    "        hdir = os.path.expanduser('~')\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"persian_medagi_export_{timestamp}.txt\"\n",
    "    full_path = os.path.join(hdir, \"Dropbox/Active_Directories/Inbox\", filename)\n",
    "    \n",
    "    with open(full_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Persian می...گی Pattern Matches\\n\")\n",
    "        f.write(f\"Total matches found: {len(df)}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        for i, (idx, row) in enumerate(df.iterrows()):\n",
    "            f.write(f\"--- Match {i+1} ---\\n\")\n",
    "            f.write(f\"Text: {row['text_name']}\\n\")\n",
    "            f.write(f\"Pattern Type: {row['pattern_type']}\\n\")\n",
    "            f.write(f\"Matched Token(s): {row['matched_token']}\\n\")\n",
    "            f.write(f\"Position: {row['token_position']}\\n\")\n",
    "            f.write(f\"Context: {row['context']}\\n\")\n",
    "            if 'match_detail' in row:\n",
    "                f.write(f\"Detail: {row['match_detail']}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"Exported {len(df)} Persian matches to {full_path}\")\n",
    "    return full_path\n",
    "\n",
    "# Run the analysis:\n",
    "print(\"Searching for Persian می...گی patterns...\")\n",
    "persian_matches = find_persian_medagi_pattern(trans_docs)\n",
    "print(f\"Found {len(persian_matches)} matches\")\n",
    "\n",
    "# To view results:\n",
    "# display_persian_matches(persian_matches, 5)\n",
    "\n",
    "# To export:\n",
    "# export_persian_sentences_to_file(persian_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c3ebaff-d75c-4a51-b168-0b2a11e49db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 Persian می...گی patterns\n",
      "\n",
      "--- Match 1 ---\n",
      "Text: ser179\n",
      "Type: single_token\n",
      "Match: میگیرید\n",
      "Context: و بیچاره گان ولایت دعا میگیرید تصدق شوم اینغلام دعاگوی موافق\n",
      "Detail: میگیرید\n",
      "--------------------------------------------------------------------------------\n",
      "--- Match 2 ---\n",
      "Text: ser193\n",
      "Type: multi_token\n",
      "Match: می ایستاده گی\n",
      "Context: عملدار دولتخانه بحضور ایلچی خانه می ایستاده گی گردیده بعد از چند وقت\n",
      "Detail: می + ایستاده + گی\n",
      "--------------------------------------------------------------------------------\n",
      "--- Match 3 ---\n",
      "Text: ser706\n",
      "Type: multi_token\n",
      "Match: می شده گی\n",
      "Context: از وجه تعفن ناک جمع می شده گی باشد و آب باران بخندقهای\n",
      "Detail: می + شده + گی\n",
      "--------------------------------------------------------------------------------\n",
      "--- Match 4 ---\n",
      "Text: ser967\n",
      "Type: single_token\n",
      "Match: میانگی\n",
      "Context: افروزی سواری اینمکان از در میانگی حکم شهزاد اومیج صاحب که\n",
      "Detail: میانگی\n",
      "--------------------------------------------------------------------------------\n",
      "--- Match 5 ---\n",
      "Text: tsgaruz_i126_1_1903_1_178\n",
      "Type: single_token\n",
      "Match: میشدگی\n",
      "Context: الله منشور رو برضای مبارک میشدگی است لهذا دارنده را تعیین\n",
      "Detail: میشدگی\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "display_persian_matches(persian_matches, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec9702-c74c-4762-be5f-31d9affacee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start over using indexed texts instead of pickles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
